{"./":{"url":"./","title":"序言","keywords":"","body":"Kubernetes 源码分析笔记 本系列是 Kubernetes 源码分析笔记 更多的学习笔记请参考： Kubernetes 学习笔记 Golang 学习笔记 Linux 学习笔记 数据结构学习笔记 个人博客：www.huweihuang.com 目录 前言 序言 kube-apiserver NewAPIServerCommand kube-controller-manager 源码思维导图 NewControllerManagerCommand DeploymentController Informer机制 Informer原理 sharedIndexInformer Reflector DeltaFIFO processLoop 总结 kube-scheduler 源码思维导图 NewSchedulerCommand registerAlgorithmProvider scheduleOne findNodesThatFit PrioritizeNodes preempt kubelet 源码思维导图 NewKubeletCommand NewMainKubelet startKubelet syncLoopIteration syncPod 赞赏 如果觉得文章有帮助的话，可以打赏一下，谢谢！ Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2021-04-18 13:19:28 "},"kube-apiserver/NewAPIServerCommand.html":{"url":"kube-apiserver/NewAPIServerCommand.html","title":"NewAPIServerCommand","keywords":"","body":"kube-apiserver源码分析（一）之 NewAPIServerCommand 以下代码分析基于 kubernetes v1.12.0 版本。 本文主要分析kube-apiserver中cmd部分的代码，即NewAPIServerCommand相关的代码。更多具体的逻辑待后续文章分析。 kube-apiserver的cmd部分目录代码结构如下： kube-apiserver ├── apiserver.go # kube-apiserver的main入口 └── app ├── aggregator.go ├── apiextensions.go ├── options # 初始化kube-apiserver使用到的option │ ├── options.go # 包括：NewServerRunOptions、Flags等 │ ├── options_test.go │ └── validation.go ├── server.go # 包括：NewAPIServerCommand、Run、CreateServerChain、Complete等 1. Main 此部分代码位于cmd/kube-apiserver/apiserver.go func main() { rand.Seed(time.Now().UTC().UnixNano()) command := app.NewAPIServerCommand(server.SetupSignalHandler()) // TODO: once we switch everything over to Cobra commands, we can go back to calling // utilflag.InitFlags() (by removing its pflag.Parse() call). For now, we have to set the // normalize func and add the go flag set by hand. pflag.CommandLine.SetNormalizeFunc(utilflag.WordSepNormalizeFunc) pflag.CommandLine.AddGoFlagSet(goflag.CommandLine) // utilflag.InitFlags() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"error: %v\\n\", err) os.Exit(1) } } 核心代码： // 初始化APIServerCommand command := app.NewAPIServerCommand(server.SetupSignalHandler()) // 执行Execute err := command.Execute() 2. NewAPIServerCommand 此部分的代码位于/cmd/kube-apiserver/app/server.go NewAPIServerCommand即Cobra命令行框架的构造函数，主要包括三部分： 构造option 添加Flags 执行Run函数 完整代码如下： 此部分代码位于cmd/kube-apiserver/app/server.go // NewAPIServerCommand creates a *cobra.Command object with default parameters func NewAPIServerCommand(stopCh 核心代码： // 构造option s := options.NewServerRunOptions() // 添加flags fs := cmd.Flags() namedFlagSets := s.Flags() for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } // set default options completedOptions, err := Complete(s) // Run Run(completedOptions, stopCh) 3. NewServerRunOptions NewServerRunOptions基于默认的参数构造ServerRunOptions结构体。ServerRunOptions是apiserver运行的配置信息。具体结构体定义如下。 3.1. ServerRunOptions 其中主要的配置如下： GenericServerRunOptions Etcd SecureServing KubeletConfig ... // ServerRunOptions runs a kubernetes api server. type ServerRunOptions struct { GenericServerRunOptions *genericoptions.ServerRunOptions Etcd *genericoptions.EtcdOptions SecureServing *genericoptions.SecureServingOptionsWithLoopback InsecureServing *genericoptions.DeprecatedInsecureServingOptionsWithLoopback Audit *genericoptions.AuditOptions Features *genericoptions.FeatureOptions Admission *kubeoptions.AdmissionOptions Authentication *kubeoptions.BuiltInAuthenticationOptions Authorization *kubeoptions.BuiltInAuthorizationOptions CloudProvider *kubeoptions.CloudProviderOptions StorageSerialization *kubeoptions.StorageSerializationOptions APIEnablement *genericoptions.APIEnablementOptions AllowPrivileged bool EnableLogsHandler bool EventTTL time.Duration KubeletConfig kubeletclient.KubeletClientConfig KubernetesServiceNodePort int MaxConnectionBytesPerSec int64 ServiceClusterIPRange net.IPNet // TODO: make this a list ServiceNodePortRange utilnet.PortRange SSHKeyfile string SSHUser string ProxyClientCertFile string ProxyClientKeyFile string EnableAggregatorRouting bool MasterCount int EndpointReconcilerType string ServiceAccountSigningKeyFile string } 3.2. NewServerRunOptions NewServerRunOptions初始化配置结构体。 // NewServerRunOptions creates a new ServerRunOptions object with default parameters func NewServerRunOptions() *ServerRunOptions { s := ServerRunOptions{ GenericServerRunOptions: genericoptions.NewServerRunOptions(), Etcd: genericoptions.NewEtcdOptions(storagebackend.NewDefaultConfig(kubeoptions.DefaultEtcdPathPrefix, nil)), SecureServing: kubeoptions.NewSecureServingOptions(), InsecureServing: kubeoptions.NewInsecureServingOptions(), Audit: genericoptions.NewAuditOptions(), Features: genericoptions.NewFeatureOptions(), Admission: kubeoptions.NewAdmissionOptions(), Authentication: kubeoptions.NewBuiltInAuthenticationOptions().WithAll(), Authorization: kubeoptions.NewBuiltInAuthorizationOptions(), CloudProvider: kubeoptions.NewCloudProviderOptions(), StorageSerialization: kubeoptions.NewStorageSerializationOptions(), APIEnablement: genericoptions.NewAPIEnablementOptions(), EnableLogsHandler: true, EventTTL: 1 * time.Hour, MasterCount: 1, EndpointReconcilerType: string(reconcilers.LeaseEndpointReconcilerType), KubeletConfig: kubeletclient.KubeletClientConfig{ Port: ports.KubeletPort, ReadOnlyPort: ports.KubeletReadOnlyPort, PreferredAddressTypes: []string{ // --override-hostname string(api.NodeHostName), // internal, preferring DNS if reported string(api.NodeInternalDNS), string(api.NodeInternalIP), // external, preferring DNS if reported string(api.NodeExternalDNS), string(api.NodeExternalIP), }, EnableHttps: true, HTTPTimeout: time.Duration(5) * time.Second, }, ServiceNodePortRange: kubeoptions.DefaultServiceNodePortRange, } s.ServiceClusterIPRange = kubeoptions.DefaultServiceIPCIDR // Overwrite the default for storage data format. s.Etcd.DefaultStorageMediaType = \"application/vnd.kubernetes.protobuf\" return &s } 3.3. Complete 当kube-apiserver的flags被解析后，调用Complete完成默认配置。 此部分代码位于cmd/kube-apiserver/app/server.go // Should be called after kube-apiserver flags parsed. func Complete(s *options.ServerRunOptions) (completedServerRunOptions, error) { var options completedServerRunOptions // set defaults if err := s.GenericServerRunOptions.DefaultAdvertiseAddress(s.SecureServing.SecureServingOptions); err != nil { return options, err } if err := kubeoptions.DefaultAdvertiseAddress(s.GenericServerRunOptions, s.InsecureServing.DeprecatedInsecureServingOptions); err != nil { return options, err } serviceIPRange, apiServerServiceIP, err := master.DefaultServiceIPRange(s.ServiceClusterIPRange) if err != nil { return options, fmt.Errorf(\"error determining service IP ranges: %v\", err) } s.ServiceClusterIPRange = serviceIPRange if err := s.SecureServing.MaybeDefaultWithSelfSignedCerts(s.GenericServerRunOptions.AdvertiseAddress.String(), []string{\"kubernetes.default.svc\", \"kubernetes.default\", \"kubernetes\"}, []net.IP{apiServerServiceIP}); err != nil { return options, fmt.Errorf(\"error creating self-signed certificates: %v\", err) } if len(s.GenericServerRunOptions.ExternalHost) == 0 { if len(s.GenericServerRunOptions.AdvertiseAddress) > 0 { s.GenericServerRunOptions.ExternalHost = s.GenericServerRunOptions.AdvertiseAddress.String() } else { if hostname, err := os.Hostname(); err == nil { s.GenericServerRunOptions.ExternalHost = hostname } else { return options, fmt.Errorf(\"error finding host name: %v\", err) } } glog.Infof(\"external host was not specified, using %v\", s.GenericServerRunOptions.ExternalHost) } s.Authentication.ApplyAuthorization(s.Authorization) // Use (ServiceAccountSigningKeyFile != \"\") as a proxy to the user enabling // TokenRequest functionality. This defaulting was convenient, but messed up // a lot of people when they rotated their serving cert with no idea it was // connected to their service account keys. We are taking this oppurtunity to // remove this problematic defaulting. if s.ServiceAccountSigningKeyFile == \"\" { // Default to the private server key for service account token signing if len(s.Authentication.ServiceAccounts.KeyFiles) == 0 && s.SecureServing.ServerCert.CertKey.KeyFile != \"\" { if kubeauthenticator.IsValidServiceAccountKeyFile(s.SecureServing.ServerCert.CertKey.KeyFile) { s.Authentication.ServiceAccounts.KeyFiles = []string{s.SecureServing.ServerCert.CertKey.KeyFile} } else { glog.Warning(\"No TLS key provided, service account token authentication disabled\") } } } if s.Etcd.StorageConfig.DeserializationCacheSize == 0 { // When size of cache is not explicitly set, estimate its size based on // target memory usage. glog.V(2).Infof(\"Initializing deserialization cache size based on %dMB limit\", s.GenericServerRunOptions.TargetRAMMB) // This is the heuristics that from memory capacity is trying to infer // the maximum number of nodes in the cluster and set cache sizes based // on that value. // From our documentation, we officially recommend 120GB machines for // 2000 nodes, and we scale from that point. Thus we assume ~60MB of // capacity per node. // TODO: We may consider deciding that some percentage of memory will // be used for the deserialization cache and divide it by the max object // size to compute its size. We may even go further and measure // collective sizes of the objects in the cache. clusterSize := s.GenericServerRunOptions.TargetRAMMB / 60 s.Etcd.StorageConfig.DeserializationCacheSize = 25 * clusterSize if s.Etcd.StorageConfig.DeserializationCacheSize 3. AddFlagSet AddFlagSet主要的作用是通过外部传入的flag的具体值，解析的时候传递给option的结构体，最终给apiserver使用。 其中NewAPIServerCommand关于AddFlagSet的相关代码如下： fs := cmd.Flags() namedFlagSets := s.Flags() for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } 3.1. Flags Flags完整代码如下： 此部分代码位于cmd/kube-apiserver/app/options/options.go // Flags returns flags for a specific APIServer by section name func (s *ServerRunOptions) Flags() (fss apiserverflag.NamedFlagSets) { // Add the generic flags. s.GenericServerRunOptions.AddUniversalFlags(fss.FlagSet(\"generic\")) s.Etcd.AddFlags(fss.FlagSet(\"etcd\")) s.SecureServing.AddFlags(fss.FlagSet(\"secure serving\")) s.InsecureServing.AddFlags(fss.FlagSet(\"insecure serving\")) s.InsecureServing.AddUnqualifiedFlags(fss.FlagSet(\"insecure serving\")) // TODO: remove it until kops stops using `--address` s.Audit.AddFlags(fss.FlagSet(\"auditing\")) s.Features.AddFlags(fss.FlagSet(\"features\")) s.Authentication.AddFlags(fss.FlagSet(\"authentication\")) s.Authorization.AddFlags(fss.FlagSet(\"authorization\")) s.CloudProvider.AddFlags(fss.FlagSet(\"cloud provider\")) s.StorageSerialization.AddFlags(fss.FlagSet(\"storage\")) s.APIEnablement.AddFlags(fss.FlagSet(\"api enablement\")) s.Admission.AddFlags(fss.FlagSet(\"admission\")) // Note: the weird \"\"+ in below lines seems to be the only way to get gofmt to // arrange these text blocks sensibly. Grrr. fs := fss.FlagSet(\"misc\") fs.DurationVar(&s.EventTTL, \"event-ttl\", s.EventTTL, \"Amount of time to retain events.\") fs.BoolVar(&s.AllowPrivileged, \"allow-privileged\", s.AllowPrivileged, \"If true, allow privileged containers. [default=false]\") fs.BoolVar(&s.EnableLogsHandler, \"enable-logs-handler\", s.EnableLogsHandler, \"If true, install a /logs handler for the apiserver logs.\") // Deprecated in release 1.9 fs.StringVar(&s.SSHUser, \"ssh-user\", s.SSHUser, \"If non-empty, use secure SSH proxy to the nodes, using this user name\") fs.MarkDeprecated(\"ssh-user\", \"This flag will be removed in a future version.\") // Deprecated in release 1.9 fs.StringVar(&s.SSHKeyfile, \"ssh-keyfile\", s.SSHKeyfile, \"If non-empty, use secure SSH proxy to the nodes, using this user keyfile\") fs.MarkDeprecated(\"ssh-keyfile\", \"This flag will be removed in a future version.\") fs.Int64Var(&s.MaxConnectionBytesPerSec, \"max-connection-bytes-per-sec\", s.MaxConnectionBytesPerSec, \"\"+ \"If non-zero, throttle each user connection to this number of bytes/sec. \"+ \"Currently only applies to long-running requests.\") fs.IntVar(&s.MasterCount, \"apiserver-count\", s.MasterCount, \"The number of apiservers running in the cluster, must be a positive number. (In use when --endpoint-reconciler-type=master-count is enabled.)\") fs.StringVar(&s.EndpointReconcilerType, \"endpoint-reconciler-type\", string(s.EndpointReconcilerType), \"Use an endpoint reconciler (\"+strings.Join(reconcilers.AllTypes.Names(), \", \")+\")\") // See #14282 for details on how to test/try this option out. // TODO: remove this comment once this option is tested in CI. fs.IntVar(&s.KubernetesServiceNodePort, \"kubernetes-service-node-port\", s.KubernetesServiceNodePort, \"\"+ \"If non-zero, the Kubernetes master service (which apiserver creates/maintains) will be \"+ \"of type NodePort, using this as the value of the port. If zero, the Kubernetes master \"+ \"service will be of type ClusterIP.\") fs.IPNetVar(&s.ServiceClusterIPRange, \"service-cluster-ip-range\", s.ServiceClusterIPRange, \"\"+ \"A CIDR notation IP range from which to assign service cluster IPs. This must not \"+ \"overlap with any IP ranges assigned to nodes for pods.\") fs.Var(&s.ServiceNodePortRange, \"service-node-port-range\", \"\"+ \"A port range to reserve for services with NodePort visibility. \"+ \"Example: '30000-32767'. Inclusive at both ends of the range.\") // Kubelet related flags: fs.BoolVar(&s.KubeletConfig.EnableHttps, \"kubelet-https\", s.KubeletConfig.EnableHttps, \"Use https for kubelet connections.\") fs.StringSliceVar(&s.KubeletConfig.PreferredAddressTypes, \"kubelet-preferred-address-types\", s.KubeletConfig.PreferredAddressTypes, \"List of the preferred NodeAddressTypes to use for kubelet connections.\") fs.UintVar(&s.KubeletConfig.Port, \"kubelet-port\", s.KubeletConfig.Port, \"DEPRECATED: kubelet port.\") fs.MarkDeprecated(\"kubelet-port\", \"kubelet-port is deprecated and will be removed.\") fs.UintVar(&s.KubeletConfig.ReadOnlyPort, \"kubelet-read-only-port\", s.KubeletConfig.ReadOnlyPort, \"DEPRECATED: kubelet port.\") fs.DurationVar(&s.KubeletConfig.HTTPTimeout, \"kubelet-timeout\", s.KubeletConfig.HTTPTimeout, \"Timeout for kubelet operations.\") fs.StringVar(&s.KubeletConfig.CertFile, \"kubelet-client-certificate\", s.KubeletConfig.CertFile, \"Path to a client cert file for TLS.\") fs.StringVar(&s.KubeletConfig.KeyFile, \"kubelet-client-key\", s.KubeletConfig.KeyFile, \"Path to a client key file for TLS.\") fs.StringVar(&s.KubeletConfig.CAFile, \"kubelet-certificate-authority\", s.KubeletConfig.CAFile, \"Path to a cert file for the certificate authority.\") // TODO: delete this flag in 1.13 repair := false fs.BoolVar(&repair, \"repair-malformed-updates\", false, \"deprecated\") fs.MarkDeprecated(\"repair-malformed-updates\", \"This flag will be removed in a future version\") fs.StringVar(&s.ProxyClientCertFile, \"proxy-client-cert-file\", s.ProxyClientCertFile, \"\"+ \"Client certificate used to prove the identity of the aggregator or kube-apiserver \"+ \"when it must call out during a request. This includes proxying requests to a user \"+ \"api-server and calling out to webhook admission plugins. It is expected that this \"+ \"cert includes a signature from the CA in the --requestheader-client-ca-file flag. \"+ \"That CA is published in the 'extension-apiserver-authentication' configmap in \"+ \"the kube-system namespace. Components receiving calls from kube-aggregator should \"+ \"use that CA to perform their half of the mutual TLS verification.\") fs.StringVar(&s.ProxyClientKeyFile, \"proxy-client-key-file\", s.ProxyClientKeyFile, \"\"+ \"Private key for the client certificate used to prove the identity of the aggregator or kube-apiserver \"+ \"when it must call out during a request. This includes proxying requests to a user \"+ \"api-server and calling out to webhook admission plugins.\") fs.BoolVar(&s.EnableAggregatorRouting, \"enable-aggregator-routing\", s.EnableAggregatorRouting, \"Turns on aggregator routing requests to endpoints IP rather than cluster IP.\") fs.StringVar(&s.ServiceAccountSigningKeyFile, \"service-account-signing-key-file\", s.ServiceAccountSigningKeyFile, \"\"+ \"Path to the file that contains the current private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. (Requires the 'TokenRequest' feature gate.)\") return fss } 4. Run Run以常驻的方式运行apiserver。 主要内容如下： 构造一个聚合的server结构体。 执行PrepareRun。 最终执行Run。 此部分代码位于cmd/kube-apiserver/app/server.go // Run runs the specified APIServer. This should never exit. func Run(completeOptions completedServerRunOptions, stopCh 4.1. CreateServerChain 构造聚合的Server。 基本流程如下： 首先生成config对象，包括kubeAPIServerConfig、apiExtensionsConfig。 再通过config生成server对象，包括apiExtensionsServer、kubeAPIServer。 执行apiExtensionsServer、kubeAPIServer的PrepareRun部分。 生成聚合的config对象aggregatorConfig。 基于aggregatorConfig、kubeAPIServer、apiExtensionsServer生成聚合的serveraggregatorServer。 此部分代码位于cmd/kube-apiserver/app/server.go // CreateServerChain creates the apiservers connected via delegation. func CreateServerChain(completedOptions completedServerRunOptions, stopCh 4.2. PrepareRun PrepareRun主要执行一些API安装操作。 此部分的代码位于vendor/k8s.io/apiserver/pkg/server/genericapiserver.go // PrepareRun does post API installation setup steps. func (s *GenericAPIServer) PrepareRun() preparedGenericAPIServer { if s.swaggerConfig != nil { routes.Swagger{Config: s.swaggerConfig}.Install(s.Handler.GoRestfulContainer) } if s.openAPIConfig != nil { routes.OpenAPI{ Config: s.openAPIConfig, }.Install(s.Handler.GoRestfulContainer, s.Handler.NonGoRestfulMux) } s.installHealthz() // Register audit backend preShutdownHook. if s.AuditBackend != nil { s.AddPreShutdownHook(\"audit-backend\", func() error { s.AuditBackend.Shutdown() return nil }) } return preparedGenericAPIServer{s} } 4.3. preparedGenericAPIServer.Run preparedGenericAPIServer.Run运行一个安全的http server。具体的实现逻辑待后续文章分析。 此部分代码位于vendor/k8s.io/apiserver/pkg/server/genericapiserver.go // Run spawns the secure http server. It only returns if stopCh is closed // or the secure port cannot be listened on initially. func (s preparedGenericAPIServer) Run(stopCh 核心函数： err := s.NonBlockingRun(stopCh) preparedGenericAPIServer.Run主要是调用NonBlockingRun函数，最终运行一个http server。该部分逻辑待后续文章分析。 5. 总结 NewAPIServerCommand采用了Cobra命令行框架，该框架使用主要包含以下部分： 构造option参数，提供给执行主体(例如 本文的server)作为配置参数使用。 添加Flags，主要用来通过传入的flags参数最终解析成option中使用的结构体属性。 执行Run函数，执行主体的运行逻辑部分（核心部分）。 其中Run函数的主要内容如下： 构造一个聚合的server结构体。 执行PrepareRun。 最终执行preparedGenericAPIServer.Run。 preparedGenericAPIServer.Run主要是调用NonBlockingRun函数，最终运行一个http server。NonBlockingRun的具体逻辑待后续文章再单独分析。 参考： https://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kube-apiserver https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-apiserver/app/server.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-apiserver/app/aggregator.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-apiserver/app/options/options.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kube-controller-manager/controller-manager-xmind.html":{"url":"kube-controller-manager/controller-manager-xmind.html","title":"源码思维导图","keywords":"","body":"controller-manager 源码思维导图 源码整体结构图 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2021-04-18 13:05:27 "},"kube-controller-manager/NewControllerManagerCommand.html":{"url":"kube-controller-manager/NewControllerManagerCommand.html","title":"NewControllerManagerCommand","keywords":"","body":"kube-controller-manager源码分析（一）之 NewControllerManagerCommand 以下代码分析基于 kubernetes v1.12.0 版本。 本文主要分析https://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kube-controller-manager 部分的代码。 本文主要分析 kubernetes/cmd/kube-controller-manager部分，该部分主要涉及各种类型的controller的参数解析，及初始化，例如 deployment controller 和statefulset controller。并没有具体controller运行的详细逻辑，该部分位于kubernetes/pkg/controller模块，待后续文章分析。 kube-controller-manager的cmd部分代码目录结构如下： kube-controller-manager ├── app │ ├── apps.go # 包含:startDeploymentController、startReplicaSetController、startStatefulSetController、startDaemonSetController │ ├── autoscaling.go # startHPAController │ ├── batch.go # startJobController、startCronJobController │ ├── bootstrap.go │ ├── certificates.go │ ├── cloudproviders.go │ ├── config │ │ └── config.go # config: controller manager执行的上下文 │ ├── controllermanager.go # 包含:NewControllerManagerCommand、Run、NewControllerInitializers、StartControllers等 │ ├── core.go # startServiceController、startNodeIpamController、startPersistentVolumeBinderController、startNamespaceController等 │ ├── options # 包含不同controller的option参数 │ │ ├── attachdetachcontroller.go │ │ ├── csrsigningcontroller.go │ │ ├── daemonsetcontroller.go # DaemonSetControllerOptions │ │ ├── deploymentcontroller.go # DeploymentControllerOptions │ │ ├── deprecatedcontroller.go │ │ ├── endpointcontroller.go │ │ ├── garbagecollectorcontroller.go │ │ ├── hpacontroller.go │ │ ├── jobcontroller.go │ │ ├── namespacecontroller.go # NamespaceControllerOptions │ │ ├── nodeipamcontroller.go │ │ ├── nodelifecyclecontroller.go │ │ ├── options.go # KubeControllerManagerOptions、NewKubeControllerManagerOptions │ │ ├── persistentvolumebindercontroller.go │ │ ├── podgccontroller.go │ │ ├── replicasetcontroller.go # ReplicaSetControllerOptions │ │ ├── replicationcontroller.go │ │ ├── resourcequotacontroller.go │ │ ├── serviceaccountcontroller.go │ │ └── ttlafterfinishedcontroller.go └── controller-manager.go # main入口函数 1. Main函数 kube-controller-manager的入口函数Main函数，仍然是采用统一的代码风格，使用Cobra命令行框架。 func main() { rand.Seed(time.Now().UTC().UnixNano()) command := app.NewControllerManagerCommand() // TODO: once we switch everything over to Cobra commands, we can go back to calling // utilflag.InitFlags() (by removing its pflag.Parse() call). For now, we have to set the // normalize func and add the go flag set by hand. pflag.CommandLine.SetNormalizeFunc(utilflag.WordSepNormalizeFunc) pflag.CommandLine.AddGoFlagSet(goflag.CommandLine) // utilflag.InitFlags() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } 核心代码： // 初始化命令行结构体 command := app.NewControllerManagerCommand() // 执行Execute err := command.Execute() 2. NewControllerManagerCommand 该部分代码位于：kubernetes/cmd/kube-controller-manager/app/controllermanager.go // NewControllerManagerCommand creates a *cobra.Command object with default parameters func NewControllerManagerCommand() *cobra.Command { ... cmd := &cobra.Command{ Use: \"kube-controller-manager\", Long: `The Kubernetes controller manager is a daemon that embeds the core control loops shipped with Kubernetes. In applications of robotics and automation, a control loop is a non-terminating loop that regulates the state of the system. In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state. Examples of controllers that ship with Kubernetes today are the replication controller, endpoints controller, namespace controller, and serviceaccounts controller.`, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) c, err := s.Config(KnownControllers(), ControllersDisabledByDefault.List()) if err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } if err := Run(c.Complete(), wait.NeverStop); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } }, } ... } 构建一个*cobra.Command对象，然后执行Run函数。 2.1. NewKubeControllerManagerOptions s, err := options.NewKubeControllerManagerOptions() if err != nil { glog.Fatalf(\"unable to initialize command options: %v\", err) } 初始化controllerManager的参数，其中主要包括了各种controller的option，例如DeploymentControllerOptions: // DeploymentControllerOptions holds the DeploymentController options. type DeploymentControllerOptions struct { ConcurrentDeploymentSyncs int32 DeploymentControllerSyncPeriod metav1.Duration } 具体代码如下： // NewKubeControllerManagerOptions creates a new KubeControllerManagerOptions with a default config. func NewKubeControllerManagerOptions() (*KubeControllerManagerOptions, error) { componentConfig, err := NewDefaultComponentConfig(ports.InsecureKubeControllerManagerPort) if err != nil { return nil, err } s := KubeControllerManagerOptions{ Generic: cmoptions.NewGenericControllerManagerConfigurationOptions(componentConfig.Generic), KubeCloudShared: cmoptions.NewKubeCloudSharedOptions(componentConfig.KubeCloudShared), AttachDetachController: &AttachDetachControllerOptions{ ReconcilerSyncLoopPeriod: componentConfig.AttachDetachController.ReconcilerSyncLoopPeriod, }, CSRSigningController: &CSRSigningControllerOptions{ ClusterSigningCertFile: componentConfig.CSRSigningController.ClusterSigningCertFile, ClusterSigningKeyFile: componentConfig.CSRSigningController.ClusterSigningKeyFile, ClusterSigningDuration: componentConfig.CSRSigningController.ClusterSigningDuration, }, DaemonSetController: &DaemonSetControllerOptions{ ConcurrentDaemonSetSyncs: componentConfig.DaemonSetController.ConcurrentDaemonSetSyncs, }, DeploymentController: &DeploymentControllerOptions{ ConcurrentDeploymentSyncs: componentConfig.DeploymentController.ConcurrentDeploymentSyncs, DeploymentControllerSyncPeriod: componentConfig.DeploymentController.DeploymentControllerSyncPeriod, }, DeprecatedFlags: &DeprecatedControllerOptions{ RegisterRetryCount: componentConfig.DeprecatedController.RegisterRetryCount, }, EndpointController: &EndpointControllerOptions{ ConcurrentEndpointSyncs: componentConfig.EndpointController.ConcurrentEndpointSyncs, }, GarbageCollectorController: &GarbageCollectorControllerOptions{ ConcurrentGCSyncs: componentConfig.GarbageCollectorController.ConcurrentGCSyncs, EnableGarbageCollector: componentConfig.GarbageCollectorController.EnableGarbageCollector, }, HPAController: &HPAControllerOptions{ HorizontalPodAutoscalerSyncPeriod: componentConfig.HPAController.HorizontalPodAutoscalerSyncPeriod, HorizontalPodAutoscalerUpscaleForbiddenWindow: componentConfig.HPAController.HorizontalPodAutoscalerUpscaleForbiddenWindow, HorizontalPodAutoscalerDownscaleForbiddenWindow: componentConfig.HPAController.HorizontalPodAutoscalerDownscaleForbiddenWindow, HorizontalPodAutoscalerDownscaleStabilizationWindow: componentConfig.HPAController.HorizontalPodAutoscalerDownscaleStabilizationWindow, HorizontalPodAutoscalerCPUInitializationPeriod: componentConfig.HPAController.HorizontalPodAutoscalerCPUInitializationPeriod, HorizontalPodAutoscalerInitialReadinessDelay: componentConfig.HPAController.HorizontalPodAutoscalerInitialReadinessDelay, HorizontalPodAutoscalerTolerance: componentConfig.HPAController.HorizontalPodAutoscalerTolerance, HorizontalPodAutoscalerUseRESTClients: componentConfig.HPAController.HorizontalPodAutoscalerUseRESTClients, }, JobController: &JobControllerOptions{ ConcurrentJobSyncs: componentConfig.JobController.ConcurrentJobSyncs, }, NamespaceController: &NamespaceControllerOptions{ NamespaceSyncPeriod: componentConfig.NamespaceController.NamespaceSyncPeriod, ConcurrentNamespaceSyncs: componentConfig.NamespaceController.ConcurrentNamespaceSyncs, }, NodeIPAMController: &NodeIPAMControllerOptions{ NodeCIDRMaskSize: componentConfig.NodeIPAMController.NodeCIDRMaskSize, }, NodeLifecycleController: &NodeLifecycleControllerOptions{ EnableTaintManager: componentConfig.NodeLifecycleController.EnableTaintManager, NodeMonitorGracePeriod: componentConfig.NodeLifecycleController.NodeMonitorGracePeriod, NodeStartupGracePeriod: componentConfig.NodeLifecycleController.NodeStartupGracePeriod, PodEvictionTimeout: componentConfig.NodeLifecycleController.PodEvictionTimeout, }, PersistentVolumeBinderController: &PersistentVolumeBinderControllerOptions{ PVClaimBinderSyncPeriod: componentConfig.PersistentVolumeBinderController.PVClaimBinderSyncPeriod, VolumeConfiguration: componentConfig.PersistentVolumeBinderController.VolumeConfiguration, }, PodGCController: &PodGCControllerOptions{ TerminatedPodGCThreshold: componentConfig.PodGCController.TerminatedPodGCThreshold, }, ReplicaSetController: &ReplicaSetControllerOptions{ ConcurrentRSSyncs: componentConfig.ReplicaSetController.ConcurrentRSSyncs, }, ReplicationController: &ReplicationControllerOptions{ ConcurrentRCSyncs: componentConfig.ReplicationController.ConcurrentRCSyncs, }, ResourceQuotaController: &ResourceQuotaControllerOptions{ ResourceQuotaSyncPeriod: componentConfig.ResourceQuotaController.ResourceQuotaSyncPeriod, ConcurrentResourceQuotaSyncs: componentConfig.ResourceQuotaController.ConcurrentResourceQuotaSyncs, }, SAController: &SAControllerOptions{ ConcurrentSATokenSyncs: componentConfig.SAController.ConcurrentSATokenSyncs, }, ServiceController: &cmoptions.ServiceControllerOptions{ ConcurrentServiceSyncs: componentConfig.ServiceController.ConcurrentServiceSyncs, }, TTLAfterFinishedController: &TTLAfterFinishedControllerOptions{ ConcurrentTTLSyncs: componentConfig.TTLAfterFinishedController.ConcurrentTTLSyncs, }, SecureServing: apiserveroptions.NewSecureServingOptions().WithLoopback(), InsecureServing: (&apiserveroptions.DeprecatedInsecureServingOptions{ BindAddress: net.ParseIP(componentConfig.Generic.Address), BindPort: int(componentConfig.Generic.Port), BindNetwork: \"tcp\", }).WithLoopback(), Authentication: apiserveroptions.NewDelegatingAuthenticationOptions(), Authorization: apiserveroptions.NewDelegatingAuthorizationOptions(), } s.Authentication.RemoteKubeConfigFileOptional = true s.Authorization.RemoteKubeConfigFileOptional = true s.Authorization.AlwaysAllowPaths = []string{\"/healthz\"} s.SecureServing.ServerCert.CertDirectory = \"/var/run/kubernetes\" s.SecureServing.ServerCert.PairName = \"kube-controller-manager\" s.SecureServing.BindPort = ports.KubeControllerManagerPort gcIgnoredResources := make([]kubectrlmgrconfig.GroupResource, 0, len(garbagecollector.DefaultIgnoredResources())) for r := range garbagecollector.DefaultIgnoredResources() { gcIgnoredResources = append(gcIgnoredResources, kubectrlmgrconfig.GroupResource{Group: r.Group, Resource: r.Resource}) } s.GarbageCollectorController.GCIgnoredResources = gcIgnoredResources return &s, nil } 2.2. AddFlagSet 添加参数及帮助函数。 fs := cmd.Flags() namedFlagSets := s.Flags(KnownControllers(), ControllersDisabledByDefault.List()) for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } usageFmt := \"Usage:\\n %s\\n\" cols, _, _ := apiserverflag.TerminalSize(cmd.OutOrStdout()) cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine()) apiserverflag.PrintSections(cmd.OutOrStderr(), namedFlagSets, cols) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine()) apiserverflag.PrintSections(cmd.OutOrStdout(), namedFlagSets, cols) }) 3. Run 此部分的代码位于cmd/kube-controller-manager/app/controllermanager.go 基于KubeControllerManagerOptions运行controllerManager，不退出。 // Run runs the KubeControllerManagerOptions. This should never exit. func Run(c *config.CompletedConfig, stopCh Run函数涉及的核心代码如下： // 创建controller的context controllerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done()) // 启动各种controller err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux) 其中StartControllers中的入参NewControllerInitializers初始化了各种controller。 3.1. CreateControllerContext CreateControllerContext构建了各种controller所需的资源的上下文，各种controller在启动时，入参为该context，具体参考initFn(ctx)。 // CreateControllerContext creates a context struct containing references to resources needed by the // controllers such as the cloud provider and clientBuilder. rootClientBuilder is only used for // the shared-informers client and token controller. func CreateControllerContext(s *config.CompletedConfig, rootClientBuilder, clientBuilder controller.ControllerClientBuilder, stop 核心代码为NewSharedInformerFactory。 // 创建SharedInformerFactory sharedInformers := informers.NewSharedInformerFactory(versionedClient, ResyncPeriod(s)()) // 赋值给ControllerContext ctx := ControllerContext{ InformerFactory: sharedInformers, } SharedInformerFactory提供了公共的k8s对象的informers。 // SharedInformerFactory provides shared informers for resources in all known // API group versions. type SharedInformerFactory interface { internalinterfaces.SharedInformerFactory ForResource(resource schema.GroupVersionResource) (GenericInformer, error) WaitForCacheSync(stopCh 3.2. NewControllerInitializers NewControllerInitializers定义了各种controller的类型和其对于的启动函数，例如deployment``、statefulset、replicaset、replicationcontroller、namespace等。 // NewControllerInitializers is a public map of named controller groups (you can start more than one in an init func) // paired to their InitFunc. This allows for structured downstream composition and subdivision. func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc { controllers := map[string]InitFunc{} controllers[\"endpoint\"] = startEndpointController controllers[\"replicationcontroller\"] = startReplicationController controllers[\"podgc\"] = startPodGCController controllers[\"resourcequota\"] = startResourceQuotaController controllers[\"namespace\"] = startNamespaceController controllers[\"serviceaccount\"] = startServiceAccountController controllers[\"garbagecollector\"] = startGarbageCollectorController controllers[\"daemonset\"] = startDaemonSetController controllers[\"job\"] = startJobController controllers[\"deployment\"] = startDeploymentController controllers[\"replicaset\"] = startReplicaSetController controllers[\"horizontalpodautoscaling\"] = startHPAController controllers[\"disruption\"] = startDisruptionController controllers[\"statefulset\"] = startStatefulSetController controllers[\"cronjob\"] = startCronJobController controllers[\"csrsigning\"] = startCSRSigningController controllers[\"csrapproving\"] = startCSRApprovingController controllers[\"csrcleaner\"] = startCSRCleanerController controllers[\"ttl\"] = startTTLController controllers[\"bootstrapsigner\"] = startBootstrapSignerController controllers[\"tokencleaner\"] = startTokenCleanerController controllers[\"nodeipam\"] = startNodeIpamController if loopMode == IncludeCloudLoops { controllers[\"service\"] = startServiceController controllers[\"route\"] = startRouteController // TODO: volume controller into the IncludeCloudLoops only set. // TODO: Separate cluster in cloud check from node lifecycle controller. } controllers[\"nodelifecycle\"] = startNodeLifecycleController controllers[\"persistentvolume-binder\"] = startPersistentVolumeBinderController controllers[\"attachdetach\"] = startAttachDetachController controllers[\"persistentvolume-expander\"] = startVolumeExpandController controllers[\"clusterrole-aggregation\"] = startClusterRoleAggregrationController controllers[\"pvc-protection\"] = startPVCProtectionController controllers[\"pv-protection\"] = startPVProtectionController controllers[\"ttl-after-finished\"] = startTTLAfterFinishedController return controllers } 3.3. StartControllers func StartControllers(ctx ControllerContext, startSATokenController InitFunc, controllers map[string]InitFunc, unsecuredMux *mux.PathRecorderMux) error { ... for controllerName, initFn := range controllers { if !ctx.IsControllerEnabled(controllerName) { glog.Warningf(\"%q is disabled\", controllerName) continue } time.Sleep(wait.Jitter(ctx.ComponentConfig.Generic.ControllerStartInterval.Duration, ControllerStartJitter)) glog.V(1).Infof(\"Starting %q\", controllerName) debugHandler, started, err := initFn(ctx) if err != nil { glog.Errorf(\"Error starting %q\", controllerName) return err } if !started { glog.Warningf(\"Skipping %q\", controllerName) continue } if debugHandler != nil && unsecuredMux != nil { basePath := \"/debug/controllers/\" + controllerName unsecuredMux.UnlistedHandle(basePath, http.StripPrefix(basePath, debugHandler)) unsecuredMux.UnlistedHandlePrefix(basePath+\"/\", http.StripPrefix(basePath, debugHandler)) } glog.Infof(\"Started %q\", controllerName) } return nil } 核心代码： for controllerName, initFn := range controllers { debugHandler, started, err := initFn(ctx) } 启动各种controller，controller的启动函数在NewControllerInitializers中定义了，例如： // deployment controllers[\"deployment\"] = startDeploymentController // statefulset controllers[\"statefulset\"] = startStatefulSetController 3.4. InformerFactory.Start InformerFactory实际上是SharedInformerFactory，具体的实现逻辑在client-go中的informer的实现机制。 controllerContext.InformerFactory.Start(controllerContext.Stop) close(controllerContext.InformersStarted) 3.4.1. SharedInformerFactory SharedInformerFactory是一个informer工厂的接口定义。 // SharedInformerFactory a small interface to allow for adding an informer without an import cycle type SharedInformerFactory interface { Start(stopCh 3.4.2. sharedInformerFactory.Start Start方法初始化各种类型的informer // Start initializes all requested informers. func (f *sharedInformerFactory) Start(stopCh 3.4.3. sharedIndexInformer.Run sharedIndexInformer.Run具体运行了sharedIndexInformer的实现逻辑，该部分待后续对informer机制做专题分析。 func (s *sharedIndexInformer) Run(stopCh 4. initFn(ctx) initFn实际调用的就是各种类型的controller，代码位于kubernetes/cmd/kube-controller-manager/app/apps.go，本文以startStatefulSetController和startDeploymentController为例，controller中实际调用的函数逻辑位于kubernetes/pkg/controller中，待后续分析。 4.1. startStatefulSetController func startStatefulSetController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \"apps\", Version: \"v1\", Resource: \"statefulsets\"}] { return nil, false, nil } go statefulset.NewStatefulSetController( ctx.InformerFactory.Core().V1().Pods(), ctx.InformerFactory.Apps().V1().StatefulSets(), ctx.InformerFactory.Core().V1().PersistentVolumeClaims(), ctx.InformerFactory.Apps().V1().ControllerRevisions(), ctx.ClientBuilder.ClientOrDie(\"statefulset-controller\"), ).Run(1, ctx.Stop) return nil, true, nil } 其中使用到了InformerFactory，包含了Pods、StatefulSets、PersistentVolumeClaims、ControllerRevisions的informer。 startStatefulSetController主要调用的函数为NewStatefulSetController和对应的Run函数。 4.2. startDeploymentController func startDeploymentController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \"apps\", Version: \"v1\", Resource: \"deployments\"}] { return nil, false, nil } dc, err := deployment.NewDeploymentController( ctx.InformerFactory.Apps().V1().Deployments(), ctx.InformerFactory.Apps().V1().ReplicaSets(), ctx.InformerFactory.Core().V1().Pods(), ctx.ClientBuilder.ClientOrDie(\"deployment-controller\"), ) if err != nil { return nil, true, fmt.Errorf(\"error creating Deployment controller: %v\", err) } go dc.Run(int(ctx.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs), ctx.Stop) return nil, true, nil } startDeploymentController主要调用的函数为NewDeploymentController和对应的Run函数。该部分逻辑在kubernetes/pkg/controller中。 5. 总结 Kube-controller-manager的代码风格仍然是Cobra命令行框架。通过构造ControllerManagerCommand，然后执行command.Execute()函数。基本的流程就是构造option，添加Flags，执行Run函数。 cmd部分的调用流程如下：Main-->NewControllerManagerCommand--> Run(c.Complete(), wait.NeverStop)-->StartControllers-->initFn(ctx)-->startDeploymentController/startStatefulSetController-->sts.NewStatefulSetController.Run/dc.NewDeploymentController.Run-->pkg/controller。 其中CreateControllerContext函数用来创建各类型controller所需要使用的context，NewControllerInitializers初始化了各种类型的controller，其中就包括DeploymentController和StatefulSetController等。 基本流程如下： 构造controller manager option，并转化为Config对象，执行Run函数。 基于Config对象创建ControllerContext，其中包含InformerFactory。 基于ControllerContext运行各种controller，各种controller的定义在NewControllerInitializers中。 执行InformerFactory.Start。 每种controller都会构造自身的结构体并执行对应的Run函数。 参考： https://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kube-controller-manager https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-controller-manager/controller-manager.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-controller-manager/app/controllermanager.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-controller-manager/app/apps.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kube-controller-manager/deployment-controller.html":{"url":"kube-controller-manager/deployment-controller.html","title":"DeploymentController","keywords":"","body":"kube-controller-manager源码分析（二）之 DeploymentController 以下代码分析基于 kubernetes v1.12.0 版本。 本文主要以deployment controller为例，分析该类controller的运行逻辑。此部分代码主要为位于pkg/controller/deployment。pkg/controller部分的代码包括了各种类型的controller的具体实现。 controller manager的pkg部分代码目录结构如下： controller # 主要包含各种controller的具体实现 ├── apis ├── bootstrap ├── certificates ├── client_builder.go ├── cloud ├── clusterroleaggregation ├── controller_ref_manager.go ├── controller_utils.go # WaitForCacheSync ├── cronjob ├── daemon ├── deployment # deployment controller │ ├── deployment_controller.go # NewDeploymentController、Run、syncDeployment │ ├── progress.go # syncRolloutStatus │ ├── recreate.go # rolloutRecreate │ ├── rollback.go # rollback │ ├── rolling.go # rolloutRolling │ ├── sync.go ├── disruption # disruption controller ├── endpoint ├── garbagecollector ├── history ├── job ├── lookup_cache.go ├── namespace # namespace controller ├── nodeipam ├── nodelifecycle ├── podautoscaler ├── podgc ├── replicaset # replicaset controller ├── replication # replication controller ├── resourcequota ├── route ├── service # service controller ├── serviceaccount ├── statefulset # statefulset controller └── volume # PersistentVolumeController、AttachDetachController、PVCProtectionController 1. startDeploymentController func startDeploymentController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \"apps\", Version: \"v1\", Resource: \"deployments\"}] { return nil, false, nil } dc, err := deployment.NewDeploymentController( ctx.InformerFactory.Apps().V1().Deployments(), ctx.InformerFactory.Apps().V1().ReplicaSets(), ctx.InformerFactory.Core().V1().Pods(), ctx.ClientBuilder.ClientOrDie(\"deployment-controller\"), ) if err != nil { return nil, true, fmt.Errorf(\"error creating Deployment controller: %v\", err) } go dc.Run(int(ctx.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs), ctx.Stop) return nil, true, nil } startDeploymentController主要调用的函数为NewDeploymentController和对应的Run函数。该部分逻辑在kubernetes/pkg/controller中。 2. NewDeploymentController NewDeploymentController主要构建DeploymentController结构体。 该部分主要处理了以下逻辑： 构建并运行事件处理器eventBroadcaster。 初始化赋值rsControl、clientset、workqueue。 添加dInformer、rsInformer、podInformer的ResourceEventHandlerFuncs，其中主要为AddFunc、UpdateFunc、DeleteFunc三类方法。 构造deployment、rs、pod的Informer的Lister函数和HasSynced函数。 调用syncHandler，来实现syncDeployment。 2.1. eventBroadcaster 调用事件处理器来记录deployment相关的事件。 eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(glog.Infof) // TODO: remove the wrapper when every clients have moved to use the clientset. eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: v1core.New(client.CoreV1().RESTClient()).Events(\"\")}) 2.2. rsControl 构造DeploymentController，包括clientset、workqueue和rsControl。其中rsControl是具体实现rs逻辑的controller。 dc := &DeploymentController{ client: client, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"deployment-controller\"}), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"deployment\"), } dc.rsControl = controller.RealRSControl{ KubeClient: client, Recorder: dc.eventRecorder, } 2.3. Informer().AddEventHandler 添加dInformer、rsInformer、podInformer的ResourceEventHandlerFuncs，其中主要为AddFunc、UpdateFunc、DeleteFunc三类方法。 dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addDeployment, UpdateFunc: dc.updateDeployment, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: dc.deleteDeployment, }) rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addReplicaSet, UpdateFunc: dc.updateReplicaSet, DeleteFunc: dc.deleteReplicaSet, }) podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: dc.deletePod, }) 2.4. Informer.Lister() 调用dInformer、rsInformer和podInformer的Lister()方法。 dc.dLister = dInformer.Lister() dc.rsLister = rsInformer.Lister() dc.podLister = podInformer.Lister() 2.5. Informer().HasSynced 调用Informer().HasSynced，判断是否缓存完成； dc.dListerSynced = dInformer.Informer().HasSynced dc.rsListerSynced = rsInformer.Informer().HasSynced dc.podListerSynced = podInformer.Informer().HasSynced 2.6. syncHandler syncHandler具体为syncDeployment，syncHandler负责deployment的同步实现。 dc.syncHandler = dc.syncDeployment dc.enqueueDeployment = dc.enqueue 完整代码如下： // NewDeploymentController creates a new DeploymentController. func NewDeploymentController(dInformer extensionsinformers.DeploymentInformer, rsInformer extensionsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(glog.Infof) // TODO: remove the wrapper when every clients have moved to use the clientset. eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: v1core.New(client.CoreV1().RESTClient()).Events(\"\")}) if client != nil && client.CoreV1().RESTClient().GetRateLimiter() != nil { if err := metrics.RegisterMetricAndTrackRateLimiterUsage(\"deployment_controller\", client.CoreV1().RESTClient().GetRateLimiter()); err != nil { return nil, err } } dc := &DeploymentController{ client: client, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"deployment-controller\"}), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"deployment\"), } dc.rsControl = controller.RealRSControl{ KubeClient: client, Recorder: dc.eventRecorder, } dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addDeployment, UpdateFunc: dc.updateDeployment, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: dc.deleteDeployment, }) rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addReplicaSet, UpdateFunc: dc.updateReplicaSet, DeleteFunc: dc.deleteReplicaSet, }) podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: dc.deletePod, }) dc.syncHandler = dc.syncDeployment dc.enqueueDeployment = dc.enqueue dc.dLister = dInformer.Lister() dc.rsLister = rsInformer.Lister() dc.podLister = podInformer.Lister() dc.dListerSynced = dInformer.Informer().HasSynced dc.rsListerSynced = rsInformer.Informer().HasSynced dc.podListerSynced = podInformer.Informer().HasSynced return dc, nil } 3. DeploymentController.Run Run执行watch和sync的操作。 // Run begins watching and syncing. func (dc *DeploymentController) Run(workers int, stopCh 3.1. WaitForCacheSync WaitForCacheSync主要是用来在List-Watch机制中可以保持当前cache的数据与etcd的数据一致。 // WaitForCacheSync is a wrapper around cache.WaitForCacheSync that generates log messages // indicating that the controller identified by controllerName is waiting for syncs, followed by // either a successful or failed sync. func WaitForCacheSync(controllerName string, stopCh 3.2. dc.worker worker调用了processNextWorkItem，processNextWorkItem最终调用了syncHandler，而syncHandler在NewDeploymentController中赋值的具体函数为syncDeployment。 // worker runs a worker thread that just dequeues items, processes them, and marks them done. // It enforces that the syncHandler is never invoked concurrently with the same key. func (dc *DeploymentController) worker() { for dc.processNextWorkItem() { } } func (dc *DeploymentController) processNextWorkItem() bool { key, quit := dc.queue.Get() if quit { return false } defer dc.queue.Done(key) err := dc.syncHandler(key.(string)) dc.handleErr(err, key) return true } NewDeploymentController中的syncHandler赋值： func NewDeploymentController(dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { ... dc.syncHandler = dc.syncDeployment ... } 4. syncDeployment syncDeployment基于给定的key执行sync deployment的操作。 主要流程如下： 通过SplitMetaNamespaceKey获取namespace和deployment对象的name。 调用Lister的接口获取的deployment的对象。 getReplicaSetsForDeployment获取deployment管理的ReplicaSet对象。 getPodMapForDeployment获取deployment管理的pod，基于ReplicaSet来分组。 checkPausedConditions检查deployment是否是pause状态并添加合适的condition。 isScalingEvent检查deployment的更新是否来自于一个scale的事件，如果是则执行scale的操作。 根据DeploymentStrategyType类型执行rolloutRecreate或rolloutRolling。 完整代码如下： // syncDeployment will sync the deployment with the given key. // This function is not meant to be invoked concurrently with the same key. func (dc *DeploymentController) syncDeployment(key string) error { startTime := time.Now() glog.V(4).Infof(\"Started syncing deployment %q (%v)\", key, startTime) defer func() { glog.V(4).Infof(\"Finished syncing deployment %q (%v)\", key, time.Since(startTime)) }() namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil { return err } deployment, err := dc.dLister.Deployments(namespace).Get(name) if errors.IsNotFound(err) { glog.V(2).Infof(\"Deployment %v has been deleted\", key) return nil } if err != nil { return err } // Deep-copy otherwise we are mutating our cache. // TODO: Deep-copy only when needed. d := deployment.DeepCopy() everything := metav1.LabelSelector{} if reflect.DeepEqual(d.Spec.Selector, &everything) { dc.eventRecorder.Eventf(d, v1.EventTypeWarning, \"SelectingAll\", \"This deployment is selecting all pods. A non-empty selector is required.\") if d.Status.ObservedGeneration 4.1. Get deployment // get namespace and deployment name namespace, name, err := cache.SplitMetaNamespaceKey(key) // get deployment by name deployment, err := dc.dLister.Deployments(namespace).Get(name) 4.2. getReplicaSetsForDeployment // List ReplicaSets owned by this Deployment, while reconciling ControllerRef // through adoption/orphaning. rsList, err := dc.getReplicaSetsForDeployment(d) getReplicaSetsForDeployment具体代码: // getReplicaSetsForDeployment uses ControllerRefManager to reconcile // ControllerRef by adopting and orphaning. // It returns the list of ReplicaSets that this Deployment should manage. func (dc *DeploymentController) getReplicaSetsForDeployment(d *apps.Deployment) ([]*apps.ReplicaSet, error) { // List all ReplicaSets to find those we own but that no longer match our // selector. They will be orphaned by ClaimReplicaSets(). rsList, err := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything()) if err != nil { return nil, err } deploymentSelector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector) if err != nil { return nil, fmt.Errorf(\"deployment %s/%s has invalid label selector: %v\", d.Namespace, d.Name, err) } // If any adoptions are attempted, we should first recheck for deletion with // an uncached quorum read sometime after listing ReplicaSets (see #42639). canAdoptFunc := controller.RecheckDeletionTimestamp(func() (metav1.Object, error) { fresh, err := dc.client.AppsV1().Deployments(d.Namespace).Get(d.Name, metav1.GetOptions{}) if err != nil { return nil, err } if fresh.UID != d.UID { return nil, fmt.Errorf(\"original Deployment %v/%v is gone: got uid %v, wanted %v\", d.Namespace, d.Name, fresh.UID, d.UID) } return fresh, nil }) cm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc) return cm.ClaimReplicaSets(rsList) } 4.3. getPodMapForDeployment // List all Pods owned by this Deployment, grouped by their ReplicaSet. // Current uses of the podMap are: // // * check if a Pod is labeled correctly with the pod-template-hash label. // * check that no old Pods are running in the middle of Recreate Deployments. podMap, err := dc.getPodMapForDeployment(d, rsList) getPodMapForDeployment具体代码： // getPodMapForDeployment returns the Pods managed by a Deployment. // // It returns a map from ReplicaSet UID to a list of Pods controlled by that RS, // according to the Pod's ControllerRef. func (dc *DeploymentController) getPodMapForDeployment(d *apps.Deployment, rsList []*apps.ReplicaSet) (map[types.UID]*v1.PodList, error) { // Get all Pods that potentially belong to this Deployment. selector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector) if err != nil { return nil, err } pods, err := dc.podLister.Pods(d.Namespace).List(selector) if err != nil { return nil, err } // Group Pods by their controller (if it's in rsList). podMap := make(map[types.UID]*v1.PodList, len(rsList)) for _, rs := range rsList { podMap[rs.UID] = &v1.PodList{} } for _, pod := range pods { // Do not ignore inactive Pods because Recreate Deployments need to verify that no // Pods from older versions are running before spinning up new Pods. controllerRef := metav1.GetControllerOf(pod) if controllerRef == nil { continue } // Only append if we care about this UID. if podList, ok := podMap[controllerRef.UID]; ok { podList.Items = append(podList.Items, *pod) } } return podMap, nil } 4.4. checkPausedConditions // Update deployment conditions with an Unknown condition when pausing/resuming // a deployment. In this way, we can be sure that we won't timeout when a user // resumes a Deployment with a set progressDeadlineSeconds. if err = dc.checkPausedConditions(d); err != nil { return err } if d.Spec.Paused { return dc.sync(d, rsList) } checkPausedConditions具体代码: // checkPausedConditions checks if the given deployment is paused or not and adds an appropriate condition. // These conditions are needed so that we won't accidentally report lack of progress for resumed deployments // that were paused for longer than progressDeadlineSeconds. func (dc *DeploymentController) checkPausedConditions(d *apps.Deployment) error { if !deploymentutil.HasProgressDeadline(d) { return nil } cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing) if cond != nil && cond.Reason == deploymentutil.TimedOutReason { // If we have reported lack of progress, do not overwrite it with a paused condition. return nil } pausedCondExists := cond != nil && cond.Reason == deploymentutil.PausedDeployReason needsUpdate := false if d.Spec.Paused && !pausedCondExists { condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.PausedDeployReason, \"Deployment is paused\") deploymentutil.SetDeploymentCondition(&d.Status, *condition) needsUpdate = true } else if !d.Spec.Paused && pausedCondExists { condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionUnknown, deploymentutil.ResumedDeployReason, \"Deployment is resumed\") deploymentutil.SetDeploymentCondition(&d.Status, *condition) needsUpdate = true } if !needsUpdate { return nil } var err error d, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(d) return err } 4.5. isScalingEvent scalingEvent, err := dc.isScalingEvent(d, rsList) if err != nil { return err } if scalingEvent { return dc.sync(d, rsList) } isScalingEvent具体代码: // isScalingEvent checks whether the provided deployment has been updated with a scaling event // by looking at the desired-replicas annotation in the active replica sets of the deployment. // // rsList should come from getReplicaSetsForDeployment(d). // podMap should come from getPodMapForDeployment(d, rsList). func (dc *DeploymentController) isScalingEvent(d *apps.Deployment, rsList []*apps.ReplicaSet) (bool, error) { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) if err != nil { return false, err } allRSs := append(oldRSs, newRS) for _, rs := range controller.FilterActiveReplicaSets(allRSs) { desired, ok := deploymentutil.GetDesiredReplicasAnnotation(rs) if !ok { continue } if desired != *(d.Spec.Replicas) { return true, nil } } return false, nil } 4.6. rolloutRecreate switch d.Spec.Strategy.Type { case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) rolloutRecreate具体代码: // rolloutRecreate implements the logic for recreating a replica set. func (dc *DeploymentController) rolloutRecreate(d *apps.Deployment, rsList []*apps.ReplicaSet, podMap map[types.UID]*v1.PodList) error { // Don't create a new RS if not already existed, so that we avoid scaling up before scaling down. newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) if err != nil { return err } allRSs := append(oldRSs, newRS) activeOldRSs := controller.FilterActiveReplicaSets(oldRSs) // scale down old replica sets. scaledDown, err := dc.scaleDownOldReplicaSetsForRecreate(activeOldRSs, d) if err != nil { return err } if scaledDown { // Update DeploymentStatus. return dc.syncRolloutStatus(allRSs, newRS, d) } // Do not process a deployment when it has old pods running. if oldPodsRunning(newRS, oldRSs, podMap) { return dc.syncRolloutStatus(allRSs, newRS, d) } // If we need to create a new RS, create it now. if newRS == nil { newRS, oldRSs, err = dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) if err != nil { return err } allRSs = append(oldRSs, newRS) } // scale up new replica set. if _, err := dc.scaleUpNewReplicaSetForRecreate(newRS, d); err != nil { return err } if util.DeploymentComplete(d, &d.Status) { if err := dc.cleanupDeployment(oldRSs, d); err != nil { return err } } // Sync deployment status. return dc.syncRolloutStatus(allRSs, newRS, d) } 4.7. rolloutRolling switch d.Spec.Strategy.Type { case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) case apps.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(d, rsList) } rolloutRolling具体代码: // rolloutRolling implements the logic for rolling a new replica set. func (dc *DeploymentController) rolloutRolling(d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) if err != nil { return err } allRSs := append(oldRSs, newRS) // Scale up, if we can. scaledUp, err := dc.reconcileNewReplicaSet(allRSs, newRS, d) if err != nil { return err } if scaledUp { // Update DeploymentStatus return dc.syncRolloutStatus(allRSs, newRS, d) } // Scale down, if we can. scaledDown, err := dc.reconcileOldReplicaSets(allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d) if err != nil { return err } if scaledDown { // Update DeploymentStatus return dc.syncRolloutStatus(allRSs, newRS, d) } if deploymentutil.DeploymentComplete(d, &d.Status) { if err := dc.cleanupDeployment(oldRSs, d); err != nil { return err } } // Sync deployment status return dc.syncRolloutStatus(allRSs, newRS, d) } 5. 总结 startDeploymentController主要包括NewDeploymentController和DeploymentController.Run两部分。 NewDeploymentController主要构建DeploymentController结构体。 该部分主要处理了以下逻辑： 构建并运行事件处理器eventBroadcaster。 初始化赋值rsControl、clientset、workqueue。 添加dInformer、rsInformer、podInformer的ResourceEventHandlerFuncs，其中主要为AddFunc、UpdateFunc、DeleteFunc三类方法。 构造deployment、rs、pod的Informer的Lister函数和HasSynced函数。 赋值syncHandler，来实现syncDeployment。 DeploymentController.Run主要包含WaitForCacheSync和syncDeployment两部分。 syncDeployment基于给定的key执行sync deployment的操作。 主要流程如下： 通过SplitMetaNamespaceKey获取namespace和deployment对象的name。 调用Lister的接口获取的deployment的对象。 getReplicaSetsForDeployment获取deployment管理的ReplicaSet对象。 getPodMapForDeployment获取deployment管理的pod，基于ReplicaSet来分组。 checkPausedConditions检查deployment是否是pause状态并添加合适的condition。 isScalingEvent检查deployment的更新是否来自于一个scale的事件，如果是则执行scale的操作。 根据DeploymentStrategyType类型执行rolloutRecreate或rolloutRolling。 参考： https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/controller/deployment/deployment_controller.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/controller/deployment/rolling.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-controller-manager/app/apps.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kube-controller-manager/list-watch/informer.html":{"url":"kube-controller-manager/list-watch/informer.html","title":"Informer原理","keywords":"","body":"kube-controller-manager源码分析（三）之 Informer机制 以下代码分析基于 kubernetes v1.12.0 版本。 list-watch机制 本文主要分析k8s中各个核心组件经常使用到的Informer机制(即List-Watch)。该部分的代码主要位于client-go这个第三方包中。 此部分的逻辑主要位于/vendor/k8s.io/client-go/tools/cache包中，代码目录结构如下： cache ├── controller.go # 包含：Config、Run、processLoop、NewInformer、NewIndexerInformer ├── delta_fifo.go # 包含：NewDeltaFIFO、DeltaFIFO、AddIfNotPresent ├── expiration_cache.go ├── expiration_cache_fakes.go ├── fake_custom_store.go ├── fifo.go # 包含：Queue、FIFO、NewFIFO ├── heap.go ├── index.go # 包含：Indexer、MetaNamespaceIndexFunc ├── listers.go ├── listwatch.go # 包含：ListerWatcher、ListWatch、List、Watch ├── mutation_cache.go ├── mutation_detector.go ├── reflector.go # 包含：Reflector、NewReflector、Run、ListAndWatch ├── reflector_metrics.go ├── shared_informer.go # 包含：NewSharedInformer、WaitForCacheSync、Run、HasSynced ├── store.go # 包含：Store、MetaNamespaceKeyFunc、SplitMetaNamespaceKey ├── testing │ ├── fake_controller_source.go ├── thread_safe_store.go # 包含：ThreadSafeStore、threadSafeMap ├── undelta_store.go 1. 原理示意图 示意图1： 示意图2： 2. 组件 2.1. client-go组件 Reflector：reflector用来watch特定的k8s API资源。具体的实现是通过ListAndWatch的方法，watch可以是k8s内建的资源或者是自定义的资源。当reflector通过watch API接收到有关新资源实例存在的通知时，它使用相应的列表API获取新创建的对象，并将其放入watchHandler函数内的Delta Fifo队列中。 Informer：informer从Delta Fifo队列中弹出对象。执行此操作的功能是processLoop。base controller的作用是保存对象以供以后检索，并调用我们的控制器将对象传递给它。 Indexer：索引器提供对象的索引功能。典型的索引用例是基于对象标签创建索引。 Indexer可以根据多个索引函数维护索引。Indexer使用线程安全的数据存储来存储对象及其键。 在Store中定义了一个名为MetaNamespaceKeyFunc的默认函数，该函数生成对象的键作为该对象的 / 组合。 2.2. 自定义controller组件 Informer reference：指的是Informer实例的引用，定义如何使用自定义资源对象。 自定义控制器代码需要创建对应的Informer。 Indexer reference: 自定义控制器对Indexer实例的引用。自定义控制器需要创建对应的Indexser。 client-go中提供NewIndexerInformer函数可以创建Informer 和 Indexer。 Resource Event Handlers：资源事件回调函数，当它想要将对象传递给控制器时，它将被调用。 编写这些函数的典型模式是获取调度对象的key，并将该key排入工作队列以进行进一步处理。 Work queue：任务队列。 编写资源事件处理程序函数以提取传递的对象的key并将其添加到任务队列。 Process Item：处理任务队列中对象的函数， 这些函数通常使用Indexer引用或Listing包装器来重试与该key对应的对象。 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2021-03-17 19:14:26 "},"kube-controller-manager/list-watch/sharedIndexInformer.html":{"url":"kube-controller-manager/list-watch/sharedIndexInformer.html","title":"sharedIndexInformer","keywords":"","body":"1. sharedInformerFactory.Start 在controller-manager的Run函数部分调用了InformerFactory.Start的方法。 此部分代码位于/cmd/kube-controller-manager/app/controllermanager.go // Run runs the KubeControllerManagerOptions. This should never exit. func Run(c *config.CompletedConfig, stopCh InformerFactory是一个SharedInformerFactory的接口，接口定义如下： 此部分代码位于vendor/k8s.io/client-go/informers/internalinterfaces/factory_interfaces.go // SharedInformerFactory a small interface to allow for adding an informer without an import cycle type SharedInformerFactory interface { Start(stopCh Start方法初始化各种类型的informer，并且每个类型起了个informer.Run的goroutine。 此部分代码位于vendor/k8s.io/client-go/informers/factory.go // Start initializes all requested informers. func (f *sharedInformerFactory) Start(stopCh 2. sharedIndexInformer.Run 此部分的代码位于/vendor/k8s.io/client-go/tools/cache/shared_informer.go func (s *sharedIndexInformer) Run(stopCh 2.1. NewDeltaFIFO DeltaFIFO是一个对象变化的存储队列，依据先进先出的原则，process的函数接收该队列的Pop方法的输出对象来处理相关功能。 fifo := NewDeltaFIFO(MetaNamespaceKeyFunc, nil, s.indexer) 2.2. Config 构造controller的配置文件，构造process，即HandleDeltas，该函数为后面使用到的process函数。 cfg := &Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, } 2.3. controller 调用New(cfg)，构建sharedIndexInformer的controller。 func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true }() 2.4. cacheMutationDetector.Run 调用s.cacheMutationDetector.Run，检查缓存对象是否变化。 wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) defaultCacheMutationDetector.Run func (d *defaultCacheMutationDetector) Run(stopCh CompareObjects func (d *defaultCacheMutationDetector) CompareObjects() { d.lock.Lock() defer d.lock.Unlock() altered := false for i, obj := range d.cachedObjs { if !reflect.DeepEqual(obj.cached, obj.copied) { fmt.Printf(\"CACHE %s[%d] ALTERED!\\n%v\\n\", d.name, i, diff.ObjectDiff(obj.cached, obj.copied)) altered = true } } if altered { msg := fmt.Sprintf(\"cache %s modified\", d.name) if d.failureFunc != nil { d.failureFunc(msg) return } panic(msg) } } 2.5. processor.run 调用s.processor.run，将调用sharedProcessor.run，会调用Listener.run和Listener.pop,执行处理queue的函数。 wg.StartWithChannel(processorStopCh, s.processor.run) sharedProcessor.Run func (p *sharedProcessor) run(stopCh 该部分逻辑待后面分析。 2.6. controller.Run 调用s.controller.Run，构建Reflector，进行对etcd的缓存 defer func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.stopped = true // Don't want any new listeners }() s.controller.Run(stopCh) controller.Run 此部分代码位于/vendor/k8s.io/client-go/tools/cache/controller.go // Run begins processing items, and will continue until a value is sent down stopCh. // It's an error to call Run more than once. // Run blocks; call via go. func (c *controller) Run(stopCh 核心代码： // 构建Reflector r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) // 运行Reflector wg.StartWithChannel(stopCh, r.Run) // 执行processLoop wait.Until(c.processLoop, time.Second, stopCh) Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2021-03-17 19:14:33 "},"kube-controller-manager/list-watch/reflector.html":{"url":"kube-controller-manager/list-watch/reflector.html","title":"Reflector","keywords":"","body":"3. Reflector 3.1. Reflector Reflector的主要作用是watch指定的k8s资源，并将变化同步到本地是store中。Reflector只会放置指定的expectedType类型的资源到store中，除非expectedType为nil。如果resyncPeriod不为零，那么Reflector为以resyncPeriod为周期定期执行list的操作，这样就可以使用Reflector来定期处理所有的对象，也可以逐步处理变化的对象。 常用属性说明： expectedType：期望放入缓存store的资源类型。 store：watch的资源对应的本地缓存。 listerWatcher：list和watch的接口。 period：watch的周期，默认为1秒。 resyncPeriod：resync的周期，当非零的时候，会按该周期执行list。 lastSyncResourceVersion：最新一次看到的资源的版本号，主要在watch时候使用。 // Reflector watches a specified resource and causes all changes to be reflected in the given store. type Reflector struct { // name identifies this reflector. By default it will be a file:line if possible. name string // metrics tracks basic metric information about the reflector metrics *reflectorMetrics // The type of object we expect to place in the store. expectedType reflect.Type // The destination to sync up with the watch source store Store // listerWatcher is used to perform lists and watches. listerWatcher ListerWatcher // period controls timing between one watch ending and // the beginning of the next one. period time.Duration resyncPeriod time.Duration ShouldResync func() bool // clock allows tests to manipulate time clock clock.Clock // lastSyncResourceVersion is the resource version token last // observed when doing a sync with the underlying store // it is thread safe, but not synchronized with the underlying store lastSyncResourceVersion string // lastSyncResourceVersionMutex guards read/write access to lastSyncResourceVersion lastSyncResourceVersionMutex sync.RWMutex } 3.2. NewReflector NewReflector主要用来构建Reflector的结构体。 此部分的代码位于/vendor/k8s.io/client-go/tools/cache/reflector.go // NewReflector creates a new Reflector object which will keep the given store up to // date with the server's contents for the given resource. Reflector promises to // only put things in the store that have the type of expectedType, unless expectedType // is nil. If resyncPeriod is non-zero, then lists will be executed after every // resyncPeriod, so that you can use reflectors to periodically process everything as // well as incrementally processing the things that change. func NewReflector(lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { return NewNamedReflector(getDefaultReflectorName(internalPackages...), lw, expectedType, store, resyncPeriod) } // reflectorDisambiguator is used to disambiguate started reflectors. // initialized to an unstable value to ensure meaning isn't attributed to the suffix. var reflectorDisambiguator = int64(time.Now().UnixNano() % 12345) // NewNamedReflector same as NewReflector, but with a specified name for logging func NewNamedReflector(name string, lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { reflectorSuffix := atomic.AddInt64(&reflectorDisambiguator, 1) r := &Reflector{ name: name, // we need this to be unique per process (some names are still the same)but obvious who it belongs to metrics: newReflectorMetrics(makeValidPromethusMetricLabel(fmt.Sprintf(\"reflector_\"+name+\"_%d\", reflectorSuffix))), listerWatcher: lw, store: store, expectedType: reflect.TypeOf(expectedType), period: time.Second, resyncPeriod: resyncPeriod, clock: &clock.RealClock{}, } return r } 3.3. Reflector.Run Reflector.Run主要执行了ListAndWatch的方法。 // Run starts a watch and handles watch events. Will restart the watch if it is closed. // Run will exit when stopCh is closed. func (r *Reflector) Run(stopCh 3.4. ListAndWatch ListAndWatch第一次会列出所有的对象，并获取资源对象的版本号，然后watch资源对象的版本号来查看是否有被变更。首先会将资源版本号设置为0，list()可能会导致本地的缓存相对于etcd里面的内容存在延迟，Reflector会通过watch的方法将延迟的部分补充上，使得本地的缓存数据与etcd的数据保持一致。 3.4.1. List // ListAndWatch first lists all items and get the resource version at the moment of call, // and then use the resource version to watch. // It returns error if ListAndWatch didn't even try to initialize watch. func (r *Reflector) ListAndWatch(stopCh 首先将资源的版本号设置为0，然后调用listerWatcher.List(options)，列出所有list的内容。 // 版本号设置为0 options := metav1.ListOptions{ResourceVersion: \"0\"} // list接口 list, err := r.listerWatcher.List(options) 获取资源版本号，并将list的内容提取成对象列表。 // 获取版本号 resourceVersion = listMetaInterface.GetResourceVersion() // 将list的内容提取成对象列表 items, err := meta.ExtractList(list) 将list中对象列表的内容和版本号存储到本地的缓存store中，并全量替换已有的store的内容。 err := r.syncWith(items, resourceVersion) syncWith调用了store的Replace的方法来替换原来store中的数据。 // syncWith replaces the store's items with the given list. func (r *Reflector) syncWith(items []runtime.Object, resourceVersion string) error { found := make([]interface{}, 0, len(items)) for _, item := range items { found = append(found, item) } return r.store.Replace(found, resourceVersion) } Store.Replace方法定义如下： type Store interface { ... // Replace will delete the contents of the store, using instead the // given list. Store takes ownership of the list, you should not reference // it after calling this function. Replace([]interface{}, string) error ... } 最后设置最新的资源版本号。 r.setLastSyncResourceVersion(resourceVersion) setLastSyncResourceVersion: func (r *Reflector) setLastSyncResourceVersion(v string) { r.lastSyncResourceVersionMutex.Lock() defer r.lastSyncResourceVersionMutex.Unlock() r.lastSyncResourceVersion = v rv, err := strconv.Atoi(v) if err == nil { r.metrics.lastResourceVersion.Set(float64(rv)) } } 3.4.2. store.Resync resyncerrc := make(chan error, 1) cancelCh := make(chan struct{}) defer close(cancelCh) go func() { resyncCh, cleanup := r.resyncChan() defer func() { cleanup() // Call the last one written into cleanup }() for { select { case 核心代码： err := r.store.Resync() store的具体对象为DeltaFIFO，即调用DeltaFIFO.Resync // Resync will send a sync event for each item func (f *DeltaFIFO) Resync() error { f.lock.Lock() defer f.lock.Unlock() if f.knownObjects == nil { return nil } keys := f.knownObjects.ListKeys() for _, k := range keys { if err := f.syncKeyLocked(k); err != nil { return err } } return nil } 3.4.3. Watch for { // give the stopCh a chance to stop the loop, even in case of continue statements further down on errors select { case 设置watch的超时时间，默认为5分钟。 timemoutseconds := int64(minWatchTimeout.Seconds() * (rand.Float64() + 1.0)) options = metav1.ListOptions{ ResourceVersion: resourceVersion, // We want to avoid situations of hanging watchers. Stop any wachers that do not // receive any events within the timeout window. TimeoutSeconds: &timemoutseconds, } 执行listerWatcher.Watch(options)。 w, err := r.listerWatcher.Watch(options) 执行watchHandler。 err := r.watchHandler(w, &resourceVersion, resyncerrc, stopCh) 3.4.4. watchHandler watchHandler主要是通过watch的方式保证当前的资源版本是最新的。 // watchHandler watches w and keeps *resourceVersion up to date. func (r *Reflector) watchHandler(w watch.Interface, resourceVersion *string, errc chan error, stopCh 获取watch接口中的事件的channel，来获取事件的内容。 for { select { ... case event, ok := 当获得添加、更新、删除的事件时，将对应的对象更新到本地缓存store中。 switch event.Type { case watch.Added: err := r.store.Add(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to add watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Modified: err := r.store.Update(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to update watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Deleted: // TODO: Will any consumers need access to the \"last known // state\", which is passed in event.Object? If so, may need // to change this. err := r.store.Delete(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to delete watch event object (%#v) from store: %v\", r.name, event.Object, err)) } default: utilruntime.HandleError(fmt.Errorf(\"%s: unable to understand watch event %#v\", r.name, event)) } 更新当前的最新版本号。 newResourceVersion := meta.GetResourceVersion() *resourceVersion = newResourceVersion r.setLastSyncResourceVersion(newResourceVersion) 通过对Reflector模块的分析，可以看到多次使用到本地缓存store模块，而store的数据由DeltaFIFO赋值而来，以下针对DeltaFIFO和store做分析。 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2021-03-17 19:08:21 "},"kube-controller-manager/list-watch/DeltaFIFO.html":{"url":"kube-controller-manager/list-watch/DeltaFIFO.html","title":"DeltaFIFO","keywords":"","body":"4. DeltaFIFO DeltaFIFO由NewDeltaFIFO初始化，并赋值给config.Queue。 func (s *sharedIndexInformer) Run(stopCh 4.1. NewDeltaFIFO // NewDeltaFIFO returns a Store which can be used process changes to items. // // keyFunc is used to figure out what key an object should have. (It's // exposed in the returned DeltaFIFO's KeyOf() method, with bonus features.) // // 'compressor' may compress as many or as few items as it wants // (including returning an empty slice), but it should do what it // does quickly since it is called while the queue is locked. // 'compressor' may be nil if you don't want any delta compression. // // 'keyLister' is expected to return a list of keys that the consumer of // this queue \"knows about\". It is used to decide which items are missing // when Replace() is called; 'Deleted' deltas are produced for these items. // It may be nil if you don't need to detect all deletions. // TODO: consider merging keyLister with this object, tracking a list of // \"known\" keys when Pop() is called. Have to think about how that // affects error retrying. // TODO(lavalamp): I believe there is a possible race only when using an // external known object source that the above TODO would // fix. // // Also see the comment on DeltaFIFO. func NewDeltaFIFO(keyFunc KeyFunc, compressor DeltaCompressor, knownObjects KeyListerGetter) *DeltaFIFO { f := &DeltaFIFO{ items: map[string]Deltas{}, queue: []string{}, keyFunc: keyFunc, deltaCompressor: compressor, knownObjects: knownObjects, } f.cond.L = &f.lock return f } controller.Run的部分调用了NewReflector。 func (c *controller) Run(stopCh NewReflector构造函数，将c.config.Queue赋值给Reflector.store的属性。 func NewReflector(lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { return NewNamedReflector(getDefaultReflectorName(internalPackages...), lw, expectedType, store, resyncPeriod) } // NewNamedReflector same as NewReflector, but with a specified name for logging func NewNamedReflector(name string, lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { reflectorSuffix := atomic.AddInt64(&reflectorDisambiguator, 1) r := &Reflector{ name: name, // we need this to be unique per process (some names are still the same)but obvious who it belongs to metrics: newReflectorMetrics(makeValidPromethusMetricLabel(fmt.Sprintf(\"reflector_\"+name+\"_%d\", reflectorSuffix))), listerWatcher: lw, store: store, expectedType: reflect.TypeOf(expectedType), period: time.Second, resyncPeriod: resyncPeriod, clock: &clock.RealClock{}, } return r } 4.2. DeltaFIFO DeltaFIFO是一个生产者与消费者的队列，其中Reflector是生产者，消费者调用Pop()的方法。 DeltaFIFO主要用在以下场景： 希望对象变更最多处理一次 处理对象时，希望查看自上次处理对象以来发生的所有事情 要处理对象的删除 希望定期重新处理对象 // DeltaFIFO is like FIFO, but allows you to process deletes. // // DeltaFIFO is a producer-consumer queue, where a Reflector is // intended to be the producer, and the consumer is whatever calls // the Pop() method. // // DeltaFIFO solves this use case: // * You want to process every object change (delta) at most once. // * When you process an object, you want to see everything // that's happened to it since you last processed it. // * You want to process the deletion of objects. // * You might want to periodically reprocess objects. // // DeltaFIFO's Pop(), Get(), and GetByKey() methods return // interface{} to satisfy the Store/Queue interfaces, but it // will always return an object of type Deltas. // // A note on threading: If you call Pop() in parallel from multiple // threads, you could end up with multiple threads processing slightly // different versions of the same object. // // A note on the KeyLister used by the DeltaFIFO: It's main purpose is // to list keys that are \"known\", for the purpose of figuring out which // items have been deleted when Replace() or Delete() are called. The deleted // object will be included in the DeleteFinalStateUnknown markers. These objects // could be stale. // // You may provide a function to compress deltas (e.g., represent a // series of Updates as a single Update). type DeltaFIFO struct { // lock/cond protects access to 'items' and 'queue'. lock sync.RWMutex cond sync.Cond // We depend on the property that items in the set are in // the queue and vice versa, and that all Deltas in this // map have at least one Delta. items map[string]Deltas queue []string // populated is true if the first batch of items inserted by Replace() has been populated // or Delete/Add/Update was called first. populated bool // initialPopulationCount is the number of items inserted by the first call of Replace() initialPopulationCount int // keyFunc is used to make the key used for queued item // insertion and retrieval, and should be deterministic. keyFunc KeyFunc // deltaCompressor tells us how to combine two or more // deltas. It may be nil. deltaCompressor DeltaCompressor // knownObjects list keys that are \"known\", for the // purpose of figuring out which items have been deleted // when Replace() or Delete() is called. knownObjects KeyListerGetter // Indication the queue is closed. // Used to indicate a queue is closed so a control loop can exit when a queue is empty. // Currently, not used to gate any of CRED operations. closed bool closedLock sync.Mutex } 4.3. Queue & Store DeltaFIFO的类型是Queue接口，Reflector.store是Store接口，Queue接口是一个存储队列，Process的方法执行Queue.Pop出来的数据对象， // Queue is exactly like a Store, but has a Pop() method too. type Queue interface { Store // Pop blocks until it has something to process. // It returns the object that was process and the result of processing. // The PopProcessFunc may return an ErrRequeue{...} to indicate the item // should be requeued before releasing the lock on the queue. Pop(PopProcessFunc) (interface{}, error) // AddIfNotPresent adds a value previously // returned by Pop back into the queue as long // as nothing else (presumably more recent) // has since been added. AddIfNotPresent(interface{}) error // Return true if the first batch of items has been popped HasSynced() bool // Close queue Close() } Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2021-03-17 19:09:17 "},"kube-controller-manager/list-watch/processLoop.html":{"url":"kube-controller-manager/list-watch/processLoop.html","title":"processLoop","keywords":"","body":"6. processLoop func (c *controller) Run(stopCh 在controller.Run方法中会调用processLoop，以下分析processLoop的处理逻辑。 // processLoop drains the work queue. // TODO: Consider doing the processing in parallel. This will require a little thought // to make sure that we don't end up processing the same object multiple times // concurrently. // // TODO: Plumb through the stopCh here (and down to the queue) so that this can // actually exit when the controller is stopped. Or just give up on this stuff // ever being stoppable. Converting this whole package to use Context would // also be helpful. func (c *controller) processLoop() { for { obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil { if err == FIFOClosedError { return } if c.config.RetryOnError { // This is the safe way to re-enqueue. c.config.Queue.AddIfNotPresent(obj) } } } } processLoop主要处理任务队列中的任务，其中处理逻辑是调用具体的ProcessFunc函数来实现，核心代码为： obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) 6.1. DeltaFIFO.Pop Pop会阻塞住直到队列里面添加了新的对象，如果有多个对象，按照先进先出的原则处理，如果某个对象没有处理成功会重新被加入该队列中。 Pop中会调用具体的process函数来处理对象。 // Pop blocks until an item is added to the queue, and then returns it. If // multiple items are ready, they are returned in the order in which they were // added/updated. The item is removed from the queue (and the store) before it // is returned, so if you don't successfully process it, you need to add it back // with AddIfNotPresent(). // process function is called under lock, so it is safe update data structures // in it that need to be in sync with the queue (e.g. knownKeys). The PopProcessFunc // may return an instance of ErrRequeue with a nested error to indicate the current // item should be requeued (equivalent to calling AddIfNotPresent under the lock). // // Pop returns a 'Deltas', which has a complete list of all the things // that happened to the object (deltas) while it was sitting in the queue. func (f *DeltaFIFO) Pop(process PopProcessFunc) (interface{}, error) { f.lock.Lock() defer f.lock.Unlock() for { for len(f.queue) == 0 { // When the queue is empty, invocation of Pop() is blocked until new item is enqueued. // When Close() is called, the f.closed is set and the condition is broadcasted. // Which causes this loop to continue and return from the Pop(). if f.IsClosed() { return nil, FIFOClosedError } f.cond.Wait() } id := f.queue[0] f.queue = f.queue[1:] item, ok := f.items[id] if f.initialPopulationCount > 0 { f.initialPopulationCount-- } if !ok { // Item may have been deleted subsequently. continue } delete(f.items, id) err := process(item) if e, ok := err.(ErrRequeue); ok { f.addIfNotPresent(id, item) err = e.Err } // Don't need to copyDeltas here, because we're transferring // ownership to the caller. return item, err } } 核心代码： for { ... item, ok := f.items[id] ... err := process(item) if e, ok := err.(ErrRequeue); ok { f.addIfNotPresent(id, item) err = e.Err } // Don't need to copyDeltas here, because we're transferring // ownership to the caller. return item, err } 6.2. HandleDeltas cfg := &Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, } 其中process函数就是在sharedIndexInformer.Run方法中，给config.Process赋值的HandleDeltas函数。 func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // from oldest to newest for _, d := range obj.(Deltas) { switch d.Type { case Sync, Added, Updated: isSync := d.Type == Sync s.cacheMutationDetector.AddObject(d.Object) if old, exists, err := s.indexer.Get(d.Object); err == nil && exists { if err := s.indexer.Update(d.Object); err != nil { return err } s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { if err := s.indexer.Add(d.Object); err != nil { return err } s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: if err := s.indexer.Delete(d.Object); err != nil { return err } s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } 核心代码： switch d.Type { case Sync, Added, Updated: ... if old, exists, err := s.indexer.Get(d.Object); err == nil && exists { ... s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { ... s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: ... s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } 根据不同的类型，调用processor.distribute方法，该方法将对象加入processorListener的channel中。 6.3. sharedProcessor.distribute func (p *sharedProcessor) distribute(obj interface{}, sync bool) { p.listenersLock.RLock() defer p.listenersLock.RUnlock() if sync { for _, listener := range p.syncingListeners { listener.add(obj) } } else { for _, listener := range p.listeners { listener.add(obj) } } } processorListener.add: func (p *processorListener) add(notification interface{}) { p.addCh 综合以上的分析，可以看出processLoop通过调用HandleDeltas，再调用distribute，processorListener.add最终将不同更新类型的对象加入processorListener的channel中，供processorListener.Run使用。以下分析processorListener.Run的部分。 7. processor processor的主要功能就是记录了所有的回调函数实例(即 ResourceEventHandler 实例)，并负责触发这些函数。在sharedIndexInformer.Run部分会调用processor.run。 流程： listenser的add函数负责将notify装进pendingNotifications。 pop函数取出pendingNotifications的第一个nofify,输出到nextCh channel。 run函数则负责取出notify，然后根据notify的类型(增加、删除、更新)触发相应的处理函数，这些函数是在不同的NewXxxcontroller实现中注册的。 func (s *sharedIndexInformer) Run(stopCh 7.1. sharedProcessor.Run func (p *sharedProcessor) run(stopCh 7.1.1. listener.pop pop函数取出pendingNotifications的第一个nofify,输出到nextCh channel。 func (p *processorListener) pop() { defer utilruntime.HandleCrash() defer close(p.nextCh) // Tell .run() to stop var nextCh chan 7.1.2. listener.run listener.run部分根据不同的更新类型调用不同的处理函数。 func (p *processorListener) run() { defer utilruntime.HandleCrash() for next := range p.nextCh { switch notification := next.(type) { case updateNotification: p.handler.OnUpdate(notification.oldObj, notification.newObj) case addNotification: p.handler.OnAdd(notification.newObj) case deleteNotification: p.handler.OnDelete(notification.oldObj) default: utilruntime.HandleError(fmt.Errorf(\"unrecognized notification: %#v\", next)) } } } 其中具体的实现函数handler是在NewDeploymentController（其他不同类型的controller类似）中赋值的，而该handler是一个接口，具体如下： // ResourceEventHandler can handle notifications for events that happen to a // resource. The events are informational only, so you can't return an // error. // * OnAdd is called when an object is added. // * OnUpdate is called when an object is modified. Note that oldObj is the // last known state of the object-- it is possible that several changes // were combined together, so you can't use this to see every single // change. OnUpdate is also called when a re-list happens, and it will // get called even if nothing changed. This is useful for periodically // evaluating or syncing something. // * OnDelete will get the final state of the item if it is known, otherwise // it will get an object of type DeletedFinalStateUnknown. This can // happen if the watch is closed and misses the delete event and we don't // notice the deletion until the subsequent re-list. type ResourceEventHandler interface { OnAdd(obj interface{}) OnUpdate(oldObj, newObj interface{}) OnDelete(obj interface{}) } 7.2. ResourceEventHandler 以下以DeploymentController的处理逻辑为例。 在NewDeploymentController部分会注册deployment的事件函数，以下注册了三种类型的事件函数，其中包括：dInformer、rsInformer和podInformer。 // NewDeploymentController creates a new DeploymentController. func NewDeploymentController(dInformer extensionsinformers.DeploymentInformer, rsInformer extensionsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { ... dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addDeployment, UpdateFunc: dc.updateDeployment, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: dc.deleteDeployment, }) rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: dc.addReplicaSet, UpdateFunc: dc.updateReplicaSet, DeleteFunc: dc.deleteReplicaSet, }) podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: dc.deletePod, }) ... } 7.2.1. addDeployment 以下以addDeployment为例，addDeployment主要是将对象加入到enqueueDeployment的队列中。 func (dc *DeploymentController) addDeployment(obj interface{}) { d := obj.(*extensions.Deployment) glog.V(4).Infof(\"Adding deployment %s\", d.Name) dc.enqueueDeployment(d) } enqueueDeployment的定义 type DeploymentController struct { ... enqueueDeployment func(deployment *extensions.Deployment) ... } 将dc.enqueue赋值给dc.enqueueDeployment dc.enqueueDeployment = dc.enqueue dc.enqueue调用了dc.queue.Add(key) func (dc *DeploymentController) enqueue(deployment *extensions.Deployment) { key, err := controller.KeyFunc(deployment) if err != nil { utilruntime.HandleError(fmt.Errorf(\"Couldn't get key for object %#v: %v\", deployment, err)) return } dc.queue.Add(key) } dc.queue主要记录了需要被同步的deployment的对象，供syncDeployment使用。 dc := &DeploymentController{ ... queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"deployment\"), } NewNamedRateLimitingQueue func NewNamedRateLimitingQueue(rateLimiter RateLimiter, name string) RateLimitingInterface { return &rateLimitingType{ DelayingInterface: NewNamedDelayingQueue(name), rateLimiter: rateLimiter, } } 通过以上分析，可以看出processor记录了不同类似的事件函数，其中事件函数在NewXxxController构造函数部分注册，具体事件函数的处理，一般是将需要处理的对象加入对应的controller的任务队列中，然后由类似syncDeployment的同步函数来维持期望状态的同步逻辑。 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2021-03-17 19:13:16 "},"kube-controller-manager/list-watch/summary.html":{"url":"kube-controller-manager/list-watch/summary.html","title":"总结","keywords":"","body":"8. 总结 本文分析的部分主要是k8s的informer机制，即List-Watch机制。 8.1. Reflector Reflector的主要作用是watch指定的k8s资源，并将变化同步到本地是store中。Reflector只会放置指定的expectedType类型的资源到store中，除非expectedType为nil。如果resyncPeriod不为零，那么Reflector为以resyncPeriod为周期定期执行list的操作，这样就可以使用Reflector来定期处理所有的对象，也可以逐步处理变化的对象。 8.2. ListAndWatch ListAndWatch第一次会列出所有的对象，并获取资源对象的版本号，然后watch资源对象的版本号来查看是否有被变更。首先会将资源版本号设置为0，list()可能会导致本地的缓存相对于etcd里面的内容存在延迟，Reflector会通过watch的方法将延迟的部分补充上，使得本地的缓存数据与etcd的数据保持一致。 8.3. DeltaFIFO DeltaFIFO是一个生产者与消费者的队列，其中Reflector是生产者，消费者调用Pop()的方法。 DeltaFIFO主要用在以下场景： 希望对象变更最多处理一次 处理对象时，希望查看自上次处理对象以来发生的所有事情 要处理对象的删除 希望定期重新处理对象 8.4. store Store是一个通用的存储接口，Reflector通过watch server的方式更新数据到store中，store给Reflector提供本地的缓存，让Reflector可以像消息队列一样的工作。 Store实现的是一种可以准确的写入对象和获取对象的机制。 8.5. processor processor的主要功能就是记录了所有的回调函数实例(即 ResourceEventHandler 实例)，并负责触发这些函数。在sharedIndexInformer.Run部分会调用processor.run。 流程： listenser的add函数负责将notify装进pendingNotifications。 pop函数取出pendingNotifications的第一个nofify,输出到nextCh channel。 run函数则负责取出notify，然后根据notify的类型(增加、删除、更新)触发相应的处理函数，这些函数是在不同的NewXxxcontroller实现中注册的。 processor记录了不同类似的事件函数，其中事件函数在NewXxxController构造函数部分注册，具体事件函数的处理，一般是将需要处理的对象加入对应的controller的任务队列中，然后由类似syncDeployment的同步函数来维持期望状态的同步逻辑。 8.6. 主要步骤 在controller-manager的Run函数部分调用了InformerFactory.Start的方法，Start方法初始化各种类型的informer，并且每个类型起了个informer.Run的goroutine。 informer.Run的部分先生成一个DeltaFIFO的队列来存储对象变化的数据。然后调用processor.Run和controller.Run函数。 controller.Run函数会生成一个Reflector，Reflector的主要作用是watch指定的k8s资源，并将变化同步到本地是store中。Reflector以resyncPeriod为周期定期执行list的操作，这样就可以使用Reflector来定期处理所有的对象，也可以逐步处理变化的对象。 Reflector接着执行ListAndWatch函数，ListAndWatch第一次会列出所有的对象，并获取资源对象的版本号，然后watch资源对象的版本号来查看是否有被变更。首先会将资源版本号设置为0，list()可能会导致本地的缓存相对于etcd里面的内容存在延迟，Reflector会通过watch的方法将延迟的部分补充上，使得本地的缓存数据与etcd的数据保持一致。 controller.Run函数还会调用processLoop函数，processLoop通过调用HandleDeltas，再调用distribute，processorListener.add最终将不同更新类型的对象加入processorListener的channel中，供processorListener.Run使用。 processor的主要功能就是记录了所有的回调函数实例(即 ResourceEventHandler 实例)，并负责触发这些函数。processor记录了不同类型的事件函数，其中事件函数在NewXxxController构造函数部分注册，具体事件函数的处理，一般是将需要处理的对象加入对应的controller的任务队列中，然后由类似syncDeployment的同步函数来维持期望状态的同步逻辑。 参考文章： https://github.com/kubernetes/client-go/tree/master/tools/cache https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md https://github.com/kubernetes/client-go/blob/master/examples/workqueue/main.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2021-03-17 19:12:33 "},"kube-scheduler/scheduler-xmind.html":{"url":"kube-scheduler/scheduler-xmind.html","title":"源码思维导图","keywords":"","body":"kube-scheduler 源码思维导图 源码整体结构图 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2021-04-18 13:07:38 "},"kube-scheduler/NewSchedulerCommand.html":{"url":"kube-scheduler/NewSchedulerCommand.html","title":"NewSchedulerCommand","keywords":"","body":"kube-scheduler源码分析（一）之 NewSchedulerCommand 以下代码分析基于 kubernetes v1.12.0 版本。 scheduler的cmd代码目录结构如下： kube-scheduler ├── BUILD ├── OWNERS ├── app # app的目录下主要为运行scheduler相关的对象 │ ├── BUILD │ ├── config │ │ ├── BUILD │ │ └── config.go # Scheduler的配置对象config │ ├── options # options主要记录 Scheduler 使用到的参数 │ │ ├── BUILD │ │ ├── configfile.go │ │ ├── deprecated.go │ │ ├── deprecated_test.go │ │ ├── insecure_serving.go │ │ ├── insecure_serving_test.go │ │ ├── options.go # 主要包括Options、NewOptions、AddFlags、Config等函数 │ │ └── options_test.go │ └── server.go # 主要包括 NewSchedulerCommand、NewSchedulerConfig、Run等函数 └── scheduler.go # main入口函数 1. Main函数 此部分的代码为/cmd/kube-scheduler/scheduler.go kube-scheduler的入口函数Main函数，仍然是采用统一的代码风格，使用Cobra命令行框架。 func main() { rand.Seed(time.Now().UTC().UnixNano()) command := app.NewSchedulerCommand() // TODO: once we switch everything over to Cobra commands, we can go back to calling // utilflag.InitFlags() (by removing its pflag.Parse() call). For now, we have to set the // normalize func and add the go flag set by hand. pflag.CommandLine.SetNormalizeFunc(utilflag.WordSepNormalizeFunc) pflag.CommandLine.AddGoFlagSet(goflag.CommandLine) // utilflag.InitFlags() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } 核心代码： // 初始化scheduler命令结构体 command := app.NewSchedulerCommand() // 执行Execute err := command.Execute() 2. NewSchedulerCommand 此部分的代码为/cmd/kube-scheduler/app/server.go NewSchedulerCommand主要用来构造和初始化SchedulerCommand结构体， // NewSchedulerCommand creates a *cobra.Command object with default parameters func NewSchedulerCommand() *cobra.Command { opts, err := options.NewOptions() if err != nil { glog.Fatalf(\"unable to initialize command options: %v\", err) } cmd := &cobra.Command{ Use: \"kube-scheduler\", Long: `The Kubernetes scheduler is a policy-rich, topology-aware, workload-specific function that significantly impacts availability, performance, and capacity. The scheduler needs to take into account individual and collective resource requirements, quality of service requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, deadlines, and so on. Workload-specific requirements will be exposed through the API as necessary.`, Run: func(cmd *cobra.Command, args []string) { verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cmd.Flags()) if len(args) != 0 { fmt.Fprint(os.Stderr, \"arguments are not supported\\n\") } if errs := opts.Validate(); len(errs) > 0 { fmt.Fprintf(os.Stderr, \"%v\\n\", utilerrors.NewAggregate(errs)) os.Exit(1) } if len(opts.WriteConfigTo) > 0 { if err := options.WriteConfigFile(opts.WriteConfigTo, &opts.ComponentConfig); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } glog.Infof(\"Wrote configuration to: %s\\n\", opts.WriteConfigTo) return } c, err := opts.Config() if err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } stopCh := make(chan struct{}) if err := Run(c.Complete(), stopCh); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } }, } opts.AddFlags(cmd.Flags()) cmd.MarkFlagFilename(\"config\", \"yaml\", \"yml\", \"json\") return cmd } 核心代码： // 构造option opts, err := options.NewOptions() // 初始化config对象 c, err := opts.Config() // 执行run函数 err := Run(c.Complete(), stopCh) // 添加参数 opts.AddFlags(cmd.Flags()) 2.1. NewOptions NewOptions主要用来构造SchedulerServer使用的参数和上下文，其中核心参数是KubeSchedulerConfiguration。 opts, err := options.NewOptions() NewOptions: // NewOptions returns default scheduler app options. func NewOptions() (*Options, error) { cfg, err := newDefaultComponentConfig() if err != nil { return nil, err } hhost, hport, err := splitHostIntPort(cfg.HealthzBindAddress) if err != nil { return nil, err } o := &Options{ ComponentConfig: *cfg, SecureServing: nil, // TODO: enable with apiserveroptions.NewSecureServingOptions() CombinedInsecureServing: &CombinedInsecureServingOptions{ Healthz: &apiserveroptions.DeprecatedInsecureServingOptions{ BindNetwork: \"tcp\", }, Metrics: &apiserveroptions.DeprecatedInsecureServingOptions{ BindNetwork: \"tcp\", }, BindPort: hport, BindAddress: hhost, }, Authentication: nil, // TODO: enable with apiserveroptions.NewDelegatingAuthenticationOptions() Authorization: nil, // TODO: enable with apiserveroptions.NewDelegatingAuthorizationOptions() Deprecated: &DeprecatedOptions{ UseLegacyPolicyConfig: false, PolicyConfigMapNamespace: metav1.NamespaceSystem, }, } return o, nil } 2.2. Options.Config Config初始化调度器的配置对象。 c, err := opts.Config() Config函数主要执行以下操作： 构建scheduler client、leaderElectionClient、eventClient。 创建event recorder 设置leader选举 创建informer对象，主要函数有NewSharedInformerFactory和NewPodInformer。 Config具体代码如下： // Config return a scheduler config object func (o *Options) Config() (*schedulerappconfig.Config, error) { c := &schedulerappconfig.Config{} if err := o.ApplyTo(c); err != nil { return nil, err } // prepare kube clients. client, leaderElectionClient, eventClient, err := createClients(c.ComponentConfig.ClientConnection, o.Master, c.ComponentConfig.LeaderElection.RenewDeadline.Duration) if err != nil { return nil, err } // Prepare event clients. eventBroadcaster := record.NewBroadcaster() recorder := eventBroadcaster.NewRecorder(legacyscheme.Scheme, corev1.EventSource{Component: c.ComponentConfig.SchedulerName}) // Set up leader election if enabled. var leaderElectionConfig *leaderelection.LeaderElectionConfig if c.ComponentConfig.LeaderElection.LeaderElect { leaderElectionConfig, err = makeLeaderElectionConfig(c.ComponentConfig.LeaderElection, leaderElectionClient, recorder) if err != nil { return nil, err } } c.Client = client c.InformerFactory = informers.NewSharedInformerFactory(client, 0) c.PodInformer = factory.NewPodInformer(client, 0) c.EventClient = eventClient c.Recorder = recorder c.Broadcaster = eventBroadcaster c.LeaderElection = leaderElectionConfig return c, nil } 2.3. AddFlags AddFlags为SchedulerServer添加指定的参数。 opts.AddFlags(cmd.Flags()) AddFlags函数的具体代码如下： // AddFlags adds flags for the scheduler options. func (o *Options) AddFlags(fs *pflag.FlagSet) { fs.StringVar(&o.ConfigFile, \"config\", o.ConfigFile, \"The path to the configuration file. Flags override values in this file.\") fs.StringVar(&o.WriteConfigTo, \"write-config-to\", o.WriteConfigTo, \"If set, write the configuration values to this file and exit.\") fs.StringVar(&o.Master, \"master\", o.Master, \"The address of the Kubernetes API server (overrides any value in kubeconfig)\") o.SecureServing.AddFlags(fs) o.CombinedInsecureServing.AddFlags(fs) o.Authentication.AddFlags(fs) o.Authorization.AddFlags(fs) o.Deprecated.AddFlags(fs, &o.ComponentConfig) leaderelectionconfig.BindFlags(&o.ComponentConfig.LeaderElection.LeaderElectionConfiguration, fs) utilfeature.DefaultFeatureGate.AddFlag(fs) } 3. Run 此部分的代码为/cmd/kube-scheduler/app/server.go err := Run(c.Complete(), stopCh) Run运行一个不退出的常驻进程，来执行scheduler的相关操作。 Run函数的主要内容如下： 通过scheduler config来创建scheduler的结构体。 运行event broadcaster、healthz server、metrics server。 运行所有的informer并在调度前等待cache的同步（重点）。 执行sched.Run()来运行scheduler的调度逻辑。 如果多个scheduler并开启了LeaderElect，则执行leader选举。 以下对重点代码分开分析： 3.1. NewSchedulerConfig NewSchedulerConfig初始化SchedulerConfig（此部分具体逻辑待后续专门分析），最后初始化生成scheduler结构体。 // Build a scheduler config from the provided algorithm source. schedulerConfig, err := NewSchedulerConfig(c) if err != nil { return err } // Create the scheduler. sched := scheduler.NewFromConfig(schedulerConfig) 3.2. InformerFactory.Start 运行PodInformer，并运行InformerFactory。此部分的逻辑为client-go的informer机制，在Informer机制中有详细分析。 // Start all informers. go c.PodInformer.Informer().Run(stopCh) c.InformerFactory.Start(stopCh) 3.3. WaitForCacheSync 在调度前等待cache同步。 // Wait for all caches to sync before scheduling. c.InformerFactory.WaitForCacheSync(stopCh) controller.WaitForCacheSync(\"scheduler\", stopCh, c.PodInformer.Informer().HasSynced) 3.3.1. InformerFactory.WaitForCacheSync InformerFactory.WaitForCacheSync等待所有启动的informer的cache进行同步，保持本地的store信息与etcd的信息是最新一致的。 // WaitForCacheSync waits for all started informers' cache were synced. func (f *sharedInformerFactory) WaitForCacheSync(stopCh 接着调用cache.WaitForCacheSync。 // WaitForCacheSync waits for caches to populate. It returns true if it was successful, false // if the controller should shutdown func WaitForCacheSync(stopCh 3.3.2. controller.WaitForCacheSync controller.WaitForCacheSync是对cache.WaitForCacheSync的一层封装，通过不同的controller的名字来记录不同controller等待cache同步。 controller.WaitForCacheSync(\"scheduler\", stop, s.PodInformer.Informer().HasSynced) controller.WaitForCacheSync具体代码如下： // WaitForCacheSync is a wrapper around cache.WaitForCacheSync that generates log messages // indicating that the controller identified by controllerName is waiting for syncs, followed by // either a successful or failed sync. func WaitForCacheSync(controllerName string, stopCh 3.4. LeaderElection 如果有多个scheduler，并开启leader选举，则运行LeaderElector直到选举结束或退出。 // If leader election is enabled, run via LeaderElector until done and exit. if c.LeaderElection != nil { c.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{ OnStartedLeading: run, OnStoppedLeading: func() { utilruntime.HandleError(fmt.Errorf(\"lost master\")) }, } leaderElector, err := leaderelection.NewLeaderElector(*c.LeaderElection) if err != nil { return fmt.Errorf(\"couldn't create leader elector: %v\", err) } leaderElector.Run(ctx) return fmt.Errorf(\"lost lease\") } 3.5. Scheduler.Run // Prepare a reusable run function. run := func(ctx context.Context) { sched.Run() Scheduler.Run先等待cache同步，然后开启调度逻辑的goroutine。 Scheduler.Run的具体代码如下： // Run begins watching and scheduling. It waits for cache to be synced, then starts a goroutine and returns immediately. func (sched *Scheduler) Run() { if !sched.config.WaitForCacheSync() { return } go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything) } 以上是对/cmd/kube-scheduler/scheduler.go部分代码的分析，Scheduler.Run后续的具体代码位于pkg/scheduler/scheduler.go待后续文章分析。 参考： https://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kube-scheduler https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-scheduler/scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kube-scheduler/app/server.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kube-scheduler/registerAlgorithmProvider.html":{"url":"kube-scheduler/registerAlgorithmProvider.html","title":"registerAlgorithmProvider","keywords":"","body":"kube-scheduler源码分析（二）之 registerAlgorithmProvider 以下代码分析基于 kubernetes v1.12.0 版本。 此部分主要介绍调度中使用的各种调度算法，包括调度算法的注册部分。注册部分的代码主要在/pkg/scheduler/algorithmprovider中，具体的预选策略和优选策略的算法实现在/pkg/scheduler/algorithm中。 1. ApplyFeatureGates 注册调度算法的调用入口在SchedulerCommand的Run函数中。 此部分代码位于/cmd/kube-scheduler/app/server.go // Run runs the Scheduler. func Run(c schedulerserverconfig.CompletedConfig, stopCh ApplyFeatureGates的具体实现在pkg/scheduler/algorithmprovider的包中。 此部分代码位于/pkg/scheduler/algorithmprovider/plugins.go // ApplyFeatureGates applies algorithm by feature gates. func ApplyFeatureGates() { defaults.ApplyFeatureGates() } ApplyFeatureGates具体实现如下： 此部分代码位于/pkg/scheduler/algorithmprovider/defaults/defaults.go 根据feature移除部分调度策略。 // ApplyFeatureGates applies algorithm by feature gates. func ApplyFeatureGates() { if utilfeature.DefaultFeatureGate.Enabled(features.TaintNodesByCondition) { // Remove \"CheckNodeCondition\", \"CheckNodeMemoryPressure\", \"CheckNodePIDPressurePred\" // and \"CheckNodeDiskPressure\" predicates factory.RemoveFitPredicate(predicates.CheckNodeConditionPred) factory.RemoveFitPredicate(predicates.CheckNodeMemoryPressurePred) factory.RemoveFitPredicate(predicates.CheckNodeDiskPressurePred) factory.RemoveFitPredicate(predicates.CheckNodePIDPressurePred) // Remove key \"CheckNodeCondition\", \"CheckNodeMemoryPressure\" and \"CheckNodeDiskPressure\" // from ALL algorithm provider // The key will be removed from all providers which in algorithmProviderMap[] // if you just want remove specific provider, call func RemovePredicateKeyFromAlgoProvider() factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeConditionPred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeMemoryPressurePred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodeDiskPressurePred) factory.RemovePredicateKeyFromAlgorithmProviderMap(predicates.CheckNodePIDPressurePred) // Fit is determined based on whether a pod can tolerate all of the node's taints factory.RegisterMandatoryFitPredicate(predicates.PodToleratesNodeTaintsPred, predicates.PodToleratesNodeTaints) // Fit is determined based on whether a pod can tolerate unschedulable of node factory.RegisterMandatoryFitPredicate(predicates.CheckNodeUnschedulablePred, predicates.CheckNodeUnschedulablePredicate) // Insert Key \"PodToleratesNodeTaints\" and \"CheckNodeUnschedulable\" To All Algorithm Provider // The key will insert to all providers which in algorithmProviderMap[] // if you just want insert to specific provider, call func InsertPredicateKeyToAlgoProvider() factory.InsertPredicateKeyToAlgorithmProviderMap(predicates.PodToleratesNodeTaintsPred) factory.InsertPredicateKeyToAlgorithmProviderMap(predicates.CheckNodeUnschedulablePred) glog.Warningf(\"TaintNodesByCondition is enabled, PodToleratesNodeTaints predicate is mandatory\") } // Prioritizes nodes that satisfy pod's resource limits if utilfeature.DefaultFeatureGate.Enabled(features.ResourceLimitsPriorityFunction) { factory.RegisterPriorityFunction2(\"ResourceLimitsPriority\", priorities.ResourceLimitsPriorityMap, nil, 1) } } 2. init 当函数逻辑调用到algorithmprovider包时，就会自动调用init的初始化函数，此部分主要包括对预选算法和优选算法的注册。 此部分代码位于/pkg/scheduler/algorithmprovider/defaults/defaults.go func init() { // Register functions that extract metadata used by predicates and priorities computations. factory.RegisterPredicateMetadataProducerFactory( func(args factory.PluginFactoryArgs) algorithm.PredicateMetadataProducer { return predicates.NewPredicateMetadataFactory(args.PodLister) }) factory.RegisterPriorityMetadataProducerFactory( func(args factory.PluginFactoryArgs) algorithm.PriorityMetadataProducer { return priorities.NewPriorityMetadataFactory(args.ServiceLister, args.ControllerLister, args.ReplicaSetLister, args.StatefulSetLister) }) registerAlgorithmProvider(defaultPredicates(), defaultPriorities()) // IMPORTANT NOTES for predicate developers: // We are using cached predicate result for pods belonging to the same equivalence class. // So when implementing a new predicate, you are expected to check whether the result // of your predicate function can be affected by related API object change (ADD/DELETE/UPDATE). // If yes, you are expected to invalidate the cached predicate result for related API object change. // For example: // https://github.com/kubernetes/kubernetes/blob/36a218e/plugin/pkg/scheduler/factory/factory.go#L422 // Registers predicates and priorities that are not enabled by default, but user can pick when creating their // own set of priorities/predicates. // PodFitsPorts has been replaced by PodFitsHostPorts for better user understanding. // For backwards compatibility with 1.0, PodFitsPorts is registered as well. factory.RegisterFitPredicate(\"PodFitsPorts\", predicates.PodFitsHostPorts) // Fit is defined based on the absence of port conflicts. // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.PodFitsHostPortsPred, predicates.PodFitsHostPorts) // Fit is determined by resource availability. // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.PodFitsResourcesPred, predicates.PodFitsResources) // Fit is determined by the presence of the Host parameter and a string match // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.HostNamePred, predicates.PodFitsHost) // Fit is determined by node selector query. factory.RegisterFitPredicate(predicates.MatchNodeSelectorPred, predicates.PodMatchNodeSelector) // ServiceSpreadingPriority is a priority config factory that spreads pods by minimizing // the number of pods (belonging to the same service) on the same node. // Register the factory so that it's available, but do not include it as part of the default priorities // Largely replaced by \"SelectorSpreadPriority\", but registered for backward compatibility with 1.0 factory.RegisterPriorityConfigFactory( \"ServiceSpreadingPriority\", factory.PriorityConfigFactory{ MapReduceFunction: func(args factory.PluginFactoryArgs) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { return priorities.NewSelectorSpreadPriority(args.ServiceLister, algorithm.EmptyControllerLister{}, algorithm.EmptyReplicaSetLister{}, algorithm.EmptyStatefulSetLister{}) }, Weight: 1, }, ) // EqualPriority is a prioritizer function that gives an equal weight of one to all nodes // Register the priority function so that its available // but do not include it as part of the default priorities factory.RegisterPriorityFunction2(\"EqualPriority\", core.EqualPriorityMap, nil, 1) // Optional, cluster-autoscaler friendly priority function - give used nodes higher priority. factory.RegisterPriorityFunction2(\"MostRequestedPriority\", priorities.MostRequestedPriorityMap, nil, 1) factory.RegisterPriorityFunction2( \"RequestedToCapacityRatioPriority\", priorities.RequestedToCapacityRatioResourceAllocationPriorityDefault().PriorityMap, nil, 1) } 以下对init中的注册进行拆分介绍。 2.1. registerAlgorithmProvider 此部分主要注册默认的预选和优选策略。 // Register functions that extract metadata used by predicates and priorities computations. factory.RegisterPredicateMetadataProducerFactory( func(args factory.PluginFactoryArgs) algorithm.PredicateMetadataProducer { return predicates.NewPredicateMetadataFactory(args.PodLister) }) factory.RegisterPriorityMetadataProducerFactory( func(args factory.PluginFactoryArgs) algorithm.PriorityMetadataProducer { return priorities.NewPriorityMetadataFactory(args.ServiceLister, args.ControllerLister, args.ReplicaSetLister, args.StatefulSetLister) }) registerAlgorithmProvider(defaultPredicates(), defaultPriorities()) registerAlgorithmProvider 注册AlgorithmProvider，其中包括DefaultProvider和ClusterAutoscalerProvider。 func registerAlgorithmProvider(predSet, priSet sets.String) { // Registers algorithm providers. By default we use 'DefaultProvider', but user can specify one to be used // by specifying flag. factory.RegisterAlgorithmProvider(factory.DefaultProvider, predSet, priSet) // Cluster autoscaler friendly scheduling algorithm. factory.RegisterAlgorithmProvider(ClusterAutoscalerProvider, predSet, copyAndReplace(priSet, \"LeastRequestedPriority\", \"MostRequestedPriority\")) } 2.2. RegisterFitPredicate 在init部分注册预选策略函数。 预选策略如下： 调度策略 函数 描述 PodFitsPorts PodFitsHostPorts PodFitsPorts已经被PodFitsHostPorts代替，此处主要是为了兼容性。 PodFitsHostPortsPred PodFitsHostPorts 判断是否与宿主机的端口冲突。 PodFitsResourcesPred PodFitsResources 判断node资源是否充足。 HostNamePred PodFitsHost 判断pod所指定调度的节点是否是当前的节点。 MatchNodeSelectorPred PodMatchNodeSelector 判断pod指定的node selector是否匹配当前的node。 具体代码如下： // PodFitsPorts has been replaced by PodFitsHostPorts for better user understanding. // For backwards compatibility with 1.0, PodFitsPorts is registered as well. factory.RegisterFitPredicate(\"PodFitsPorts\", predicates.PodFitsHostPorts) // Fit is defined based on the absence of port conflicts. // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.PodFitsHostPortsPred, predicates.PodFitsHostPorts) // Fit is determined by resource availability. // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.PodFitsResourcesPred, predicates.PodFitsResources) // Fit is determined by the presence of the Host parameter and a string match // This predicate is actually a default predicate, because it is invoked from // predicates.GeneralPredicates() factory.RegisterFitPredicate(predicates.HostNamePred, predicates.PodFitsHost) // Fit is determined by node selector query. factory.RegisterFitPredicate(predicates.MatchNodeSelectorPred, predicates.PodMatchNodeSelector) 2.3. RegisterPriorityFunction2 在init部分注册优选策略函数。 // EqualPriority is a prioritizer function that gives an equal weight of one to all nodes // Register the priority function so that its available // but do not include it as part of the default priorities factory.RegisterPriorityFunction2(\"EqualPriority\", core.EqualPriorityMap, nil, 1) // Optional, cluster-autoscaler friendly priority function - give used nodes higher priority. factory.RegisterPriorityFunction2(\"MostRequestedPriority\", priorities.MostRequestedPriorityMap, nil, 1) factory.RegisterPriorityFunction2( \"RequestedToCapacityRatioPriority\", priorities.RequestedToCapacityRatioResourceAllocationPriorityDefault().PriorityMap, nil, 1) 3. defaultPredicates 此部分为默认预选策略的注册函数。 默认的预选策略如下： 预选策略 函数 描述 NoVolumeZoneConflictPred NewVolumeZonePredicate 判断pod使用到的volume是否有节点的要求。目前只支持pvc。 MaxEBSVolumeCountPred NewMaxPDVolumeCountPredicate 判断pod使用EBSVolume在该节点上是否已经达到上限了。 MaxGCEPDVolumeCountPred NewMaxPDVolumeCountPredicate 判断pod使用GCEPDVolume在该节点上是否已经达到上限了。 MaxAzureDiskVolumeCountPred NewMaxPDVolumeCountPredicate 判断pod使用AzureDiskVolume在该节点上是否已经达到上限了。 MaxCSIVolumeCountPred NewCSIMaxVolumeLimitPredicate 判断CSIVolume是否达到上限了。 MatchInterPodAffinityPred NewPodAffinityPredicate 匹配pod的亲缘性。 NoDiskConflictPred NoDiskConflict 判断是否有disk volumes的冲突。 GeneralPred GeneralPredicates 通用的预选策略 CheckNodeMemoryPressurePred CheckNodeMemoryPressurePredicate 判断节点内存是否充足。 CheckNodeDiskPressurePred CheckNodeDiskPressurePredicate 判断节点是否有磁盘压力。 CheckNodePIDPressurePred CheckNodePIDPressurePredicate 判断节点上的PID CheckNodeConditionPred CheckNodeConditionPredicate 判断node是否ready。 PodToleratesNodeTaintsPred PodToleratesNodeTaints 判断pod是否可以容忍节点的taints。 CheckVolumeBindingPred NewVolumeBindingPredicate 判断是否有volume拓扑的要求。 具体代码如下： func defaultPredicates() sets.String { return sets.NewString( // Fit is determined by volume zone requirements. factory.RegisterFitPredicateFactory( predicates.NoVolumeZoneConflictPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewVolumeZonePredicate(args.PVInfo, args.PVCInfo, args.StorageClassInfo) }, ), // Fit is determined by whether or not there would be too many AWS EBS volumes attached to the node factory.RegisterFitPredicateFactory( predicates.MaxEBSVolumeCountPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewMaxPDVolumeCountPredicate(predicates.EBSVolumeFilterType, args.PVInfo, args.PVCInfo) }, ), // Fit is determined by whether or not there would be too many GCE PD volumes attached to the node factory.RegisterFitPredicateFactory( predicates.MaxGCEPDVolumeCountPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewMaxPDVolumeCountPredicate(predicates.GCEPDVolumeFilterType, args.PVInfo, args.PVCInfo) }, ), // Fit is determined by whether or not there would be too many Azure Disk volumes attached to the node factory.RegisterFitPredicateFactory( predicates.MaxAzureDiskVolumeCountPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewMaxPDVolumeCountPredicate(predicates.AzureDiskVolumeFilterType, args.PVInfo, args.PVCInfo) }, ), factory.RegisterFitPredicateFactory( predicates.MaxCSIVolumeCountPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewCSIMaxVolumeLimitPredicate(args.PVInfo, args.PVCInfo) }, ), // Fit is determined by inter-pod affinity. factory.RegisterFitPredicateFactory( predicates.MatchInterPodAffinityPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewPodAffinityPredicate(args.NodeInfo, args.PodLister) }, ), // Fit is determined by non-conflicting disk volumes. factory.RegisterFitPredicate(predicates.NoDiskConflictPred, predicates.NoDiskConflict), // GeneralPredicates are the predicates that are enforced by all Kubernetes components // (e.g. kubelet and all schedulers) factory.RegisterFitPredicate(predicates.GeneralPred, predicates.GeneralPredicates), // Fit is determined by node memory pressure condition. factory.RegisterFitPredicate(predicates.CheckNodeMemoryPressurePred, predicates.CheckNodeMemoryPressurePredicate), // Fit is determined by node disk pressure condition. factory.RegisterFitPredicate(predicates.CheckNodeDiskPressurePred, predicates.CheckNodeDiskPressurePredicate), // Fit is determined by node pid pressure condition. factory.RegisterFitPredicate(predicates.CheckNodePIDPressurePred, predicates.CheckNodePIDPressurePredicate), // Fit is determined by node conditions: not ready, network unavailable or out of disk. factory.RegisterMandatoryFitPredicate(predicates.CheckNodeConditionPred, predicates.CheckNodeConditionPredicate), // Fit is determined based on whether a pod can tolerate all of the node's taints factory.RegisterFitPredicate(predicates.PodToleratesNodeTaintsPred, predicates.PodToleratesNodeTaints), // Fit is determined by volume topology requirements. factory.RegisterFitPredicateFactory( predicates.CheckVolumeBindingPred, func(args factory.PluginFactoryArgs) algorithm.FitPredicate { return predicates.NewVolumeBindingPredicate(args.VolumeBinder) }, ), ) } 4. defaultPriorities 此部分主要为默认优选策略的注册函数。 默认优选策略如下： 优选策略 函数 描述 SelectorSpreadPriority NewSelectorSpreadPriority 属于相同service和rs下的pod尽量分布在不同的node上。 InterPodAffinityPriority NewInterPodAffinityPriority 根据pod的亲缘性，将相同拓扑域中的pod放在同一个节点 LeastRequestedPriority LeastRequestedPriorityMap 按最少请求的利用率对节点进行优先级排序。 BalancedResourceAllocation BalancedResourceAllocationMap 实现资源的平衡使用。 NodePreferAvoidPodsPriority CalculateNodePreferAvoidPodsPriorityMap 将此权重设置为足以覆盖所有其他优先级函数。 NodeAffinityPriority CalculateNodeAffinityPriorityMap pod指定label节点调度，来匹配node亲缘性。 TaintTolerationPriority ComputeTaintTolerationPriorityMap pod有设置tolerate属性来容忍node的taint。 ImageLocalityPriority ImageLocalityPriorityMap 根据节点上是否有该pod使用到的镜像打分。 具体代码实现如下： func defaultPriorities() sets.String { return sets.NewString( // spreads pods by minimizing the number of pods (belonging to the same service or replication controller) on the same node. factory.RegisterPriorityConfigFactory( \"SelectorSpreadPriority\", factory.PriorityConfigFactory{ MapReduceFunction: func(args factory.PluginFactoryArgs) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { return priorities.NewSelectorSpreadPriority(args.ServiceLister, args.ControllerLister, args.ReplicaSetLister, args.StatefulSetLister) }, Weight: 1, }, ), // pods should be placed in the same topological domain (e.g. same node, same rack, same zone, same power domain, etc.) // as some other pods, or, conversely, should not be placed in the same topological domain as some other pods. factory.RegisterPriorityConfigFactory( \"InterPodAffinityPriority\", factory.PriorityConfigFactory{ Function: func(args factory.PluginFactoryArgs) algorithm.PriorityFunction { return priorities.NewInterPodAffinityPriority(args.NodeInfo, args.NodeLister, args.PodLister, args.HardPodAffinitySymmetricWeight) }, Weight: 1, }, ), // Prioritize nodes by least requested utilization. factory.RegisterPriorityFunction2(\"LeastRequestedPriority\", priorities.LeastRequestedPriorityMap, nil, 1), // Prioritizes nodes to help achieve balanced resource usage factory.RegisterPriorityFunction2(\"BalancedResourceAllocation\", priorities.BalancedResourceAllocationMap, nil, 1), // Set this weight large enough to override all other priority functions. // TODO: Figure out a better way to do this, maybe at same time as fixing #24720. factory.RegisterPriorityFunction2(\"NodePreferAvoidPodsPriority\", priorities.CalculateNodePreferAvoidPodsPriorityMap, nil, 10000), // Prioritizes nodes that have labels matching NodeAffinity factory.RegisterPriorityFunction2(\"NodeAffinityPriority\", priorities.CalculateNodeAffinityPriorityMap, priorities.CalculateNodeAffinityPriorityReduce, 1), // Prioritizes nodes that marked with taint which pod can tolerate. factory.RegisterPriorityFunction2(\"TaintTolerationPriority\", priorities.ComputeTaintTolerationPriorityMap, priorities.ComputeTaintTolerationPriorityReduce, 1), // ImageLocalityPriority prioritizes nodes that have images requested by the pod present. factory.RegisterPriorityFunction2(\"ImageLocalityPriority\", priorities.ImageLocalityPriorityMap, nil, 1), ) } 参考： https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/algorithmprovider/defaults/defaults.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kube-scheduler/scheduleOne.html":{"url":"kube-scheduler/scheduleOne.html","title":"scheduleOne","keywords":"","body":"kube-scheduler源码分析（三）之 scheduleOne 以下代码分析基于 kubernetes v1.12.0 版本。 本文主要分析/pkg/scheduler/中调度的基本流程。具体的预选调度逻辑、优选调度逻辑、节点抢占逻辑待后续再独立分析。 scheduler的pkg代码目录结构如下： scheduler ├── algorithm # 主要包含调度的算法 │ ├── predicates # 预选的策略 │ ├── priorities # 优选的策略 │ ├── scheduler_interface.go # ScheduleAlgorithm、SchedulerExtender接口定义 │ ├── types.go # 使用到的type的定义 ├── algorithmprovider │ ├── defaults │ │ ├── defaults.go # 默认算法的初始化操作，包括预选和优选策略 ├── cache # scheduler调度使用到的cache │ ├── cache.go # schedulerCache │ ├── interface.go │ ├── node_info.go │ ├── node_tree.go ├── core # 调度逻辑的核心代码 │ ├── equivalence │ │ ├── eqivalence.go # 存储相同pod的调度结果缓存，主要给预选策略使用 │ ├── extender.go │ ├── generic_scheduler.go # genericScheduler,主要包含默认调度器的调度逻辑 │ ├── scheduling_queue.go # 调度使用到的队列，主要用来存储需要被调度的pod ├── factory │ ├── factory.go # 主要包括NewConfigFactory、NewPodInformer，监听pod事件来更新调度队列 ├── metrics │ └── metrics.go # 主要给prometheus使用 ├── scheduler.go # pkg部分的Run入口(核心代码)，主要包含Run、scheduleOne、schedule、preempt等函数 └── volumebinder └── volume_binder.go # volume bind 1. Scheduler.Run 此部分代码位于pkg/scheduler/scheduler.go 此处为具体调度逻辑的入口。 // Run begins watching and scheduling. It waits for cache to be synced, then starts a goroutine and returns immediately. func (sched *Scheduler) Run() { if !sched.config.WaitForCacheSync() { return } go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything) } 2. Scheduler.scheduleOne 此部分代码位于pkg/scheduler/scheduler.go scheduleOne主要为单个pod选择一个适合的节点，为调度逻辑的核心函数。 对单个pod进行调度的基本流程如下： 通过podQueue的待调度队列中弹出需要调度的pod。 通过具体的调度算法为该pod选出合适的节点，其中调度算法就包括预选和优选两步策略。 如果上述调度失败，则会尝试抢占机制，将优先级低的pod剔除，让优先级高的pod调度成功。 将该pod和选定的节点进行假性绑定，存入scheduler cache中，方便具体绑定操作可以异步进行。 实际执行绑定操作，将node的名字添加到pod的节点相关属性中。 完整代码如下： // scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm's host fitting. func (sched *Scheduler) scheduleOne() { pod := sched.config.NextPod() if pod.DeletionTimestamp != nil { sched.config.Recorder.Eventf(pod, v1.EventTypeWarning, \"FailedScheduling\", \"skip schedule deleting pod: %v/%v\", pod.Namespace, pod.Name) glog.V(3).Infof(\"Skip schedule deleting pod: %v/%v\", pod.Namespace, pod.Name) return } glog.V(3).Infof(\"Attempting to schedule pod: %v/%v\", pod.Namespace, pod.Name) // Synchronously attempt to find a fit for the pod. start := time.Now() suggestedHost, err := sched.schedule(pod) if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() metrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime)) } return } metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInMicroseconds(start)) // Tell the cache to assume that a pod now is running on a given node, even though it hasn't been bound yet. // This allows us to keep scheduling without waiting on binding to occur. assumedPod := pod.DeepCopy() // Assume volumes first before assuming the pod. // // If all volumes are completely bound, then allBound is true and binding will be skipped. // // Otherwise, binding of volumes is started after the pod is assumed, but before pod binding. // // This function modifies 'assumedPod' if volume binding is required. allBound, err := sched.assumeVolumes(assumedPod, suggestedHost) if err != nil { return } // assume modifies `assumedPod` by setting NodeName=suggestedHost err = sched.assume(assumedPod, suggestedHost) if err != nil { return } // bind the pod to its host asynchronously (we can do this b/c of the assumption step above). go func() { // Bind volumes first before Pod if !allBound { err = sched.bindVolumes(assumedPod) if err != nil { return } } err := sched.bind(assumedPod, &v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: assumedPod.Namespace, Name: assumedPod.Name, UID: assumedPod.UID}, Target: v1.ObjectReference{ Kind: \"Node\", Name: suggestedHost, }, }) metrics.E2eSchedulingLatency.Observe(metrics.SinceInMicroseconds(start)) if err != nil { glog.Errorf(\"Internal error binding pod: (%v)\", err) } }() } 以下对重要代码分别进行分析。 3. config.NextPod 通过podQueue的方式存储待调度的pod队列，NextPod拿出下一个需要被调度的pod。 pod := sched.config.NextPod() if pod.DeletionTimestamp != nil { sched.config.Recorder.Eventf(pod, v1.EventTypeWarning, \"FailedScheduling\", \"skip schedule deleting pod: %v/%v\", pod.Namespace, pod.Name) glog.V(3).Infof(\"Skip schedule deleting pod: %v/%v\", pod.Namespace, pod.Name) return } glog.V(3).Infof(\"Attempting to schedule pod: %v/%v\", pod.Namespace, pod.Name) NextPod的具体函数在factory.go的CreateFromKey函数中定义，如下： func (c *configFactory) CreateFromKeys(predicateKeys, priorityKeys sets.String, extenders []algorithm.SchedulerExtender) (*scheduler.Config, error) { ... return &scheduler.Config{ ... NextPod: func() *v1.Pod { return c.getNextPod() } ... } 3.1. getNextPod 通过一个podQueue来存储需要调度的pod的队列，通过队列Pop的方式弹出需要被调度的pod。 func (c *configFactory) getNextPod() *v1.Pod { pod, err := c.podQueue.Pop() if err == nil { glog.V(4).Infof(\"About to try and schedule pod %v/%v\", pod.Namespace, pod.Name) return pod } glog.Errorf(\"Error while retrieving next pod from scheduling queue: %v\", err) return nil } 4. Scheduler.schedule 此部分代码位于pkg/scheduler/scheduler.go 此部分为调度逻辑的核心，通过不同的算法为具体的pod选择一个最合适的节点。 // Synchronously attempt to find a fit for the pod. start := time.Now() suggestedHost, err := sched.schedule(pod) if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() metrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime)) } return } schedule通过调度算法返回一个最优的节点。 // schedule implements the scheduling algorithm and returns the suggested host. func (sched *Scheduler) schedule(pod *v1.Pod) (string, error) { host, err := sched.config.Algorithm.Schedule(pod, sched.config.NodeLister) if err != nil { pod = pod.DeepCopy() sched.config.Error(pod, err) sched.config.Recorder.Eventf(pod, v1.EventTypeWarning, \"FailedScheduling\", \"%v\", err) sched.config.PodConditionUpdater.Update(pod, &v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, Reason: v1.PodReasonUnschedulable, Message: err.Error(), }) return \"\", err } return host, err } 4.1. ScheduleAlgorithm ScheduleAlgorithm是一个调度算法的接口，主要的实现体是genericScheduler，后续分析genericScheduler.Schedule。 ScheduleAlgorithm接口定义如下： // ScheduleAlgorithm is an interface implemented by things that know how to schedule pods // onto machines. type ScheduleAlgorithm interface { Schedule(*v1.Pod, NodeLister) (selectedMachine string, err error) // Preempt receives scheduling errors for a pod and tries to create room for // the pod by preempting lower priority pods if possible. // It returns the node where preemption happened, a list of preempted pods, a // list of pods whose nominated node name should be removed, and error if any. Preempt(*v1.Pod, NodeLister, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error) // Predicates() returns a pointer to a map of predicate functions. This is // exposed for testing. Predicates() map[string]FitPredicate // Prioritizers returns a slice of priority config. This is exposed for // testing. Prioritizers() []PriorityConfig } 5. genericScheduler.Schedule 此部分代码位于/pkg/scheduler/core/generic_scheduler.go genericScheduler.Schedule实现了基本的调度逻辑，基于给定需要调度的pod和node列表，如果执行成功返回调度的节点的名字，如果执行失败，则返回错误和原因。主要通过预选和优选两步操作完成调度的逻辑。 基本流程如下： 对pod做基本性检查，目前主要是对pvc的检查。 通过findNodesThatFit预选策略选出满足调度条件的node列表。 通过PrioritizeNodes优选策略给预选的node列表中的node进行打分。 在打分的node列表中选择一个分数最高的node作为调度的节点。 完整代码如下： // Schedule tries to schedule the given pod to one of the nodes in the node list. // If it succeeds, it will return the name of the node. // If it fails, it will return a FitError error with reasons. func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { trace := utiltrace.New(fmt.Sprintf(\"Scheduling %s/%s\", pod.Namespace, pod.Name)) defer trace.LogIfLong(100 * time.Millisecond) if err := podPassesBasicChecks(pod, g.pvcLister); err != nil { return \"\", err } nodes, err := nodeLister.List() if err != nil { return \"\", err } if len(nodes) == 0 { return \"\", ErrNoNodesAvailable } // Used for all fit and priority funcs. err = g.cache.UpdateNodeNameToInfoMap(g.cachedNodeInfoMap) if err != nil { return \"\", err } trace.Step(\"Computing predicates\") startPredicateEvalTime := time.Now() filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) if err != nil { return \"\", err } if len(filteredNodes) == 0 { return \"\", &FitError{ Pod: pod, NumAllNodes: len(nodes), FailedPredicates: failedPredicateMap, } } metrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime)) trace.Step(\"Prioritizing\") startPriorityEvalTime := time.Now() // When only one node after predicate, just use it. if len(filteredNodes) == 1 { metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) return filteredNodes[0].Name, nil } metaPrioritiesInterface := g.priorityMetaProducer(pod, g.cachedNodeInfoMap) priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) if err != nil { return \"\", err } metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime)) trace.Step(\"Selecting host\") return g.selectHost(priorityList) } 5.1. podPassesBasicChecks podPassesBasicChecks主要做一下基本性检查，目前主要是对pvc的检查。 if err := podPassesBasicChecks(pod, g.pvcLister); err != nil { return \"\", err } podPassesBasicChecks具体实现如下： // podPassesBasicChecks makes sanity checks on the pod if it can be scheduled. func podPassesBasicChecks(pod *v1.Pod, pvcLister corelisters.PersistentVolumeClaimLister) error { // Check PVCs used by the pod namespace := pod.Namespace manifest := &(pod.Spec) for i := range manifest.Volumes { volume := &manifest.Volumes[i] if volume.PersistentVolumeClaim == nil { // Volume is not a PVC, ignore continue } pvcName := volume.PersistentVolumeClaim.ClaimName pvc, err := pvcLister.PersistentVolumeClaims(namespace).Get(pvcName) if err != nil { // The error has already enough context (\"persistentvolumeclaim \"myclaim\" not found\") return err } if pvc.DeletionTimestamp != nil { return fmt.Errorf(\"persistentvolumeclaim %q is being deleted\", pvc.Name) } } return nil } 5.2. findNodesThatFit 预选，通过预选函数来判断每个节点是否适合被该Pod调度。 具体的findNodesThatFit代码实现细节待后续文章独立分析。 genericScheduler.Schedule中对findNodesThatFit的调用过程如下： trace.Step(\"Computing predicates\") startPredicateEvalTime := time.Now() filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) if err != nil { return \"\", err } if len(filteredNodes) == 0 { return \"\", &FitError{ Pod: pod, NumAllNodes: len(nodes), FailedPredicates: failedPredicateMap, } } metrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime)) 5.3. PrioritizeNodes 优选，从满足的节点中选择出最优的节点。 具体操作如下： PrioritizeNodes通过并行运行各个优先级函数来对节点进行优先级排序。 每个优先级函数会给节点打分，打分范围为0-10分。 0 表示优先级最低的节点，10表示优先级最高的节点。 每个优先级函数也有各自的权重。 优先级函数返回的节点分数乘以权重以获得加权分数。 最后组合（添加）所有分数以获得所有节点的总加权分数。 具体PrioritizeNodes的实现逻辑待后续文章独立分析。 genericScheduler.Schedule中对PrioritizeNodes的调用过程如下： trace.Step(\"Prioritizing\") startPriorityEvalTime := time.Now() // When only one node after predicate, just use it. if len(filteredNodes) == 1 { metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) return filteredNodes[0].Name, nil } metaPrioritiesInterface := g.priorityMetaProducer(pod, g.cachedNodeInfoMap) priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) if err != nil { return \"\", err } metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime)) 5.4. selectHost scheduler在最后会从priorityList中选择分数最高的一个节点。 trace.Step(\"Selecting host\") return g.selectHost(priorityList) selectHost获取优先级的节点列表，然后从分数最高的节点以循环方式选择一个节点。 具体代码如下： // selectHost takes a prioritized list of nodes and then picks one // in a round-robin manner from the nodes that had the highest score. func (g *genericScheduler) selectHost(priorityList schedulerapi.HostPriorityList) (string, error) { if len(priorityList) == 0 { return \"\", fmt.Errorf(\"empty priorityList\") } maxScores := findMaxScores(priorityList) ix := int(g.lastNodeIndex % uint64(len(maxScores))) g.lastNodeIndex++ return priorityList[maxScores[ix]].Host, nil } 5.4.1. findMaxScores findMaxScores返回priorityList中具有最高Score的节点的索引。 // findMaxScores returns the indexes of nodes in the \"priorityList\" that has the highest \"Score\". func findMaxScores(priorityList schedulerapi.HostPriorityList) []int { maxScoreIndexes := make([]int, 0, len(priorityList)/2) maxScore := priorityList[0].Score for i, hp := range priorityList { if hp.Score > maxScore { maxScore = hp.Score maxScoreIndexes = maxScoreIndexes[:0] maxScoreIndexes = append(maxScoreIndexes, i) } else if hp.Score == maxScore { maxScoreIndexes = append(maxScoreIndexes, i) } } return maxScoreIndexes } 6. Scheduler.preempt 如果pod在预选和优选调度中失败，则执行抢占操作。抢占主要是将低优先级的pod的资源空间腾出给待调度的高优先级的pod。 具体Scheduler.preempt的实现逻辑待后续文章独立分析。 suggestedHost, err := sched.schedule(pod) if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() metrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime)) } return } 7. Scheduler.assume 将该pod和选定的节点进行假性绑定，存入scheduler cache中，方便可以继续执行调度逻辑，而不需要等待绑定操作的发生，具体绑定操作可以异步进行。 // Tell the cache to assume that a pod now is running on a given node, even though it hasn't been bound yet. // This allows us to keep scheduling without waiting on binding to occur. assumedPod := pod.DeepCopy() // Assume volumes first before assuming the pod. // // If all volumes are completely bound, then allBound is true and binding will be skipped. // // Otherwise, binding of volumes is started after the pod is assumed, but before pod binding. // // This function modifies 'assumedPod' if volume binding is required. allBound, err := sched.assumeVolumes(assumedPod, suggestedHost) if err != nil { return } // assume modifies `assumedPod` by setting NodeName=suggestedHost err = sched.assume(assumedPod, suggestedHost) if err != nil { return } 如果假性绑定成功则发送请求给apiserver，如果失败则scheduler会立即释放已分配给假性绑定的pod的资源。 assume方法的具体实现： // assume signals to the cache that a pod is already in the cache, so that binding can be asynchronous. // assume modifies `assumed`. func (sched *Scheduler) assume(assumed *v1.Pod, host string) error { // Optimistically assume that the binding will succeed and send it to apiserver // in the background. // If the binding fails, scheduler will release resources allocated to assumed pod // immediately. assumed.Spec.NodeName = host // NOTE: Because the scheduler uses snapshots of SchedulerCache and the live // version of Ecache, updates must be written to SchedulerCache before // invalidating Ecache. if err := sched.config.SchedulerCache.AssumePod(assumed); err != nil { glog.Errorf(\"scheduler cache AssumePod failed: %v\", err) // This is most probably result of a BUG in retrying logic. // We report an error here so that pod scheduling can be retried. // This relies on the fact that Error will check if the pod has been bound // to a node and if so will not add it back to the unscheduled pods queue // (otherwise this would cause an infinite loop). sched.config.Error(assumed, err) sched.config.Recorder.Eventf(assumed, v1.EventTypeWarning, \"FailedScheduling\", \"AssumePod failed: %v\", err) sched.config.PodConditionUpdater.Update(assumed, &v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, Reason: \"SchedulerError\", Message: err.Error(), }) return err } // Optimistically assume that the binding will succeed, so we need to invalidate affected // predicates in equivalence cache. // If the binding fails, these invalidated item will not break anything. if sched.config.Ecache != nil { sched.config.Ecache.InvalidateCachedPredicateItemForPodAdd(assumed, host) } return nil } 8. Scheduler.bind 异步的方式给pod绑定到具体的调度节点上。 // bind the pod to its host asynchronously (we can do this b/c of the assumption step above). go func() { // Bind volumes first before Pod if !allBound { err = sched.bindVolumes(assumedPod) if err != nil { return } } err := sched.bind(assumedPod, &v1.Binding{ ObjectMeta: metav1.ObjectMeta{Namespace: assumedPod.Namespace, Name: assumedPod.Name, UID: assumedPod.UID}, Target: v1.ObjectReference{ Kind: \"Node\", Name: suggestedHost, }, }) metrics.E2eSchedulingLatency.Observe(metrics.SinceInMicroseconds(start)) if err != nil { glog.Errorf(\"Internal error binding pod: (%v)\", err) } }() bind具体实现如下： // bind binds a pod to a given node defined in a binding object. We expect this to run asynchronously, so we // handle binding metrics internally. func (sched *Scheduler) bind(assumed *v1.Pod, b *v1.Binding) error { bindingStart := time.Now() // If binding succeeded then PodScheduled condition will be updated in apiserver so that // it's atomic with setting host. err := sched.config.GetBinder(assumed).Bind(b) if err := sched.config.SchedulerCache.FinishBinding(assumed); err != nil { glog.Errorf(\"scheduler cache FinishBinding failed: %v\", err) } if err != nil { glog.V(1).Infof(\"Failed to bind pod: %v/%v\", assumed.Namespace, assumed.Name) if err := sched.config.SchedulerCache.ForgetPod(assumed); err != nil { glog.Errorf(\"scheduler cache ForgetPod failed: %v\", err) } sched.config.Error(assumed, err) sched.config.Recorder.Eventf(assumed, v1.EventTypeWarning, \"FailedScheduling\", \"Binding rejected: %v\", err) sched.config.PodConditionUpdater.Update(assumed, &v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, Reason: \"BindingRejected\", }) return err } metrics.BindingLatency.Observe(metrics.SinceInMicroseconds(bindingStart)) metrics.SchedulingLatency.WithLabelValues(metrics.Binding).Observe(metrics.SinceInSeconds(bindingStart)) sched.config.Recorder.Eventf(assumed, v1.EventTypeNormal, \"Scheduled\", \"Successfully assigned %v/%v to %v\", assumed.Namespace, assumed.Name, b.Target.Name) return nil } 9. 总结 本文主要分析了单个pod的调度过程。具体流程如下： 通过podQueue的待调度队列中弹出需要调度的pod。 通过具体的调度算法为该pod选出合适的节点，其中调度算法就包括预选和优选两步策略。 如果上述调度失败，则会尝试抢占机制，将优先级低的pod剔除，让优先级高的pod调度成功。 将该pod和选定的节点进行假性绑定，存入scheduler cache中，方便具体绑定操作可以异步进行。 实际执行绑定操作，将node的名字添加到pod的节点相关属性中。 其中核心的部分为通过具体的调度算法选出调度节点的过程，即genericScheduler.Schedule的实现部分。该部分包括预选和优选两个部分。 genericScheduler.Schedule调度的基本流程如下： 对pod做基本性检查，目前主要是对pvc的检查。 通过findNodesThatFit预选策略选出满足调度条件的node列表。 通过PrioritizeNodes优选策略给预选的node列表中的node进行打分。 在打分的node列表中选择一个分数最高的node作为调度的节点。 参考： https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/core/generic_scheduler.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kube-scheduler/findNodesThatFit.html":{"url":"kube-scheduler/findNodesThatFit.html","title":"findNodesThatFit","keywords":"","body":"kube-scheduler源码分析（四）之 findNodesThatFit 以下代码分析基于 kubernetes v1.12.0 版本。 本文主要分析调度逻辑中的预选策略，即第一步筛选出符合pod调度条件的节点。 1. 调用入口 预选，通过预选函数来判断每个节点是否适合被该Pod调度。 genericScheduler.Schedule中对findNodesThatFit的调用过程如下： 此部分代码位于pkg/scheduler/core/generic_scheduler.go func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { ... // 列出所有的节点 nodes, err := nodeLister.List() if err != nil { return \"\", err } if len(nodes) == 0 { return \"\", ErrNoNodesAvailable } // Used for all fit and priority funcs. err = g.cache.UpdateNodeNameToInfoMap(g.cachedNodeInfoMap) if err != nil { return \"\", err } trace.Step(\"Computing predicates\") startPredicateEvalTime := time.Now() // 调用findNodesThatFit过滤出预选节点 filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) if err != nil { return \"\", err } if len(filteredNodes) == 0 { return \"\", &FitError{ Pod: pod, NumAllNodes: len(nodes), FailedPredicates: failedPredicateMap, } } // metrics metrics.SchedulingAlgorithmPredicateEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPredicateEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PredicateEvaluation).Observe(metrics.SinceInSeconds(startPredicateEvalTime)) ... } 核心代码： // 调用findNodesThatFit过滤出预选节点 filteredNodes, failedPredicateMap, err := g.findNodesThatFit(pod, nodes) 2. findNodesThatFit findNodesThatFit基于给定的预选函数过滤node，每个node传入到预选函数中来确实该节点是否符合要求。 findNodesThatFit的入参是被调度的pod和当前的节点列表，返回预选节点列表和错误。 findNodesThatFit基本流程如下： 设置可行节点的总数，作为预选节点数组的容量，避免总节点过多需要筛选的节点过多。 通过NodeTree不断获取下一个节点来判断该节点是否满足pod的调度条件。 通过之前注册的各种预选函数来判断当前节点是否符合pod的调度条件。 最后返回满足调度条件的node列表，供下一步的优选操作。 findNodesThatFit完整代码如下： 此部分代码位于pkg/scheduler/core/generic_scheduler.go // Filters the nodes to find the ones that fit based on the given predicate functions // Each node is passed through the predicate functions to determine if it is a fit func (g *genericScheduler) findNodesThatFit(pod *v1.Pod, nodes []*v1.Node) ([]*v1.Node, FailedPredicateMap, error) { var filtered []*v1.Node failedPredicateMap := FailedPredicateMap{} if len(g.predicates) == 0 { filtered = nodes } else { allNodes := int32(g.cache.NodeTree().NumNodes) numNodesToFind := g.numFeasibleNodesToFind(allNodes) // Create filtered list with enough space to avoid growing it // and allow assigning. filtered = make([]*v1.Node, numNodesToFind) errs := errors.MessageCountMap{} var ( predicateResultLock sync.Mutex filteredLen int32 equivClass *equivalence.Class ) ctx, cancel := context.WithCancel(context.Background()) // We can use the same metadata producer for all nodes. meta := g.predicateMetaProducer(pod, g.cachedNodeInfoMap) if g.equivalenceCache != nil { // getEquivalenceClassInfo will return immediately if no equivalence pod found equivClass = equivalence.NewClass(pod) } checkNode := func(i int) { var nodeCache *equivalence.NodeCache nodeName := g.cache.NodeTree().Next() if g.equivalenceCache != nil { nodeCache, _ = g.equivalenceCache.GetNodeCache(nodeName) } fits, failedPredicates, err := podFitsOnNode( pod, meta, g.cachedNodeInfoMap[nodeName], g.predicates, g.cache, nodeCache, g.schedulingQueue, g.alwaysCheckAllPredicates, equivClass, ) if err != nil { predicateResultLock.Lock() errs[err.Error()]++ predicateResultLock.Unlock() return } if fits { length := atomic.AddInt32(&filteredLen, 1) if length > numNodesToFind { cancel() atomic.AddInt32(&filteredLen, -1) } else { filtered[length-1] = g.cachedNodeInfoMap[nodeName].Node() } } else { predicateResultLock.Lock() failedPredicateMap[nodeName] = failedPredicates predicateResultLock.Unlock() } } // Stops searching for more nodes once the configured number of feasible nodes // are found. workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) filtered = filtered[:filteredLen] if len(errs) > 0 { return []*v1.Node{}, FailedPredicateMap{}, errors.CreateAggregateFromMessageCountMap(errs) } } if len(filtered) > 0 && len(g.extenders) != 0 { for _, extender := range g.extenders { if !extender.IsInterested(pod) { continue } filteredList, failedMap, err := extender.Filter(pod, filtered, g.cachedNodeInfoMap) if err != nil { if extender.IsIgnorable() { glog.Warningf(\"Skipping extender %v as it returned error %v and has ignorable flag set\", extender, err) continue } else { return []*v1.Node{}, FailedPredicateMap{}, err } } for failedNodeName, failedMsg := range failedMap { if _, found := failedPredicateMap[failedNodeName]; !found { failedPredicateMap[failedNodeName] = []algorithm.PredicateFailureReason{} } failedPredicateMap[failedNodeName] = append(failedPredicateMap[failedNodeName], predicates.NewFailureReason(failedMsg)) } filtered = filteredList if len(filtered) == 0 { break } } } return filtered, failedPredicateMap, nil } 以下对findNodesThatFit分段分析。 3. numFeasibleNodesToFind findNodesThatFit先基于所有的节点找出可行的节点是总数。numFeasibleNodesToFind的作用主要是避免当节点过多（超过100）影响调度的效率。 allNodes := int32(g.cache.NodeTree().NumNodes) numNodesToFind := g.numFeasibleNodesToFind(allNodes) // Create filtered list with enough space to avoid growing it // and allow assigning. filtered = make([]*v1.Node, numNodesToFind) numFeasibleNodesToFind基本流程如下： 如果所有的node节点小于minFeasibleNodesToFind(当前默认为100)则返回节点数。 如果节点数超100，则取指定计分的百分比的节点数，当该百分比后的数目仍小于minFeasibleNodesToFind，则返回minFeasibleNodesToFind。 如果百分比后的数目大于minFeasibleNodesToFind，则返回该百分比。 // numFeasibleNodesToFind returns the number of feasible nodes that once found, the scheduler stops // its search for more feasible nodes. func (g *genericScheduler) numFeasibleNodesToFind(numAllNodes int32) int32 { if numAllNodes = 100 { return numAllNodes } numNodes := numAllNodes * g.percentageOfNodesToScore / 100 if numNodes 4. checkNode checkNode是一个校验node是否符合要求的函数，其中实际调用到的核心函数是podFitsOnNode。再通过workqueue并发执行checkNode操作。 checkNode主要流程如下： 通过cache中的nodeTree不断获取下一个node。 将当前node和pod传入podFitsOnNode判断当前node是否符合要求。 如果当前node符合要求就将当前node加入预选节点的数组中filtered。 如果当前node不满足要求，则加入到失败的数组中，并记录原因。 通过workqueue.ParallelizeUntil并发执行checkNode函数，一旦找到配置的可行节点数，就停止搜索更多节点。 checkNode := func(i int) { var nodeCache *equivalence.NodeCache nodeName := g.cache.NodeTree().Next() if g.equivalenceCache != nil { nodeCache, _ = g.equivalenceCache.GetNodeCache(nodeName) } fits, failedPredicates, err := podFitsOnNode( pod, meta, g.cachedNodeInfoMap[nodeName], g.predicates, g.cache, nodeCache, g.schedulingQueue, g.alwaysCheckAllPredicates, equivClass, ) if err != nil { predicateResultLock.Lock() errs[err.Error()]++ predicateResultLock.Unlock() return } if fits { length := atomic.AddInt32(&filteredLen, 1) if length > numNodesToFind { cancel() atomic.AddInt32(&filteredLen, -1) } else { filtered[length-1] = g.cachedNodeInfoMap[nodeName].Node() } } else { predicateResultLock.Lock() failedPredicateMap[nodeName] = failedPredicates predicateResultLock.Unlock() } } workqueue的并发操作： // Stops searching for more nodes once the configured number of feasible nodes // are found. workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) ParallelizeUntil具体代码如下： // ParallelizeUntil is a framework that allows for parallelizing N // independent pieces of work until done or the context is canceled. func ParallelizeUntil(ctx context.Context, workers, pieces int, doWorkPiece DoWorkPieceFunc) { var stop 5. podFitsOnNode podFitsOnNode主要内容如下： podFitsOnNode会检查给定的某个Node是否满足预选的函数。 对于给定的pod，podFitsOnNode会检查是否有相同的pod存在，尽量复用缓存过的预选结果。 podFitsOnNode主要在Schedule（调度）和Preempt（抢占）的时候被调用。 当在Schedule中被调用的时候，主要判断是否可以被调度到当前节点，依据为当前节点上所有已存在的pod及被提名要运行到该节点的具有相等或更高优先级的pod。 当在Preempt中被调用的时候，即发生抢占的时候，通过SelectVictimsOnNode函数选出需要被移除的pod，移除后然后将预调度的pod调度到该节点上。 podFitsOnNode基本流程如下： 遍历之前注册好的预选策略predicates.Ordering，并获取预选策略的执行函数。 遍历执行每个预选函数，并返回是否合适，预选失败的原因和错误。 如果预选函数执行的结果不合适，则加入预选失败的数组中。 最后返回预选失败的个数是否为0，和预选失败的原因。 入参： pod PredicateMetadata NodeInfo predicateFuncs schedulercache.Cache nodeCache SchedulingQueue alwaysCheckAllPredicates equivClass 出参： fit PredicateFailureReason 完整代码如下： 此部分代码位于pkg/scheduler/core/generic_scheduler.go // podFitsOnNode checks whether a node given by NodeInfo satisfies the given predicate functions. // For given pod, podFitsOnNode will check if any equivalent pod exists and try to reuse its cached // predicate results as possible. // This function is called from two different places: Schedule and Preempt. // When it is called from Schedule, we want to test whether the pod is schedulable // on the node with all the existing pods on the node plus higher and equal priority // pods nominated to run on the node. // When it is called from Preempt, we should remove the victims of preemption and // add the nominated pods. Removal of the victims is done by SelectVictimsOnNode(). // It removes victims from meta and NodeInfo before calling this function. func podFitsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, info *schedulercache.NodeInfo, predicateFuncs map[string]algorithm.FitPredicate, cache schedulercache.Cache, nodeCache *equivalence.NodeCache, queue SchedulingQueue, alwaysCheckAllPredicates bool, equivClass *equivalence.Class, ) (bool, []algorithm.PredicateFailureReason, error) { var ( eCacheAvailable bool failedPredicates []algorithm.PredicateFailureReason ) podsAdded := false // We run predicates twice in some cases. If the node has greater or equal priority // nominated pods, we run them when those pods are added to meta and nodeInfo. // If all predicates succeed in this pass, we run them again when these // nominated pods are not added. This second pass is necessary because some // predicates such as inter-pod affinity may not pass without the nominated pods. // If there are no nominated pods for the node or if the first run of the // predicates fail, we don't run the second pass. // We consider only equal or higher priority pods in the first pass, because // those are the current \"pod\" must yield to them and not take a space opened // for running them. It is ok if the current \"pod\" take resources freed for // lower priority pods. // Requiring that the new pod is schedulable in both circumstances ensures that // we are making a conservative decision: predicates like resources and inter-pod // anti-affinity are more likely to fail when the nominated pods are treated // as running, while predicates like pod affinity are more likely to fail when // the nominated pods are treated as not running. We can't just assume the // nominated pods are running because they are not running right now and in fact, // they may end up getting scheduled to a different node. for i := 0; i 5.1. predicateFuncs 根据之前初注册好的预选策略函数来执行预选，判断节点是否符合调度。 for _, predicateKey := range predicates.Ordering() { if predicate, exist := predicateFuncs[predicateKey]; exist { if eCacheAvailable { fit, reasons, err = nodeCache.RunPredicate(predicate, predicateKey, pod, metaToUse, nodeInfoToUse, equivClass, cache) } else { fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) } 预选策略如下： var ( predicatesOrdering = []string{CheckNodeConditionPred, CheckNodeUnschedulablePred, GeneralPred, HostNamePred, PodFitsHostPortsPred, MatchNodeSelectorPred, PodFitsResourcesPred, NoDiskConflictPred, PodToleratesNodeTaintsPred, PodToleratesNodeNoExecuteTaintsPred, CheckNodeLabelPresencePred, CheckServiceAffinityPred, MaxEBSVolumeCountPred, MaxGCEPDVolumeCountPred, MaxCSIVolumeCountPred, MaxAzureDiskVolumeCountPred, CheckVolumeBindingPred, NoVolumeZoneConflictPred, CheckNodeMemoryPressurePred, CheckNodePIDPressurePred, CheckNodeDiskPressurePred, MatchInterPodAffinityPred} ) 6. PodFitsResources 以下以PodFitsResources这个预选函数为例做分析，其他重要的预选函数待后续单独分析。 PodFitsResources用来检查一个节点是否有足够的资源来运行当前的pod，包括CPU、内存、GPU等。 PodFitsResources基本流程如下： 判断当前节点上pod总数加上预调度pod个数是否大于node的可分配pod总数，若是则不允许调度。 判断pod的request值是否都为0，若是则允许调度。 判断pod的request值加上当前node上所有pod的request值总和是否大于node的可分配资源，若是则不允许调度。 判断pod的拓展资源request值加上当前node上所有pod对应的request值总和是否大于node对应的可分配资源，若是则不允许调度。 PodFitsResources的注册代码如下： factory.RegisterFitPredicate(predicates.PodFitsResourcesPred, predicates.PodFitsResources) PodFitsResources入参： pod nodeInfo PredicateMetadata PodFitsResources出参： fit PredicateFailureReason PodFitsResources完整代码： 此部分的代码位于pkg/scheduler/algorithm/predicates/predicates.go // PodFitsResources checks if a node has sufficient resources, such as cpu, memory, gpu, opaque int resources etc to run a pod. // First return value indicates whether a node has sufficient resources to run a pod while the second return value indicates the // predicate failure reasons if the node has insufficient resources to run the pod. func PodFitsResources(pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo) (bool, []algorithm.PredicateFailureReason, error) { node := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf(\"node not found\") } var predicateFails []algorithm.PredicateFailureReason allowedPodNumber := nodeInfo.AllowedPodNumber() if len(nodeInfo.Pods())+1 > allowedPodNumber { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourcePods, 1, int64(len(nodeInfo.Pods())), int64(allowedPodNumber))) } // No extended resources should be ignored by default. ignoredExtendedResources := sets.NewString() var podRequest *schedulercache.Resource if predicateMeta, ok := meta.(*predicateMetadata); ok { podRequest = predicateMeta.podRequest if predicateMeta.ignoredExtendedResources != nil { ignoredExtendedResources = predicateMeta.ignoredExtendedResources } } else { // We couldn't parse metadata - fallback to computing it. podRequest = GetResourceRequest(pod) } if podRequest.MilliCPU == 0 && podRequest.Memory == 0 && podRequest.EphemeralStorage == 0 && len(podRequest.ScalarResources) == 0 { return len(predicateFails) == 0, predicateFails, nil } allocatable := nodeInfo.AllocatableResource() if allocatable.MilliCPU 6.1. NodeInfo NodeInfo是node的聚合信息，主要包括： node：k8s node的结构体 pods：当前node上pod的数量 requestedResource：当前node上所有pod的request总和 allocatableResource：node的实际所有的可分配资源(对应于Node.Status.Allocatable.*)，可理解为node的资源总量。 此部分代码位于pkg/scheduler/cache/node_info.go // NodeInfo is node level aggregated information. type NodeInfo struct { // Overall node information. node *v1.Node pods []*v1.Pod podsWithAffinity []*v1.Pod usedPorts util.HostPortInfo // Total requested resource of all pods on this node. // It includes assumed pods which scheduler sends binding to apiserver but // didn't get it as scheduled yet. requestedResource *Resource nonzeroRequest *Resource // We store allocatedResources (which is Node.Status.Allocatable.*) explicitly // as int64, to avoid conversions and accessing map. allocatableResource *Resource // Cached taints of the node for faster lookup. taints []v1.Taint taintsErr error // imageStates holds the entry of an image if and only if this image is on the node. The entry can be used for // checking an image's existence and advanced usage (e.g., image locality scheduling policy) based on the image // state information. imageStates map[string]*ImageStateSummary // TransientInfo holds the information pertaining to a scheduling cycle. This will be destructed at the end of // scheduling cycle. // TODO: @ravig. Remove this once we have a clear approach for message passing across predicates and priorities. TransientInfo *transientSchedulerInfo // Cached conditions of node for faster lookup. memoryPressureCondition v1.ConditionStatus diskPressureCondition v1.ConditionStatus pidPressureCondition v1.ConditionStatus // Whenever NodeInfo changes, generation is bumped. // This is used to avoid cloning it if the object didn't change. generation int64 } 6.2. Resource Resource是可计算资源的集合体。主要包括： MilliCPU Memory EphemeralStorage AllowedPodNumber：允许的pod总数(对应于Node.Status.Allocatable.Pods().Value())，一般为110。 ScalarResources // Resource is a collection of compute resource. type Resource struct { MilliCPU int64 Memory int64 EphemeralStorage int64 // We store allowedPodNumber (which is Node.Status.Allocatable.Pods().Value()) // explicitly as int, to avoid conversions and improve performance. AllowedPodNumber int // ScalarResources ScalarResources map[v1.ResourceName]int64 } 以下分析podFitsOnNode的具体流程。 6.3. allowedPodNumber 首先获取节点的信息，先判断如果该节点当前所有的pod的个数加上当前预调度的pod是否会大于该节点允许的pod的总数，一般为110个。如果超过，则predicateFails数组增加1，即当前节点不适合该pod。 node := nodeInfo.Node() if node == nil { return false, nil, fmt.Errorf(\"node not found\") } var predicateFails []algorithm.PredicateFailureReason allowedPodNumber := nodeInfo.AllowedPodNumber() if len(nodeInfo.Pods())+1 > allowedPodNumber { predicateFails = append(predicateFails, NewInsufficientResourceError(v1.ResourcePods, 1, int64(len(nodeInfo.Pods())), int64(allowedPodNumber))) } 6.4. podRequest 如果podRequest都为0，则允许调度到该节点，直接返回结果。 if podRequest.MilliCPU == 0 && podRequest.Memory == 0 && podRequest.EphemeralStorage == 0 && len(podRequest.ScalarResources) == 0 { return len(predicateFails) == 0, predicateFails, nil } 6.5. AllocatableResource 如果当前预调度的pod的request资源加上当前node上所有pod的request总和大于该node的可分配资源总量，则不允许调度到该节点，直接返回结果。其中request资源包括CPU、内存、storage。 allocatable := nodeInfo.AllocatableResource() if allocatable.MilliCPU 6.6. ScalarResources 判断其他拓展的标量资源，是否该pod的request值加上当前node上所有pod的对应资源的request总和大于该node上对应资源的可分配总量，如果是，则不允许调度到该节点。 for rName, rQuant := range podRequest.ScalarResources { if v1helper.IsExtendedResourceName(rName) { // If this resource is one of the extended resources that should be // ignored, we will skip checking it. if ignoredExtendedResources.Has(string(rName)) { continue } } if allocatable.ScalarResources[rName] 7. 总结 findNodesThatFit基于给定的预选函数过滤node，每个node传入到预选函数中来确实该节点是否符合要求。 findNodesThatFit的入参是被调度的pod和当前的节点列表，返回预选节点列表和错误。 findNodesThatFit基本流程如下： 设置可行节点的总数，作为预选节点数组的容量，避免总节点过多导致需要筛选的节点过多，效率低。 通过NodeTree不断获取下一个节点来判断该节点是否满足pod的调度条件。 通过之前注册的各种预选函数来判断当前节点是否符合pod的调度条件。 最后返回满足调度条件的node列表，供下一步的优选操作。 7.1. checkNode checkNode是一个校验node是否符合要求的函数，其中实际调用到的核心函数是podFitsOnNode。再通过workqueue并发执行checkNode操作。 checkNode主要流程如下： 通过cache中的nodeTree不断获取下一个node。 将当前node和pod传入podFitsOnNode判断当前node是否符合要求。 如果当前node符合要求就将当前node加入预选节点的数组中filtered。 如果当前node不满足要求，则加入到失败的数组中，并记录原因。 通过workqueue.ParallelizeUntil并发执行checkNode函数，一旦找到配置的可行节点数，就停止搜索更多节点。 7.2. podFitsOnNode 其中会调用到核心函数podFitsOnNode。 podFitsOnNode主要内容如下： podFitsOnNode会检查给定的某个Node是否满足预选的函数。 对于给定的pod，podFitsOnNode会检查是否有相同的pod存在，尽量复用缓存过的预选结果。 podFitsOnNode主要在Schedule（调度）和Preempt（抢占）的时候被调用。 当在Schedule中被调用的时候，主要判断是否可以被调度到当前节点，依据为当前节点上所有已存在的pod及被提名要运行到该节点的具有相等或更高优先级的pod。 当在Preempt中被调用的时候，即发生抢占的时候，通过SelectVictimsOnNode函数选出需要被移除的pod，移除后然后将预调度的pod调度到该节点上。 podFitsOnNode基本流程如下： 遍历之前注册好的预选策略predicates.Ordering，并获取预选策略的执行函数。 遍历执行每个预选函数，并返回是否合适，预选失败的原因和错误。 如果预选函数执行的结果不合适，则加入预选失败的数组中。 最后返回预选失败的个数是否为0，和预选失败的原因。 7.3. PodFitsResources 本文只示例分析了其中一个重要的预选函数：PodFitsResources PodFitsResources用来检查一个节点是否有足够的资源来运行当前的pod，包括CPU、内存、GPU等。 PodFitsResources基本流程如下： 判断当前节点上pod总数加上预调度pod个数是否大于node的可分配pod总数，若是则不允许调度。 判断pod的request值是否都为0，若是则允许调度。 判断pod的request值加上当前node上所有pod的request值总和是否大于node的可分配资源，若是则不允许调度。 判断pod的拓展资源request值加上当前node上所有pod对应的request值总和是否大于node对应的可分配资源，若是则不允许调度。 参考： https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/core/generic_scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/algorithm/predicates/predicates.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kube-scheduler/PrioritizeNodes.html":{"url":"kube-scheduler/PrioritizeNodes.html","title":"PrioritizeNodes","keywords":"","body":"kube-scheduler源码分析（五）之 PrioritizeNodes 以下代码分析基于 kubernetes v1.12.0 版本。 本文主要分析优选策略逻辑，即从预选的节点中选择出最优的节点。优选策略的具体实现函数为PrioritizeNodes。PrioritizeNodes最终返回是一个记录了各个节点分数的列表。 1. 调用入口 genericScheduler.Schedule中对PrioritizeNodes的调用过程如下： 此部分代码位于pkg/scheduler/core/generic_scheduler.go func (g *genericScheduler) Schedule(pod *v1.Pod, nodeLister algorithm.NodeLister) (string, error) { ... trace.Step(\"Prioritizing\") startPriorityEvalTime := time.Now() // When only one node after predicate, just use it. if len(filteredNodes) == 1 { metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) return filteredNodes[0].Name, nil } metaPrioritiesInterface := g.priorityMetaProducer(pod, g.cachedNodeInfoMap) // 执行优选逻辑的操作，返回记录各个节点分数的列表 priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) if err != nil { return \"\", err } metrics.SchedulingAlgorithmPriorityEvaluationDuration.Observe(metrics.SinceInMicroseconds(startPriorityEvalTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PriorityEvaluation).Observe(metrics.SinceInSeconds(startPriorityEvalTime)) ... } 核心代码： // 基于预选节点filteredNodes进一步筛选优选的节点，返回记录各个节点分数的列表 priorityList, err := PrioritizeNodes(pod, g.cachedNodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders) 2. PrioritizeNodes 优选，从满足的节点中选择出最优的节点。PrioritizeNodes最终返回是一个记录了各个节点分数的列表。 具体操作如下： PrioritizeNodes通过并行运行各个优先级函数来对节点进行优先级排序。 每个优先级函数会给节点打分，打分范围为0-10分。 0 表示优先级最低的节点，10表示优先级最高的节点。 每个优先级函数也有各自的权重。 优先级函数返回的节点分数乘以权重以获得加权分数。 最后组合（添加）所有分数以获得所有节点的总加权分数。 PrioritizeNodes主要流程如下： 如果没有设置优选函数和拓展函数，则全部节点设置相同的分数，直接返回。 依次给node执行map函数进行打分。 再对上述map函数的执行结果执行reduce函数计算最终得分。 最后根据不同优先级函数的权重对得分取加权平均数。 入参： pod nodeNameToInfo meta interface{}, priorityConfigs nodes extenders 出参： HostPriorityList：记录节点分数的列表。 HostPriority定义如下: // HostPriority represents the priority of scheduling to a particular host, higher priority is better. type HostPriority struct { // Name of the host Host string // Score associated with the host Score int } PrioritizeNodes完整代码如下： 此部分代码位于pkg/scheduler/core/generic_scheduler.go // PrioritizeNodes prioritizes the nodes by running the individual priority functions in parallel. // Each priority function is expected to set a score of 0-10 // 0 is the lowest priority score (least preferred node) and 10 is the highest // Each priority function can also have its own weight // The node scores returned by the priority function are multiplied by the weights to get weighted scores // All scores are finally combined (added) to get the total weighted scores of all nodes func PrioritizeNodes( pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, meta interface{}, priorityConfigs []algorithm.PriorityConfig, nodes []*v1.Node, extenders []algorithm.SchedulerExtender, ) (schedulerapi.HostPriorityList, error) { // If no priority configs are provided, then the EqualPriority function is applied // This is required to generate the priority list in the required format if len(priorityConfigs) == 0 && len(extenders) == 0 { result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { hostPriority, err := EqualPriorityMap(pod, meta, nodeNameToInfo[nodes[i].Name]) if err != nil { return nil, err } result = append(result, hostPriority) } return result, nil } var ( mu = sync.Mutex{} wg = sync.WaitGroup{} errs []error ) appendError := func(err error) { mu.Lock() defer mu.Unlock() errs = append(errs, err) } results := make([]schedulerapi.HostPriorityList, len(priorityConfigs), len(priorityConfigs)) for i, priorityConfig := range priorityConfigs { if priorityConfig.Function != nil { // DEPRECATED wg.Add(1) go func(index int, config algorithm.PriorityConfig) { defer wg.Done() var err error results[index], err = config.Function(pod, nodeNameToInfo, nodes) if err != nil { appendError(err) } }(i, priorityConfig) } else { results[i] = make(schedulerapi.HostPriorityList, len(nodes)) } } processNode := func(index int) { nodeInfo := nodeNameToInfo[nodes[index].Name] var err error for i := range priorityConfigs { if priorityConfigs[i].Function != nil { continue } results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) if err != nil { appendError(err) return } } } workqueue.Parallelize(16, len(nodes), processNode) for i, priorityConfig := range priorityConfigs { if priorityConfig.Reduce == nil { continue } wg.Add(1) go func(index int, config algorithm.PriorityConfig) { defer wg.Done() if err := config.Reduce(pod, meta, nodeNameToInfo, results[index]); err != nil { appendError(err) } if glog.V(10) { for _, hostPriority := range results[index] { glog.Infof(\"%v -> %v: %v, Score: (%d)\", pod.Name, hostPriority.Host, config.Name, hostPriority.Score) } } }(i, priorityConfig) } // Wait for all computations to be finished. wg.Wait() if len(errs) != 0 { return schedulerapi.HostPriorityList{}, errors.NewAggregate(errs) } // Summarize all scores. result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { result = append(result, schedulerapi.HostPriority{Host: nodes[i].Name, Score: 0}) for j := range priorityConfigs { result[i].Score += results[j][i].Score * priorityConfigs[j].Weight } } if len(extenders) != 0 && nodes != nil { combinedScores := make(map[string]int, len(nodeNameToInfo)) for _, extender := range extenders { if !extender.IsInterested(pod) { continue } wg.Add(1) go func(ext algorithm.SchedulerExtender) { defer wg.Done() prioritizedList, weight, err := ext.Prioritize(pod, nodes) if err != nil { // Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities return } mu.Lock() for i := range *prioritizedList { host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score combinedScores[host] += score * weight } mu.Unlock() }(extender) } // wait for all go routines to finish wg.Wait() for i := range result { result[i].Score += combinedScores[result[i].Host] } } if glog.V(10) { for i := range result { glog.V(10).Infof(\"Host %s => Score %d\", result[i].Host, result[i].Score) } } return result, nil } 以下对PrioritizeNodes分段进行分析。 3. EqualPriorityMap 如果没有提供优选函数和拓展函数，则将所有的节点设置为相同的优先级，即节点的score都为1，然后直接返回结果。(但一般情况下优选函数列表都不为空) // If no priority configs are provided, then the EqualPriority function is applied // This is required to generate the priority list in the required format if len(priorityConfigs) == 0 && len(extenders) == 0 { result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { hostPriority, err := EqualPriorityMap(pod, meta, nodeNameToInfo[nodes[i].Name]) if err != nil { return nil, err } result = append(result, hostPriority) } return result, nil } EqualPriorityMap具体实现如下： // EqualPriorityMap is a prioritizer function that gives an equal weight of one to all nodes func EqualPriorityMap(_ *v1.Pod, _ interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } return schedulerapi.HostPriority{ Host: node.Name, Score: 1, }, nil } 4. processNode processNode就是基于index拿出node的信息，调用之前注册的各种优选函数（此处是mapFunction），通过优选函数对node和pod进行处理，最后返回一个记录node分数的列表result。processNode同样也使用workqueue.Parallelize来进行并行处理。(processNode类似于预选逻辑findNodesThatFit中使用到的checkNode的作用) 其中优选函数是通过priorityConfigs来记录，每类优选函数包括PriorityMapFunction和PriorityReduceFunction两种函数。优选函数的注册部分可参考registerAlgorithmProvider。 processNode := func(index int) { nodeInfo := nodeNameToInfo[nodes[index].Name] var err error for i := range priorityConfigs { if priorityConfigs[i].Function != nil { continue } results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) if err != nil { appendError(err) return } } } // 并行执行processNode workqueue.Parallelize(16, len(nodes), processNode) priorityConfigs定义如下： 核心属性： Map ：PriorityMapFunction Reduce：PriorityReduceFunction // PriorityConfig is a config used for a priority function. type PriorityConfig struct { Name string Map PriorityMapFunction Reduce PriorityReduceFunction // TODO: Remove it after migrating all functions to // Map-Reduce pattern. Function PriorityFunction Weight int } 具体的优选函数处理逻辑待下文分析，本文会以NewSelectorSpreadPriority函数为例。 5. PriorityMapFunction PriorityMapFunction是一个计算给定节点的每个节点结果的函数。 PriorityMapFunction定义如下： // PriorityMapFunction is a function that computes per-node results for a given node. // TODO: Figure out the exact API of this method. // TODO: Change interface{} to a specific type. type PriorityMapFunction func(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) PriorityMapFunction是在processNode中调用的，代码如下： results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) 下文会分析NewSelectorSpreadPriority在的map函数CalculateSpreadPriorityMap。 6. PriorityReduceFunction PriorityReduceFunction是一个聚合每个节点结果并计算所有节点的最终得分的函数。 PriorityReduceFunction定义如下： // PriorityReduceFunction is a function that aggregated per-node results and computes // final scores for all nodes. // TODO: Figure out the exact API of this method. // TODO: Change interface{} to a specific type. type PriorityReduceFunction func(pod *v1.Pod, meta interface{}, nodeNameToInfo map[string]*schedulercache.NodeInfo, result schedulerapi.HostPriorityList) error PrioritizeNodes中对reduce函数调用部分如下： for i, priorityConfig := range priorityConfigs { if priorityConfig.Reduce == nil { continue } wg.Add(1) go func(index int, config algorithm.PriorityConfig) { defer wg.Done() if err := config.Reduce(pod, meta, nodeNameToInfo, results[index]); err != nil { appendError(err) } if glog.V(10) { for _, hostPriority := range results[index] { glog.Infof(\"%v -> %v: %v, Score: (%d)\", pod.Name, hostPriority.Host, config.Name, hostPriority.Score) } } }(i, priorityConfig) } 下文会分析NewSelectorSpreadPriority在的reduce函数CalculateSpreadPriorityReduce。 7. Summarize all scores 先等待计算完成再计算加权平均数。 // Wait for all computations to be finished. wg.Wait() if len(errs) != 0 { return schedulerapi.HostPriorityList{}, errors.NewAggregate(errs) } 计算所有节点的加权平均数。 // Summarize all scores. result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes { result = append(result, schedulerapi.HostPriority{Host: nodes[i].Name, Score: 0}) for j := range priorityConfigs { result[i].Score += results[j][i].Score * priorityConfigs[j].Weight } } 当设置了拓展的计算方式，则增加拓展计算方式的加权平均数。 if len(extenders) != 0 && nodes != nil { combinedScores := make(map[string]int, len(nodeNameToInfo)) for _, extender := range extenders { if !extender.IsInterested(pod) { continue } wg.Add(1) go func(ext algorithm.SchedulerExtender) { defer wg.Done() prioritizedList, weight, err := ext.Prioritize(pod, nodes) if err != nil { // Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities return } mu.Lock() for i := range *prioritizedList { host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score combinedScores[host] += score * weight } mu.Unlock() }(extender) } // wait for all go routines to finish wg.Wait() for i := range result { result[i].Score += combinedScores[result[i].Host] } } 8. NewSelectorSpreadPriority 以下以NewSelectorSpreadPriority这个优选函数来做分析，其他重要的优选函数待后续专门分析。 NewSelectorSpreadPriority主要的功能是将属于相同service和rs下的pod尽量分布在不同的node上。 该函数的注册代码如下： 此部分代码位于pkg/scheduler/algorithmprovider/defaults/defaults.go // ServiceSpreadingPriority is a priority config factory that spreads pods by minimizing // the number of pods (belonging to the same service) on the same node. // Register the factory so that it's available, but do not include it as part of the default priorities // Largely replaced by \"SelectorSpreadPriority\", but registered for backward compatibility with 1.0 factory.RegisterPriorityConfigFactory( \"ServiceSpreadingPriority\", factory.PriorityConfigFactory{ MapReduceFunction: func(args factory.PluginFactoryArgs) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { return priorities.NewSelectorSpreadPriority(args.ServiceLister, algorithm.EmptyControllerLister{}, algorithm.EmptyReplicaSetLister{}, algorithm.EmptyStatefulSetLister{}) }, Weight: 1, }, ) NewSelectorSpreadPriority的具体实现如下： 此部分代码位于pkg/scheduler/algorithm/priorities/selector_spreading.go // NewSelectorSpreadPriority creates a SelectorSpread. func NewSelectorSpreadPriority( serviceLister algorithm.ServiceLister, controllerLister algorithm.ControllerLister, replicaSetLister algorithm.ReplicaSetLister, statefulSetLister algorithm.StatefulSetLister) (algorithm.PriorityMapFunction, algorithm.PriorityReduceFunction) { selectorSpread := &SelectorSpread{ serviceLister: serviceLister, controllerLister: controllerLister, replicaSetLister: replicaSetLister, statefulSetLister: statefulSetLister, } return selectorSpread.CalculateSpreadPriorityMap, selectorSpread.CalculateSpreadPriorityReduce } NewSelectorSpreadPriority主要包括map和reduce两种函数，分别对应CalculateSpreadPriorityMap，CalculateSpreadPriorityReduce。 8.1. CalculateSpreadPriorityMap CalculateSpreadPriorityMap的主要作用是将相同service、RC、RS或statefulset的pod分布在不同的节点上。当调度一个pod的时候，先寻找与该pod匹配的service、RS、RC或statefulset，然后寻找与其selector匹配的已存在的pod，寻找存在这类pod最少的节点。 基本流程如下： 寻找与该pod对应的service、RS、RC、statefulset匹配的selector。 遍历当前节点的所有pod，将该节点上已存在的selector匹配到的pod的个数作为该节点的分数（此时，分数大的表示匹配到的pod越多，越不符合被调度的条件，该分数在reduce阶段会被按10分制处理成分数大的越符合被调度的条件）。 此部分代码位于pkg/scheduler/algorithm/priorities/selector_spreading.go // CalculateSpreadPriorityMap spreads pods across hosts, considering pods // belonging to the same service,RC,RS or StatefulSet. // When a pod is scheduled, it looks for services, RCs,RSs and StatefulSets that match the pod, // then finds existing pods that match those selectors. // It favors nodes that have fewer existing matching pods. // i.e. it pushes the scheduler towards a node where there's the smallest number of // pods which match the same service, RC,RSs or StatefulSets selectors as the pod being scheduled. func (s *SelectorSpread) CalculateSpreadPriorityMap(pod *v1.Pod, meta interface{}, nodeInfo *schedulercache.NodeInfo) (schedulerapi.HostPriority, error) { var selectors []labels.Selector node := nodeInfo.Node() if node == nil { return schedulerapi.HostPriority{}, fmt.Errorf(\"node not found\") } priorityMeta, ok := meta.(*priorityMetadata) if ok { selectors = priorityMeta.podSelectors } else { selectors = getSelectors(pod, s.serviceLister, s.controllerLister, s.replicaSetLister, s.statefulSetLister) } if len(selectors) == 0 { return schedulerapi.HostPriority{ Host: node.Name, Score: int(0), }, nil } count := int(0) for _, nodePod := range nodeInfo.Pods() { if pod.Namespace != nodePod.Namespace { continue } // When we are replacing a failed pod, we often see the previous // deleted version while scheduling the replacement. // Ignore the previous deleted version for spreading purposes // (it can still be considered for resource restrictions etc.) if nodePod.DeletionTimestamp != nil { glog.V(4).Infof(\"skipping pending-deleted pod: %s/%s\", nodePod.Namespace, nodePod.Name) continue } for _, selector := range selectors { if selector.Matches(labels.Set(nodePod.ObjectMeta.Labels)) { count++ break } } } return schedulerapi.HostPriority{ Host: node.Name, Score: int(count), }, nil } 以下分段分析： 先获得selector。 selectors = getSelectors(pod, s.serviceLister, s.controllerLister, s.replicaSetLister, s.statefulSetLister) 计算节点上匹配selector的pod的个数，作为该节点分数，该分数并不是最终节点的分数，只是中间过渡的记录状态。 count := int(0) for _, nodePod := range nodeInfo.Pods() { ... for _, selector := range selectors { if selector.Matches(labels.Set(nodePod.ObjectMeta.Labels)) { count++ break } } } 8.2. CalculateSpreadPriorityReduce CalculateSpreadPriorityReduce根据节点上现有匹配pod的数量计算每个节点的十分制的分数，具有较少现有匹配pod的节点的分数越高，表示节点越可能被调度到。 基本流程如下： 记录所有节点中匹配到pod个数最多的节点的分数（即匹配到的pod最多的个数）。 遍历所有的节点，按比例取十分制的得分，计算方式为：(节点中最多匹配pod的个数-当前节点pod的个数)/节点中最多匹配pod的个数。此时，分数越高表示该节点上匹配到的pod的个数越少，越可能被调度到，即满足把相同selector的pod分散到不同节点的需求。 此部分代码位于pkg/scheduler/algorithm/priorities/selector_spreading.go // CalculateSpreadPriorityReduce calculates the source of each node // based on the number of existing matching pods on the node // where zone information is included on the nodes, it favors nodes // in zones with fewer existing matching pods. func (s *SelectorSpread) CalculateSpreadPriorityReduce(pod *v1.Pod, meta interface{}, nodeNameToInfo map[string]*schedulercache.NodeInfo, result schedulerapi.HostPriorityList) error { countsByZone := make(map[string]int, 10) maxCountByZone := int(0) maxCountByNodeName := int(0) for i := range result { if result[i].Score > maxCountByNodeName { maxCountByNodeName = result[i].Score } zoneID := utilnode.GetZoneKey(nodeNameToInfo[result[i].Host].Node()) if zoneID == \"\" { continue } countsByZone[zoneID] += result[i].Score } for zoneID := range countsByZone { if countsByZone[zoneID] > maxCountByZone { maxCountByZone = countsByZone[zoneID] } } haveZones := len(countsByZone) != 0 maxCountByNodeNameFloat64 := float64(maxCountByNodeName) maxCountByZoneFloat64 := float64(maxCountByZone) MaxPriorityFloat64 := float64(schedulerapi.MaxPriority) for i := range result { // initializing to the default/max node score of maxPriority fScore := MaxPriorityFloat64 if maxCountByNodeName > 0 { fScore = MaxPriorityFloat64 * (float64(maxCountByNodeName-result[i].Score) / maxCountByNodeNameFloat64) } // If there is zone information present, incorporate it if haveZones { zoneID := utilnode.GetZoneKey(nodeNameToInfo[result[i].Host].Node()) if zoneID != \"\" { zoneScore := MaxPriorityFloat64 if maxCountByZone > 0 { zoneScore = MaxPriorityFloat64 * (float64(maxCountByZone-countsByZone[zoneID]) / maxCountByZoneFloat64) } fScore = (fScore * (1.0 - zoneWeighting)) + (zoneWeighting * zoneScore) } } result[i].Score = int(fScore) if glog.V(10) { glog.Infof( \"%v -> %v: SelectorSpreadPriority, Score: (%d)\", pod.Name, result[i].Host, int(fScore), ) } } return nil } 以下分段分析： 先获取所有节点中匹配到的pod最多的个数。 for i := range result { if result[i].Score > maxCountByNodeName { maxCountByNodeName = result[i].Score } zoneID := utilnode.GetZoneKey(nodeNameToInfo[result[i].Host].Node()) if zoneID == \"\" { continue } countsByZone[zoneID] += result[i].Score } 遍历所有的节点，按比例取十分制的得分。 for i := range result { // initializing to the default/max node score of maxPriority fScore := MaxPriorityFloat64 if maxCountByNodeName > 0 { fScore = MaxPriorityFloat64 * (float64(maxCountByNodeName-result[i].Score) / maxCountByNodeNameFloat64) } ... } 9. 总结 优选，从满足的节点中选择出最优的节点。PrioritizeNodes最终返回是一个记录了各个节点分数的列表。 9.1. PrioritizeNodes 主要流程如下： 如果没有设置优选函数和拓展函数，则全部节点设置相同的分数，直接返回。 依次给node执行map函数进行打分。 再对上述map函数的执行结果执行reduce函数计算最终得分。 最后根据不同优先级函数的权重对得分取加权平均数。 其中每类优选函数会包含map函数和reduce函数两种。 9.2. NewSelectorSpreadPriority 其中以NewSelectorSpreadPriority这个优选函数为例作分析，该函数的功能是将相同service、RS、RC或statefulset下pod尽量分散到不同的节点上。包括map函数和reduce函数两部分，具体如下。 9.2.1. CalculateSpreadPriorityMap 基本流程如下： 寻找与该pod对应的service、RS、RC、statefulset匹配的selector。 遍历当前节点的所有pod，将该节点上已存在的selector匹配到的pod的个数作为该节点的分数（此时，分数大的表示匹配到的pod越多，越不符合被调度的条件，该分数在reduce阶段会被按10分制处理成分数大的越符合被调度的条件）。 9.2.2. CalculateSpreadPriorityReduce 基本流程如下： 记录所有节点中匹配到pod个数最多的节点的分数（即匹配到的pod最多的个数）。 遍历所有的节点，按比例取十分制的得分，计算方式为：(节点中最多匹配pod的个数-当前节点pod的个数)/节点中最多匹配pod的个数。此时，分数越高表示该节点上匹配到的pod的个数越少，越可能被调度到，即满足把相同selector的pod分散到不同节点的需求。 参考： https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/core/generic_scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/algorithm/priorities/selector_spreading.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kube-scheduler/preempt.html":{"url":"kube-scheduler/preempt.html","title":"preempt","keywords":"","body":"kube-scheduler源码分析（六）之 preempt 以下代码分析基于 kubernetes v1.12.0 版本。 本文主要分析调度中的抢占逻辑，当pod不适合任何节点的时候，可能pod会调度失败，这时候可能会发生抢占。抢占逻辑的具体实现函数为Scheduler.preempt。 1. 调用入口 当pod不适合任何节点的时候，可能pod会调度失败。这时候可能会发生抢占。 scheduleOne函数中关于抢占调用的逻辑如下： 此部分的代码位于/pkg/scheduler/scheduler.go // scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm's host fitting. func (sched *Scheduler) scheduleOne() { ... suggestedHost, err := sched.schedule(pod) if err != nil { // schedule() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if fitError, ok := err.(*core.FitError); ok { preemptionStartTime := time.Now() // 执行抢占逻辑 sched.preempt(pod, fitError) metrics.PreemptionAttempts.Inc() metrics.SchedulingAlgorithmPremptionEvaluationDuration.Observe(metrics.SinceInMicroseconds(preemptionStartTime)) metrics.SchedulingLatency.WithLabelValues(metrics.PreemptionEvaluation).Observe(metrics.SinceInSeconds(preemptionStartTime)) } return } ... } 其中核心代码为： // 基于sched.schedule(pod)返回的err和当前待调度的pod执行抢占策略 sched.preempt(pod, fitError) 2. Scheduler.preempt 当pod调度失败的时候，会抢占低优先级pod的空间来给高优先级的pod。其中入参为调度失败的pod对象和调度失败的err。 抢占的基本流程如下： 判断是否有关闭抢占机制，如果关闭抢占机制则直接返回。 获取调度失败pod的最新对象数据。 执行抢占算法Algorithm.Preempt，返回预调度节点和需要被剔除的pod列表。 将抢占算法返回的node添加到pod的Status.NominatedNodeName中，并删除需要被剔除的pod。 当抢占算法返回的node是nil的时候，清除pod的Status.NominatedNodeName信息。 整个抢占流程的最终结果实际上是更新Pod.Status.NominatedNodeName属性的信息。如果抢占算法返回的节点不为空，则将该node更新到Pod.Status.NominatedNodeName中，否则就将Pod.Status.NominatedNodeName设置为空。 2.1. preempt preempt的具体实现函数： 此部分的代码位于/pkg/scheduler/scheduler.go // preempt tries to create room for a pod that has failed to schedule, by preempting lower priority pods if possible. // If it succeeds, it adds the name of the node where preemption has happened to the pod annotations. // It returns the node name and an error if any. func (sched *Scheduler) preempt(preemptor *v1.Pod, scheduleErr error) (string, error) { if !util.PodPriorityEnabled() || sched.config.DisablePreemption { glog.V(3).Infof(\"Pod priority feature is not enabled or preemption is disabled by scheduler configuration.\" + \" No preemption is performed.\") return \"\", nil } preemptor, err := sched.config.PodPreemptor.GetUpdatedPod(preemptor) if err != nil { glog.Errorf(\"Error getting the updated preemptor pod object: %v\", err) return \"\", err } node, victims, nominatedPodsToClear, err := sched.config.Algorithm.Preempt(preemptor, sched.config.NodeLister, scheduleErr) metrics.PreemptionVictims.Set(float64(len(victims))) if err != nil { glog.Errorf(\"Error preempting victims to make room for %v/%v.\", preemptor.Namespace, preemptor.Name) return \"\", err } var nodeName = \"\" if node != nil { nodeName = node.Name err = sched.config.PodPreemptor.SetNominatedNodeName(preemptor, nodeName) if err != nil { glog.Errorf(\"Error in preemption process. Cannot update pod %v/%v annotations: %v\", preemptor.Namespace, preemptor.Name, err) return \"\", err } for _, victim := range victims { if err := sched.config.PodPreemptor.DeletePod(victim); err != nil { glog.Errorf(\"Error preempting pod %v/%v: %v\", victim.Namespace, victim.Name, err) return \"\", err } sched.config.Recorder.Eventf(victim, v1.EventTypeNormal, \"Preempted\", \"by %v/%v on node %v\", preemptor.Namespace, preemptor.Name, nodeName) } } // Clearing nominated pods should happen outside of \"if node != nil\". Node could // be nil when a pod with nominated node name is eligible to preempt again, // but preemption logic does not find any node for it. In that case Preempt() // function of generic_scheduler.go returns the pod itself for removal of the annotation. for _, p := range nominatedPodsToClear { rErr := sched.config.PodPreemptor.RemoveNominatedNodeName(p) if rErr != nil { glog.Errorf(\"Cannot remove nominated node annotation of pod: %v\", rErr) // We do not return as this error is not critical. } } return nodeName, err } 以下对preempt的实现分段分析。 如果设置关闭抢占机制，则直接返回。 if !util.PodPriorityEnabled() || sched.config.DisablePreemption { glog.V(3).Infof(\"Pod priority feature is not enabled or preemption is disabled by scheduler configuration.\" + \" No preemption is performed.\") return \"\", nil } 获取当前pod的最新状态。 preemptor, err := sched.config.PodPreemptor.GetUpdatedPod(preemptor) if err != nil { glog.Errorf(\"Error getting the updated preemptor pod object: %v\", err) return \"\", err } GetUpdatedPod的实现就是去拿pod的对象。 func (p *podPreemptor) GetUpdatedPod(pod *v1.Pod) (*v1.Pod, error) { return p.Client.CoreV1().Pods(pod.Namespace).Get(pod.Name, metav1.GetOptions{}) } 接着执行抢占的算法。抢占的算法返回预调度节点的信息和因抢占被剔除的pod的信息。具体的抢占算法逻辑下文分析。 node, victims, nominatedPodsToClear, err := sched.config.Algorithm.Preempt(preemptor, sched.config.NodeLister, scheduleErr) 将预调度节点的信息更新到pod的Status.NominatedNodeName属性中。 err = sched.config.PodPreemptor.SetNominatedNodeName(preemptor, nodeName) SetNominatedNodeName的具体实现为： func (p *podPreemptor) SetNominatedNodeName(pod *v1.Pod, nominatedNodeName string) error { podCopy := pod.DeepCopy() podCopy.Status.NominatedNodeName = nominatedNodeName _, err := p.Client.CoreV1().Pods(pod.Namespace).UpdateStatus(podCopy) return err } 接着删除因抢占而需要被剔除的pod。 err := sched.config.PodPreemptor.DeletePod(victim) PodPreemptor.DeletePod的具体实现就是删除具体的pod。 func (p *podPreemptor) DeletePod(pod *v1.Pod) error { return p.Client.CoreV1().Pods(pod.Namespace).Delete(pod.Name, &metav1.DeleteOptions{}) } 如果抢占算法得出的node对象为nil，则将pod的Status.NominatedNodeName属性设置为空。 // Clearing nominated pods should happen outside of \"if node != nil\". Node could // be nil when a pod with nominated node name is eligible to preempt again, // but preemption logic does not find any node for it. In that case Preempt() // function of generic_scheduler.go returns the pod itself for removal of the annotation. for _, p := range nominatedPodsToClear { rErr := sched.config.PodPreemptor.RemoveNominatedNodeName(p) if rErr != nil { glog.Errorf(\"Cannot remove nominated node annotation of pod: %v\", rErr) // We do not return as this error is not critical. } } RemoveNominatedNodeName的具体实现如下： func (p *podPreemptor) RemoveNominatedNodeName(pod *v1.Pod) error { if len(pod.Status.NominatedNodeName) == 0 { return nil } return p.SetNominatedNodeName(pod, \"\") } 2.2. NominatedNodeName Pod.Status.NominatedNodeName的说明： nominatedNodeName是调度失败的pod抢占别的pod的时候，被抢占pod的运行节点。但在剔除被抢占pod之前该调度失败的pod不会被调度。同时也不保证最终该pod一定会调度到nominatedNodeName的机器上，也可能因为之后资源充足等原因调度到其他节点上。最终该pod会被加到调度的队列中。 其中加入到调度队列的具体过程如下： func NewConfigFactory(args *ConfigFactoryArgs) scheduler.Configurator { ... // unscheduled pod queue args.PodInformer.Informer().AddEventHandler( ... Handler: cache.ResourceEventHandlerFuncs{ AddFunc: c.addPodToSchedulingQueue, UpdateFunc: c.updatePodInSchedulingQueue, DeleteFunc: c.deletePodFromSchedulingQueue, }, }, ) ... } addPodToSchedulingQueue: func (c *configFactory) addPodToSchedulingQueue(obj interface{}) { if err := c.podQueue.Add(obj.(*v1.Pod)); err != nil { runtime.HandleError(fmt.Errorf(\"unable to queue %T: %v\", obj, err)) } } PriorityQueue.Add: // Add adds a pod to the active queue. It should be called only when a new pod // is added so there is no chance the pod is already in either queue. func (p *PriorityQueue) Add(pod *v1.Pod) error { p.lock.Lock() defer p.lock.Unlock() err := p.activeQ.Add(pod) if err != nil { glog.Errorf(\"Error adding pod %v/%v to the scheduling queue: %v\", pod.Namespace, pod.Name, err) } else { if p.unschedulableQ.get(pod) != nil { glog.Errorf(\"Error: pod %v/%v is already in the unschedulable queue.\", pod.Namespace, pod.Name) p.deleteNominatedPodIfExists(pod) p.unschedulableQ.delete(pod) } p.addNominatedPodIfNeeded(pod) p.cond.Broadcast() } return err } addNominatedPodIfNeeded: // addNominatedPodIfNeeded adds a pod to nominatedPods if it has a NominatedNodeName and it does not // already exist in the map. Adding an existing pod is not going to update the pod. func (p *PriorityQueue) addNominatedPodIfNeeded(pod *v1.Pod) { nnn := NominatedNodeName(pod) if len(nnn) > 0 { for _, np := range p.nominatedPods[nnn] { if np.UID == pod.UID { glog.Errorf(\"Pod %v/%v already exists in the nominated map!\", pod.Namespace, pod.Name) return } } p.nominatedPods[nnn] = append(p.nominatedPods[nnn], pod) } } NominatedNodeName: // NominatedNodeName returns nominated node name of a Pod. func NominatedNodeName(pod *v1.Pod) string { return pod.Status.NominatedNodeName } 3. genericScheduler.Preempt 抢占算法依然是在ScheduleAlgorithm接口中定义。 // ScheduleAlgorithm is an interface implemented by things that know how to schedule pods // onto machines. type ScheduleAlgorithm interface { Schedule(*v1.Pod, NodeLister) (selectedMachine string, err error) // Preempt receives scheduling errors for a pod and tries to create room for // the pod by preempting lower priority pods if possible. // It returns the node where preemption happened, a list of preempted pods, a // list of pods whose nominated node name should be removed, and error if any. Preempt(*v1.Pod, NodeLister, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error) // Predicates() returns a pointer to a map of predicate functions. This is // exposed for testing. Predicates() map[string]FitPredicate // Prioritizers returns a slice of priority config. This is exposed for // testing. Prioritizers() []PriorityConfig } Preempt的具体实现为genericScheduler结构体。 Preempt的主要实现是找到可以调度的节点和上面因抢占而需要被剔除的pod。 基本流程如下： 根据调度失败的原因对所有节点先进行一批筛选，筛选出潜在的被调度节点列表。 通过selectNodesForPreemption筛选出需要牺牲的pod和其节点。 基于拓展抢占逻辑再次对上述筛选出来的牺牲者做过滤。 基于上述的过滤结果，选择一个最终可能因抢占被调度的节点。 基于上述的候选节点，找出该节点上优先级低于当前被调度pod的牺牲者pod列表。 完整代码如下： 此部分代码位于pkg/scheduler/core/generic_scheduler.go // preempt finds nodes with pods that can be preempted to make room for \"pod\" to // schedule. It chooses one of the nodes and preempts the pods on the node and // returns 1) the node, 2) the list of preempted pods if such a node is found, // 3) A list of pods whose nominated node name should be cleared, and 4) any // possible error. func (g *genericScheduler) Preempt(pod *v1.Pod, nodeLister algorithm.NodeLister, scheduleErr error) (*v1.Node, []*v1.Pod, []*v1.Pod, error) { // Scheduler may return various types of errors. Consider preemption only if // the error is of type FitError. fitError, ok := scheduleErr.(*FitError) if !ok || fitError == nil { return nil, nil, nil, nil } err := g.cache.UpdateNodeNameToInfoMap(g.cachedNodeInfoMap) if err != nil { return nil, nil, nil, err } if !podEligibleToPreemptOthers(pod, g.cachedNodeInfoMap) { glog.V(5).Infof(\"Pod %v/%v is not eligible for more preemption.\", pod.Namespace, pod.Name) return nil, nil, nil, nil } allNodes, err := nodeLister.List() if err != nil { return nil, nil, nil, err } if len(allNodes) == 0 { return nil, nil, nil, ErrNoNodesAvailable } potentialNodes := nodesWherePreemptionMightHelp(allNodes, fitError.FailedPredicates) if len(potentialNodes) == 0 { glog.V(3).Infof(\"Preemption will not help schedule pod %v/%v on any node.\", pod.Namespace, pod.Name) // In this case, we should clean-up any existing nominated node name of the pod. return nil, nil, []*v1.Pod{pod}, nil } pdbs, err := g.cache.ListPDBs(labels.Everything()) if err != nil { return nil, nil, nil, err } // 找出可能被抢占的节点 nodeToVictims, err := selectNodesForPreemption(pod, g.cachedNodeInfoMap, potentialNodes, g.predicates, g.predicateMetaProducer, g.schedulingQueue, pdbs) if err != nil { return nil, nil, nil, err } // We will only check nodeToVictims with extenders that support preemption. // Extenders which do not support preemption may later prevent preemptor from being scheduled on the nominated // node. In that case, scheduler will find a different host for the preemptor in subsequent scheduling cycles. nodeToVictims, err = g.processPreemptionWithExtenders(pod, nodeToVictims) if err != nil { return nil, nil, nil, err } // 选出最终被抢占的节点 candidateNode := pickOneNodeForPreemption(nodeToVictims) if candidateNode == nil { return nil, nil, nil, err } // Lower priority pods nominated to run on this node, may no longer fit on // this node. So, we should remove their nomination. Removing their // nomination updates these pods and moves them to the active queue. It // lets scheduler find another place for them. // 找出被强占节点上牺牲者pod列表 nominatedPods := g.getLowerPriorityNominatedPods(pod, candidateNode.Name) if nodeInfo, ok := g.cachedNodeInfoMap[candidateNode.Name]; ok { return nodeInfo.Node(), nodeToVictims[candidateNode].Pods, nominatedPods, err } return nil, nil, nil, fmt.Errorf( \"preemption failed: the target node %s has been deleted from scheduler cache\", candidateNode.Name) } 以下对genericScheduler.Preempt分段进行分析。 3.1. selectNodesForPreemption selectNodesForPreemption并行地所有节点中找可能被抢占的节点。 nodeToVictims, err := selectNodesForPreemption(pod, g.cachedNodeInfoMap, potentialNodes, g.predicates,g.predicateMetaProducer, g.schedulingQueue, pdbs) selectNodesForPreemption主要基于selectVictimsOnNode构造一个checkNode的函数，然后并发执行该函数。 selectNodesForPreemption具体实现如下： // selectNodesForPreemption finds all the nodes with possible victims for // preemption in parallel. func selectNodesForPreemption(pod *v1.Pod, nodeNameToInfo map[string]*schedulercache.NodeInfo, potentialNodes []*v1.Node, predicates map[string]algorithm.FitPredicate, metadataProducer algorithm.PredicateMetadataProducer, queue SchedulingQueue, pdbs []*policy.PodDisruptionBudget, ) (map[*v1.Node]*schedulerapi.Victims, error) { nodeToVictims := map[*v1.Node]*schedulerapi.Victims{} var resultLock sync.Mutex // We can use the same metadata producer for all nodes. meta := metadataProducer(pod, nodeNameToInfo) checkNode := func(i int) { nodeName := potentialNodes[i].Name var metaCopy algorithm.PredicateMetadata if meta != nil { metaCopy = meta.ShallowCopy() } pods, numPDBViolations, fits := selectVictimsOnNode(pod, metaCopy, nodeNameToInfo[nodeName], predicates, queue, pdbs) if fits { resultLock.Lock() victims := schedulerapi.Victims{ Pods: pods, NumPDBViolations: numPDBViolations, } nodeToVictims[potentialNodes[i]] = &victims resultLock.Unlock() } } workqueue.Parallelize(16, len(potentialNodes), checkNode) return nodeToVictims, nil } 3.1.1. selectVictimsOnNode selectVictimsOnNode找到应该被抢占的给定节点上的最小pod集合，以便给调度失败的pod安排足够的空间。该函数最终返回的是一个pod的数组。当有更低优先级的pod可能被选择的时候，较高优先级的pod不会被选入该待剔除的pod集合。 基本流程如下： 先检查当该节点上所有低于预被调度pod优先级的pod移除后，该pod能否被调度到当前节点上。 如果上述检查可以，则将该节点的所有低优先级pod按照优先级来排序。 // selectVictimsOnNode finds minimum set of pods on the given node that should // be preempted in order to make enough room for \"pod\" to be scheduled. The // minimum set selected is subject to the constraint that a higher-priority pod // is never preempted when a lower-priority pod could be (higher/lower relative // to one another, not relative to the preemptor \"pod\"). // The algorithm first checks if the pod can be scheduled on the node when all the // lower priority pods are gone. If so, it sorts all the lower priority pods by // their priority and then puts them into two groups of those whose PodDisruptionBudget // will be violated if preempted and other non-violating pods. Both groups are // sorted by priority. It first tries to reprieve as many PDB violating pods as // possible and then does them same for non-PDB-violating pods while checking // that the \"pod\" can still fit on the node. // NOTE: This function assumes that it is never called if \"pod\" cannot be scheduled // due to pod affinity, node affinity, or node anti-affinity reasons. None of // these predicates can be satisfied by removing more pods from the node. func selectVictimsOnNode( pod *v1.Pod, meta algorithm.PredicateMetadata, nodeInfo *schedulercache.NodeInfo, fitPredicates map[string]algorithm.FitPredicate, queue SchedulingQueue, pdbs []*policy.PodDisruptionBudget, ) ([]*v1.Pod, int, bool) { potentialVictims := util.SortableList{CompFunc: util.HigherPriorityPod} nodeInfoCopy := nodeInfo.Clone() removePod := func(rp *v1.Pod) { nodeInfoCopy.RemovePod(rp) if meta != nil { meta.RemovePod(rp) } } addPod := func(ap *v1.Pod) { nodeInfoCopy.AddPod(ap) if meta != nil { meta.AddPod(ap, nodeInfoCopy) } } // As the first step, remove all the lower priority pods from the node and // check if the given pod can be scheduled. podPriority := util.GetPodPriority(pod) for _, p := range nodeInfoCopy.Pods() { if util.GetPodPriority(p) 3.2. processPreemptionWithExtenders processPreemptionWithExtenders基于selectNodesForPreemption选出的牺牲者进行扩展的抢占逻辑继续筛选牺牲者。 // We will only check nodeToVictims with extenders that support preemption. // Extenders which do not support preemption may later prevent preemptor from being scheduled on the nominated // node. In that case, scheduler will find a different host for the preemptor in subsequent scheduling cycles. nodeToVictims, err = g.processPreemptionWithExtenders(pod, nodeToVictims) if err != nil { return nil, nil, nil, err } processPreemptionWithExtenders完整代码如下： // processPreemptionWithExtenders processes preemption with extenders func (g *genericScheduler) processPreemptionWithExtenders( pod *v1.Pod, nodeToVictims map[*v1.Node]*schedulerapi.Victims, ) (map[*v1.Node]*schedulerapi.Victims, error) { if len(nodeToVictims) > 0 { for _, extender := range g.extenders { if extender.SupportsPreemption() && extender.IsInterested(pod) { newNodeToVictims, err := extender.ProcessPreemption( pod, nodeToVictims, g.cachedNodeInfoMap, ) if err != nil { if extender.IsIgnorable() { glog.Warningf(\"Skipping extender %v as it returned error %v and has ignorable flag set\", extender, err) continue } return nil, err } // Replace nodeToVictims with new result after preemption. So the // rest of extenders can continue use it as parameter. nodeToVictims = newNodeToVictims // If node list becomes empty, no preemption can happen regardless of other extenders. if len(nodeToVictims) == 0 { break } } } } return nodeToVictims, nil } 3.3. pickOneNodeForPreemption pickOneNodeForPreemption从筛选出的node中再挑选一个节点作为最终调度节点。 candidateNode := pickOneNodeForPreemption(nodeToVictims) if candidateNode == nil { return nil, nil, nil, err } pickOneNodeForPreemption完整代码如下： // pickOneNodeForPreemption chooses one node among the given nodes. It assumes // pods in each map entry are ordered by decreasing priority. // It picks a node based on the following criteria: // 1. A node with minimum number of PDB violations. // 2. A node with minimum highest priority victim is picked. // 3. Ties are broken by sum of priorities of all victims. // 4. If there are still ties, node with the minimum number of victims is picked. // 5. If there are still ties, the first such node is picked (sort of randomly). // The 'minNodes1' and 'minNodes2' are being reused here to save the memory // allocation and garbage collection time. func pickOneNodeForPreemption(nodesToVictims map[*v1.Node]*schedulerapi.Victims) *v1.Node { if len(nodesToVictims) == 0 { return nil } minNumPDBViolatingPods := math.MaxInt32 var minNodes1 []*v1.Node lenNodes1 := 0 for node, victims := range nodesToVictims { if len(victims.Pods) == 0 { // We found a node that doesn't need any preemption. Return it! // This should happen rarely when one or more pods are terminated between // the time that scheduler tries to schedule the pod and the time that // preemption logic tries to find nodes for preemption. return node } numPDBViolatingPods := victims.NumPDBViolations if numPDBViolatingPods = 0. This is // needed so that a node with a few pods with negative priority is not // picked over a node with a smaller number of pods with the same negative // priority (and similar scenarios). sumPriorities += int64(util.GetPodPriority(pod)) + int64(math.MaxInt32+1) } if sumPriorities 0 { return minNodes2[0] } glog.Errorf(\"Error in logic of node scoring for preemption. We should never reach here!\") return nil } 3.4. getLowerPriorityNominatedPods getLowerPriorityNominatedPods的基本流程如下： 获取候选节点上的pod列表。 获取待调度pod的优先级值。 遍历该节点的pod列表，如果低于待调度pod的优先级则放入低优先级pod列表中。 genericScheduler.Preempt中相关代码如下： // Lower priority pods nominated to run on this node, may no longer fit on // this node. So, we should remove their nomination. Removing their // nomination updates these pods and moves them to the active queue. It // lets scheduler find another place for them. nominatedPods := g.getLowerPriorityNominatedPods(pod, candidateNode.Name) if nodeInfo, ok := g.cachedNodeInfoMap[candidateNode.Name]; ok { return nodeInfo.Node(), nodeToVictims[candidateNode].Pods, nominatedPods, err } getLowerPriorityNominatedPods代码如下： 此部分代码位于pkg/scheduler/core/generic_scheduler.go // getLowerPriorityNominatedPods returns pods whose priority is smaller than the // priority of the given \"pod\" and are nominated to run on the given node. // Note: We could possibly check if the nominated lower priority pods still fit // and return those that no longer fit, but that would require lots of // manipulation of NodeInfo and PredicateMeta per nominated pod. It may not be // worth the complexity, especially because we generally expect to have a very // small number of nominated pods per node. func (g *genericScheduler) getLowerPriorityNominatedPods(pod *v1.Pod, nodeName string) []*v1.Pod { pods := g.schedulingQueue.WaitingPodsForNode(nodeName) if len(pods) == 0 { return nil } var lowerPriorityPods []*v1.Pod podPriority := util.GetPodPriority(pod) for _, p := range pods { if util.GetPodPriority(p) 4. 总结 4.1. Scheduler.preempt 当pod调度失败的时候，会抢占低优先级pod的空间来给高优先级的pod。其中入参为调度失败的pod对象和调度失败的err。 抢占的基本流程如下： 判断是否有关闭抢占机制，如果关闭抢占机制则直接返回。 获取调度失败pod的最新对象数据。 执行抢占算法Algorithm.Preempt，返回预调度节点和需要被剔除的pod列表。 将抢占算法返回的node添加到pod的Status.NominatedNodeName中，并删除需要被剔除的pod。 当抢占算法返回的node是nil的时候，清除pod的Status.NominatedNodeName信息。 整个抢占流程的最终结果实际上是更新Pod.Status.NominatedNodeName属性的信息。如果抢占算法返回的节点不为空，则将该node更新到Pod.Status.NominatedNodeName中，否则就将Pod.Status.NominatedNodeName设置为空。 4.2. genericScheduler.Preempt Preempt的主要实现是找到可以调度的节点和上面因抢占而需要被剔除的pod。 基本流程如下： 根据调度失败的原因对所有节点先进行一批筛选，筛选出潜在的被调度节点列表。 通过selectNodesForPreemption筛选出需要牺牲的pod和其节点。 基于拓展抢占逻辑再次对上述筛选出来的牺牲者做过滤。 基于上述的过滤结果，选择一个最终可能因抢占被调度的节点。 基于上述的候选节点，找出该节点上优先级低于当前被调度pod的牺牲者pod列表。 参考： https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/scheduler.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/scheduler/core/generic_scheduler.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kubelet/kubelet-xmind.html":{"url":"kubelet/kubelet-xmind.html","title":"源码思维导图","keywords":"","body":"kubelet 源码思维导图 源码整体结构图 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2021-04-18 13:11:54 "},"kubelet/NewKubeletCommand.html":{"url":"kubelet/NewKubeletCommand.html","title":"NewKubeletCommand","keywords":"","body":"kubelet源码分析（一）之 NewKubeletCommand 以下代码分析基于 kubernetes v1.12.0 版本。 本文主要分析 https://github.com/kubernetes/kubernetes/tree/v1.12.0/cmd/kubelet 部分的代码。 本文主要分析 kubernetes/cmd/kubelet部分，该部分主要涉及kubelet的参数解析，及初始化和构造相关的依赖组件（主要在kubeDeps结构体中），并没有kubelet运行的详细逻辑，该部分位于kubernetes/pkg/kubelet模块，待后续文章分析。 kubelet的cmd代码目录结构如下： kubelet ├── app │ ├── auth.go │ ├── init_others.go │ ├── init_windows.go │ ├── options # 包括kubelet使用到的option │ │ ├── container_runtime.go │ │ ├── globalflags.go │ │ ├── globalflags_linux.go │ │ ├── globalflags_other.go │ │ ├── options.go # 包括KubeletFlags、AddFlags、AddKubeletConfigFlags等 │ │ ├── osflags_others.go │ │ └── osflags_windows.go │ ├── plugins.go │ ├── server.go # 包括NewKubeletCommand、Run、RunKubelet、CreateAndInitKubelet、startKubelet等 │ ├── server_linux.go │ └── server_unsupported.go └── kubelet.go # kubelet的main入口函数 1. Main 函数 kubelet的入口函数Main 函数，具体代码参考：https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kubelet/kubelet.go。 func main() { rand.Seed(time.Now().UTC().UnixNano()) command := app.NewKubeletCommand(server.SetupSignalHandler()) logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } } kubelet代码主要采用了Cobra命令行框架，核心代码如下： // 初始化命令行 command := app.NewKubeletCommand(server.SetupSignalHandler()) // 执行Execute err := command.Execute() 2. NewKubeletCommand NewKubeletCommand基于参数创建了一个*cobra.Command对象。其中核心部分代码为参数解析部分和Run函数。 // NewKubeletCommand creates a *cobra.Command object with default parameters func NewKubeletCommand(stopCh 2.1. 参数解析 kubelet开启了DisableFlagParsing参数，没有使用Cobra框架中的默认参数解析，而是自定义参数解析。 2.1.1. 初始化参数和配置 初始化参数解析，初始化cleanFlagSet，kubeletFlags，kubeletConfig。 cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError) cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc) kubeletFlags := options.NewKubeletFlags() kubeletConfig, err := options.NewKubeletConfiguration() 2.1.2. 打印帮助信息和版本信息 如果输入非法参数则打印使用帮助信息。 // initial flag parse, since we disable cobra's flag parsing if err := cleanFlagSet.Parse(args); err != nil { cmd.Usage() glog.Fatal(err) } // check if there are non-flag arguments in the command line cmds := cleanFlagSet.Args() if len(cmds) > 0 { cmd.Usage() glog.Fatalf(\"unknown command: %s\", cmds[0]) } 遇到help和version参数则打印相关内容并退出。 // short-circuit on help help, err := cleanFlagSet.GetBool(\"help\") if err != nil { glog.Fatal(`\"help\" flag is non-bool, programmer error, please correct`) } if help { cmd.Help() return } // short-circuit on verflag verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cleanFlagSet) 2.1.3. kubelet config 加载并校验kubelet config。其中包括校验初始化的kubeletFlags，并从kubeletFlags的KubeletConfigFile参数获取kubelet config的内容。 // set feature gates from initial flags-based config if err := utilfeature.DefaultFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { glog.Fatal(err) } // validate the initial KubeletFlags if err := options.ValidateKubeletFlags(kubeletFlags); err != nil { glog.Fatal(err) } if kubeletFlags.ContainerRuntime == \"remote\" && cleanFlagSet.Changed(\"pod-infra-container-image\") { glog.Warning(\"Warning: For remote container runtime, --pod-infra-container-image is ignored in kubelet, which should be set in that remote runtime instead\") } // load kubelet config file, if provided if configFile := kubeletFlags.KubeletConfigFile; len(configFile) > 0 { kubeletConfig, err = loadConfigFile(configFile) if err != nil { glog.Fatal(err) } // We must enforce flag precedence by re-parsing the command line into the new object. // This is necessary to preserve backwards-compatibility across binary upgrades. // See issue #56171 for more details. if err := kubeletConfigFlagPrecedence(kubeletConfig, args); err != nil { glog.Fatal(err) } // update feature gates based on new config if err := utilfeature.DefaultFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { glog.Fatal(err) } } // We always validate the local configuration (command line + config file). // This is the default \"last-known-good\" config for dynamic config, and must always remain valid. if err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil { glog.Fatal(err) } 2.1.4. dynamic kubelet config 如果开启使用动态kubelet的配置，则由动态配置文件替换kubelet配置文件。 // use dynamic kubelet config, if enabled var kubeletConfigController *dynamickubeletconfig.Controller if dynamicConfigDir := kubeletFlags.DynamicConfigDir.Value(); len(dynamicConfigDir) > 0 { var dynamicKubeletConfig *kubeletconfiginternal.KubeletConfiguration dynamicKubeletConfig, kubeletConfigController, err = BootstrapKubeletConfigController(dynamicConfigDir, func(kc *kubeletconfiginternal.KubeletConfiguration) error { // Here, we enforce flag precedence inside the controller, prior to the controller's validation sequence, // so that we get a complete validation at the same point where we can decide to reject dynamic config. // This fixes the flag-precedence component of issue #63305. // See issue #56171 for general details on flag precedence. return kubeletConfigFlagPrecedence(kc, args) }) if err != nil { glog.Fatal(err) } // If we should just use our existing, local config, the controller will return a nil config if dynamicKubeletConfig != nil { kubeletConfig = dynamicKubeletConfig // Note: flag precedence was already enforced in the controller, prior to validation, // by our above transform function. Now we simply update feature gates from the new config. if err := utilfeature.DefaultFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil { glog.Fatal(err) } } } 总结：以上通过对各种特定参数的解析，最终生成kubeletFlags和kubeletConfig两个重要的参数对象，用来构造kubeletServer和其他需求。 2.2. 初始化kubeletServer和kubeletDeps 2.2.1. kubeletServer // construct a KubeletServer from kubeletFlags and kubeletConfig kubeletServer := &options.KubeletServer{ KubeletFlags: *kubeletFlags, KubeletConfiguration: *kubeletConfig, } 2.2.2. kubeletDeps // use kubeletServer to construct the default KubeletDeps kubeletDeps, err := UnsecuredDependencies(kubeletServer) if err != nil { glog.Fatal(err) } // add the kubelet config controller to kubeletDeps kubeletDeps.KubeletConfigController = kubeletConfigController 2.2.3. docker shim 如果开启了docker shim参数，则执行RunDockershim。 // start the experimental docker shim, if enabled if kubeletServer.KubeletFlags.ExperimentalDockershim { if err := RunDockershim(&kubeletServer.KubeletFlags, kubeletConfig, stopCh); err != nil { glog.Fatal(err) } return } 2.3. AddFlags // keep cleanFlagSet separate, so Cobra doesn't pollute it with the global flags kubeletFlags.AddFlags(cleanFlagSet) options.AddKubeletConfigFlags(cleanFlagSet, kubeletConfig) options.AddGlobalFlags(cleanFlagSet) cleanFlagSet.BoolP(\"help\", \"h\", false, fmt.Sprintf(\"help for %s\", cmd.Name())) // ugly, but necessary, because Cobra's default UsageFunc and HelpFunc pollute the flagset with global flags const usageFmt = \"Usage:\\n %s\\n\\nFlags:\\n%s\" cmd.SetUsageFunc(func(cmd *cobra.Command) error { fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) return nil }) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) { fmt.Fprintf(cmd.OutOrStdout(), \"%s\\n\\n\"+usageFmt, cmd.Long, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) }) 其中： AddFlags代码可参考：kubernetes/cmd/kubelet/app/options/options.go#L323 AddKubeletConfigFlags代码可参考：kubernetes/cmd/kubelet/app/options/options.go#L424 2.4. 运行kubelet 运行kubelet并且不退出。由Run函数进入后续的操作。 // run the kubelet glog.V(5).Infof(\"KubeletConfiguration: %#v\", kubeletServer.KubeletConfiguration) if err := Run(kubeletServer, kubeletDeps, stopCh); err != nil { glog.Fatal(err) } 3. Run // Run runs the specified KubeletServer with the given Dependencies. This should never exit. // The kubeDeps argument may be nil - if so, it is initialized from the settings on KubeletServer. // Otherwise, the caller is assumed to have set up the Dependencies object and a default one will // not be generated. func Run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh 当运行环境是Windows的时候，初始化操作，但是该操作为空，只是预留。具体执行run(s, kubeDeps, stopCh)函数。 3.1. 构造kubeDeps 3.1.1. clientConfig 创建clientConfig，该对象用来创建各种的kubeDeps属性中包含的client。 clientConfig, err := createAPIServerClientConfig(s) if err != nil { return fmt.Errorf(\"invalid kubeconfig: %v\", err) } 3.1.2. kubeClient kubeClient, err = clientset.NewForConfig(clientConfig) if err != nil { glog.Warningf(\"New kubeClient from clientConfig error: %v\", err) } else if kubeClient.CertificatesV1beta1() != nil && clientCertificateManager != nil { glog.V(2).Info(\"Starting client certificate rotation.\") clientCertificateManager.SetCertificateSigningRequestClient(kubeClient.CertificatesV1beta1().CertificateSigningRequests()) clientCertificateManager.Start() } 3.1.3. dynamicKubeClient dynamicKubeClient, err = dynamic.NewForConfig(clientConfig) if err != nil { glog.Warningf(\"Failed to initialize dynamic KubeClient: %v\", err) } 3.1.4. eventClient // make a separate client for events eventClientConfig := *clientConfig eventClientConfig.QPS = float32(s.EventRecordQPS) eventClientConfig.Burst = int(s.EventBurst) eventClient, err = v1core.NewForConfig(&eventClientConfig) if err != nil { glog.Warningf(\"Failed to create API Server client for Events: %v\", err) } 3.1.5. heartbeatClient // make a separate client for heartbeat with throttling disabled and a timeout attached heartbeatClientConfig := *clientConfig heartbeatClientConfig.Timeout = s.KubeletConfiguration.NodeStatusUpdateFrequency.Duration // if the NodeLease feature is enabled, the timeout is the minimum of the lease duration and status update frequency if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) { leaseTimeout := time.Duration(s.KubeletConfiguration.NodeLeaseDurationSeconds) * time.Second if heartbeatClientConfig.Timeout > leaseTimeout { heartbeatClientConfig.Timeout = leaseTimeout } } heartbeatClientConfig.QPS = float32(-1) heartbeatClient, err = clientset.NewForConfig(&heartbeatClientConfig) if err != nil { glog.Warningf(\"Failed to create API Server client for heartbeat: %v\", err) } 3.1.6. csiClient // csiClient works with CRDs that support json only clientConfig.ContentType = \"application/json\" csiClient, err := csiclientset.NewForConfig(clientConfig) if err != nil { glog.Warningf(\"Failed to create CSI API client: %v\", err) } client赋值 kubeDeps.KubeClient = kubeClient kubeDeps.DynamicKubeClient = dynamicKubeClient if heartbeatClient != nil { kubeDeps.HeartbeatClient = heartbeatClient kubeDeps.OnHeartbeatFailure = closeAllConns } if eventClient != nil { kubeDeps.EventClient = eventClient } kubeDeps.CSIClient = csiClient 3.1.7. CAdvisorInterface if kubeDeps.CAdvisorInterface == nil { imageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint) kubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint)) if err != nil { return err } } 3.1.8. ContainerManager if kubeDeps.ContainerManager == nil { if s.CgroupsPerQOS && s.CgroupRoot == \"\" { glog.Infof(\"--cgroups-per-qos enabled, but --cgroup-root was not specified. defaulting to /\") s.CgroupRoot = \"/\" } kubeReserved, err := parseResourceList(s.KubeReserved) if err != nil { return err } systemReserved, err := parseResourceList(s.SystemReserved) if err != nil { return err } var hardEvictionThresholds []evictionapi.Threshold // If the user requested to ignore eviction thresholds, then do not set valid values for hardEvictionThresholds here. if !s.ExperimentalNodeAllocatableIgnoreEvictionThreshold { hardEvictionThresholds, err = eviction.ParseThresholdConfig([]string{}, s.EvictionHard, nil, nil, nil) if err != nil { return err } } experimentalQOSReserved, err := cm.ParseQOSReserved(s.QOSReserved) if err != nil { return err } devicePluginEnabled := utilfeature.DefaultFeatureGate.Enabled(features.DevicePlugins) kubeDeps.ContainerManager, err = cm.NewContainerManager( kubeDeps.Mounter, kubeDeps.CAdvisorInterface, cm.NodeConfig{ RuntimeCgroupsName: s.RuntimeCgroups, SystemCgroupsName: s.SystemCgroups, KubeletCgroupsName: s.KubeletCgroups, ContainerRuntime: s.ContainerRuntime, CgroupsPerQOS: s.CgroupsPerQOS, CgroupRoot: s.CgroupRoot, CgroupDriver: s.CgroupDriver, KubeletRootDir: s.RootDirectory, ProtectKernelDefaults: s.ProtectKernelDefaults, NodeAllocatableConfig: cm.NodeAllocatableConfig{ KubeReservedCgroupName: s.KubeReservedCgroup, SystemReservedCgroupName: s.SystemReservedCgroup, EnforceNodeAllocatable: sets.NewString(s.EnforceNodeAllocatable...), KubeReserved: kubeReserved, SystemReserved: systemReserved, HardEvictionThresholds: hardEvictionThresholds, }, QOSReserved: *experimentalQOSReserved, ExperimentalCPUManagerPolicy: s.CPUManagerPolicy, ExperimentalCPUManagerReconcilePeriod: s.CPUManagerReconcilePeriod.Duration, ExperimentalPodPidsLimit: s.PodPidsLimit, EnforceCPULimits: s.CPUCFSQuota, CPUCFSQuotaPeriod: s.CPUCFSQuotaPeriod.Duration, }, s.FailSwapOn, devicePluginEnabled, kubeDeps.Recorder) if err != nil { return err } } 3.1.9. oomAdjuster // TODO(vmarmol): Do this through container config. oomAdjuster := kubeDeps.OOMAdjuster if err := oomAdjuster.ApplyOOMScoreAdj(0, int(s.OOMScoreAdj)); err != nil { glog.Warning(err) } 3.2. Health check if s.HealthzPort > 0 { healthz.DefaultHealthz() go wait.Until(func() { err := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil) if err != nil { glog.Errorf(\"Starting health server failed: %v\", err) } }, 5*time.Second, wait.NeverStop) } 3.3. RunKubelet 通过各种赋值构造了完整的kubeDeps结构体，最后再执行RunKubelet转入后续的kubelet执行流程。 if err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil { return err } 4. RunKubelet // RunKubelet is responsible for setting up and running a kubelet. It is used in three different applications: // 1 Integration tests // 2 Kubelet binary // 3 Standalone 'kubernetes' binary // Eventually, #2 will be replaced with instances of #3 func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error { ... k, err := CreateAndInitKubelet(&kubeServer.KubeletConfiguration, ... kubeServer.NodeStatusMaxImages) if err != nil { return fmt.Errorf(\"failed to create kubelet: %v\", err) } // NewMainKubelet should have set up a pod source config if one didn't exist // when the builder was run. This is just a precaution. if kubeDeps.PodConfig == nil { return fmt.Errorf(\"failed to create kubelet, pod source config was nil\") } podCfg := kubeDeps.PodConfig rlimit.RlimitNumFiles(uint64(kubeServer.MaxOpenFiles)) // process pods and exit. if runOnce { if _, err := k.RunOnce(podCfg.Updates()); err != nil { return fmt.Errorf(\"runonce failed: %v\", err) } glog.Infof(\"Started kubelet as runonce\") } else { startKubelet(k, podCfg, &kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer) glog.Infof(\"Started kubelet\") } return nil } RunKubelet函数核心代码为执行了CreateAndInitKubelet和startKubelet两个函数的操作，以下对这两个函数进行分析。 4.1. CreateAndInitKubelet 通过传入kubeDeps调用CreateAndInitKubelet初始化Kubelet。 k, err := CreateAndInitKubelet(&kubeServer.KubeletConfiguration, kubeDeps, &kubeServer.ContainerRuntimeOptions, kubeServer.ContainerRuntime, kubeServer.RuntimeCgroups, kubeServer.HostnameOverride, kubeServer.NodeIP, kubeServer.ProviderID, kubeServer.CloudProvider, kubeServer.CertDirectory, kubeServer.RootDirectory, kubeServer.RegisterNode, kubeServer.RegisterWithTaints, kubeServer.AllowedUnsafeSysctls, kubeServer.RemoteRuntimeEndpoint, kubeServer.RemoteImageEndpoint, kubeServer.ExperimentalMounterPath, kubeServer.ExperimentalKernelMemcgNotification, kubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount, kubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold, kubeServer.MinimumGCAge, kubeServer.MaxPerPodContainerCount, kubeServer.MaxContainerCount, kubeServer.MasterServiceNamespace, kubeServer.RegisterSchedulable, kubeServer.NonMasqueradeCIDR, kubeServer.KeepTerminatedPodVolumes, kubeServer.NodeLabels, kubeServer.SeccompProfileRoot, kubeServer.BootstrapCheckpointPath, kubeServer.NodeStatusMaxImages) if err != nil { return fmt.Errorf(\"failed to create kubelet: %v\", err) } 4.1.1. NewMainKubelet CreateAndInitKubelet方法中执行的核心函数是NewMainKubelet，NewMainKubelet实例化一个kubelet对象，该部分的具体代码在kubernetes/pkg/kubelet中，具体参考：kubernetes/pkg/kubelet/kubelet.go#L325。 func CreateAndInitKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, ... nodeStatusMaxImages int32) (k kubelet.Bootstrap, err error) { // TODO: block until all sources have delivered at least one update to the channel, or break the sync loop // up into \"per source\" synchronizations k, err = kubelet.NewMainKubelet(kubeCfg, kubeDeps, crOptions, containerRuntime, runtimeCgroups, hostnameOverride, nodeIP, providerID, cloudProvider, certDirectory, rootDirectory, registerNode, registerWithTaints, allowedUnsafeSysctls, remoteRuntimeEndpoint, remoteImageEndpoint, experimentalMounterPath, experimentalKernelMemcgNotification, experimentalCheckNodeCapabilitiesBeforeMount, experimentalNodeAllocatableIgnoreEvictionThreshold, minimumGCAge, maxPerPodContainerCount, maxContainerCount, masterServiceNamespace, registerSchedulable, nonMasqueradeCIDR, keepTerminatedPodVolumes, nodeLabels, seccompProfileRoot, bootstrapCheckpointPath, nodeStatusMaxImages) if err != nil { return nil, err } k.BirthCry() k.StartGarbageCollection() return k, nil } 4.1.2. PodConfig if kubeDeps.PodConfig == nil { var err error kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath) if err != nil { return nil, err } } NewMainKubelet-->PodConfig-->NewPodConfig-->kubetypes.PodUpdate。会生成一个podUpdate的channel来监听pod的变化，该channel会在k.Run(podCfg.Updates())中作为关键入参。 4.2. startKubelet // process pods and exit. if runOnce { if _, err := k.RunOnce(podCfg.Updates()); err != nil { return fmt.Errorf(\"runonce failed: %v\", err) } glog.Infof(\"Started kubelet as runonce\") } else { startKubelet(k, podCfg, &kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer) glog.Infof(\"Started kubelet\") } 如果设置了只运行一次的参数，则执行k.RunOnce，否则执行核心函数startKubelet。具体实现如下： func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) { // start the kubelet go wait.Until(func() { k.Run(podCfg.Updates()) }, 0, wait.NeverStop) // start the kubelet server if enableServer { go k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling) } if kubeCfg.ReadOnlyPort > 0 { go k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort)) } } 4.2.1. k.Run // start the kubelet go wait.Until(func() { k.Run(podCfg.Updates()) }, 0, wait.NeverStop) 通过长驻进程的方式运行k.Run，不退出，将kubelet的运行逻辑引入kubernetes/pkg/kubelet/kubelet.go部分，kubernetes/pkg/kubelet部分的运行逻辑待后续文章分析。 5. 总结 kubelet采用Cobra命令行框架和pflag参数解析框架，和apiserver、scheduler、controller-manager形成统一的代码风格。 kubernetes/cmd/kubelet部分主要对运行参数进行定义和解析，初始化和构造相关的依赖组件（主要在kubeDeps结构体中），并没有kubelet运行的详细逻辑，该部分位于kubernetes/pkg/kubelet模块。 cmd部分调用流程如下：Main-->NewKubeletCommand-->Run(kubeletServer, kubeletDeps, stopCh)-->run(s *options.KubeletServer, kubeDeps ..., stopCh ...)--> RunKubelet(s, kubeDeps, s.RunOnce)-->startKubelet-->k.Run(podCfg.Updates())-->pkg/kubelet。 同时RunKubelet(s, kubeDeps, s.RunOnce)-->CreateAndInitKubelet-->kubelet.NewMainKubelet-->pkg/kubelet。 参考文章： https://github.com/kubernetes/kubernetes/tree/v1.12.0 Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kubelet/NewMainKubelet.html":{"url":"kubelet/NewMainKubelet.html","title":"NewMainKubelet","keywords":"","body":"kubelet源码分析（二）之 NewMainKubelet 以下代码分析基于 kubernetes v1.12.0 版本。 本文主要分析 https://github.com/kubernetes/kubernetes/tree/v1.12.0/pkg/kubelet 部分的代码。 本文主要分析kubelet中的NewMainKubelet部分。 1. NewMainKubelet NewMainKubelet主要用来初始化和构造一个kubelet结构体，kubelet结构体定义参考:https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kubelet.go#L888 // NewMainKubelet instantiates a new Kubelet object along with all the required internal modules. // No initialization of Kubelet and its modules should happen here. func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, crOptions *config.ContainerRuntimeOptions, containerRuntime string, runtimeCgroups string, hostnameOverride string, nodeIP string, providerID string, cloudProvider string, certDirectory string, rootDirectory string, registerNode bool, registerWithTaints []api.Taint, allowedUnsafeSysctls []string, remoteRuntimeEndpoint string, remoteImageEndpoint string, experimentalMounterPath string, experimentalKernelMemcgNotification bool, experimentalCheckNodeCapabilitiesBeforeMount bool, experimentalNodeAllocatableIgnoreEvictionThreshold bool, minimumGCAge metav1.Duration, maxPerPodContainerCount int32, maxContainerCount int32, masterServiceNamespace string, registerSchedulable bool, nonMasqueradeCIDR string, keepTerminatedPodVolumes bool, nodeLabels map[string]string, seccompProfileRoot string, bootstrapCheckpointPath string, nodeStatusMaxImages int32) (*Kubelet, error) { ... } 1.1. PodConfig 通过makePodSourceConfig生成Pod config。 if kubeDeps.PodConfig == nil { var err error kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath) if err != nil { return nil, err } } 1.1.1. makePodSourceConfig // makePodSourceConfig creates a config.PodConfig from the given // KubeletConfiguration or returns an error. func makePodSourceConfig(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, nodeName types.NodeName, bootstrapCheckpointPath string) (*config.PodConfig, error) { ... // source of all configuration cfg := config.NewPodConfig(config.PodConfigNotificationIncremental, kubeDeps.Recorder) // define file config source if kubeCfg.StaticPodPath != \"\" { glog.Infof(\"Adding pod path: %v\", kubeCfg.StaticPodPath) config.NewSourceFile(kubeCfg.StaticPodPath, nodeName, kubeCfg.FileCheckFrequency.Duration, cfg.Channel(kubetypes.FileSource)) } // define url config source if kubeCfg.StaticPodURL != \"\" { glog.Infof(\"Adding pod url %q with HTTP header %v\", kubeCfg.StaticPodURL, manifestURLHeader) config.NewSourceURL(kubeCfg.StaticPodURL, manifestURLHeader, nodeName, kubeCfg.HTTPCheckFrequency.Duration, cfg.Channel(kubetypes.HTTPSource)) } // Restore from the checkpoint path // NOTE: This MUST happen before creating the apiserver source // below, or the checkpoint would override the source of truth. ... if kubeDeps.KubeClient != nil { glog.Infof(\"Watching apiserver\") if updatechannel == nil { updatechannel = cfg.Channel(kubetypes.ApiserverSource) } config.NewSourceApiserver(kubeDeps.KubeClient, nodeName, updatechannel) } return cfg, nil } 1.1.2. NewPodConfig // NewPodConfig creates an object that can merge many configuration sources into a stream // of normalized updates to a pod configuration. func NewPodConfig(mode PodConfigNotificationMode, recorder record.EventRecorder) *PodConfig { updates := make(chan kubetypes.PodUpdate, 50) storage := newPodStorage(updates, mode, recorder) podConfig := &PodConfig{ pods: storage, mux: config.NewMux(storage), updates: updates, sources: sets.String{}, } return podConfig } 1.1.3. NewSourceApiserver // NewSourceApiserver creates a config source that watches and pulls from the apiserver. func NewSourceApiserver(c clientset.Interface, nodeName types.NodeName, updates chan 1.2. Lister serviceLister和nodeLister分别通过List-Watch机制监听service和node的列表变化。 1.2.1. serviceLister serviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}) if kubeDeps.KubeClient != nil { serviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"services\", metav1.NamespaceAll, fields.Everything()) r := cache.NewReflector(serviceLW, &v1.Service{}, serviceIndexer, 0) go r.Run(wait.NeverStop) } serviceLister := corelisters.NewServiceLister(serviceIndexer) 1.2.2. nodeLister nodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{}) if kubeDeps.KubeClient != nil { fieldSelector := fields.Set{api.ObjectNameField: string(nodeName)}.AsSelector() nodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), \"nodes\", metav1.NamespaceAll, fieldSelector) r := cache.NewReflector(nodeLW, &v1.Node{}, nodeIndexer, 0) go r.Run(wait.NeverStop) } nodeInfo := &predicates.CachedNodeInfo{NodeLister: corelisters.NewNodeLister(nodeIndexer)} 1.3. 各种Manager 1.3.1. containerRefManager containerRefManager := kubecontainer.NewRefManager() 1.3.2. oomWatcher oomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder) 1.3.3. dnsConfigurer clusterDNS := make([]net.IP, 0, len(kubeCfg.ClusterDNS)) for _, ipEntry := range kubeCfg.ClusterDNS { ip := net.ParseIP(ipEntry) if ip == nil { glog.Warningf(\"Invalid clusterDNS ip '%q'\", ipEntry) } else { clusterDNS = append(clusterDNS, ip) } } ... dns.NewConfigurer(kubeDeps.Recorder, nodeRef, parsedNodeIP, clusterDNS, kubeCfg.ClusterDomain, kubeCfg.ResolverConfig), 1.3.4. secretManager & configMapManager var secretManager secret.Manager var configMapManager configmap.Manager switch kubeCfg.ConfigMapAndSecretChangeDetectionStrategy { case kubeletconfiginternal.WatchChangeDetectionStrategy: secretManager = secret.NewWatchingSecretManager(kubeDeps.KubeClient) configMapManager = configmap.NewWatchingConfigMapManager(kubeDeps.KubeClient) case kubeletconfiginternal.TTLCacheChangeDetectionStrategy: secretManager = secret.NewCachingSecretManager( kubeDeps.KubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode)) configMapManager = configmap.NewCachingConfigMapManager( kubeDeps.KubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode)) case kubeletconfiginternal.GetChangeDetectionStrategy: secretManager = secret.NewSimpleSecretManager(kubeDeps.KubeClient) configMapManager = configmap.NewSimpleConfigMapManager(kubeDeps.KubeClient) default: return nil, fmt.Errorf(\"unknown configmap and secret manager mode: %v\", kubeCfg.ConfigMapAndSecretChangeDetectionStrategy) } klet.secretManager = secretManager klet.configMapManager = configMapManager 1.3.5. livenessManager klet.livenessManager = proberesults.NewManager() 1.3.6. podManager // podManager is also responsible for keeping secretManager and configMapManager contents up-to-date. klet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager) 1.3.7. resourceAnalyzer klet.resourceAnalyzer = serverstats.NewResourceAnalyzer(klet, kubeCfg.VolumeStatsAggPeriod.Duration) 1.3.8. containerGC // setup containerGC containerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady) if err != nil { return nil, err } klet.containerGC = containerGC klet.containerDeletor = newPodContainerDeletor(klet.containerRuntime, integer.IntMax(containerGCPolicy.MaxPerPodContainer, minDeadContainerInPod)) 1.3.9. imageManager // setup imageManager imageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage) if err != nil { return nil, fmt.Errorf(\"failed to initialize image manager: %v\", err) } klet.imageManager = imageManager 1.3.10. statusManager klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet) 1.3.11. probeManager klet.probeManager = prober.NewManager( klet.statusManager, klet.livenessManager, klet.runner, containerRefManager, kubeDeps.Recorder) 1.3.12. tokenManager tokenManager := token.NewManager(kubeDeps.KubeClient) 1.3.13. volumePluginMgr klet.volumePluginMgr, err = NewInitializedVolumePluginMgr(klet, secretManager, configMapManager, tokenManager, kubeDeps.VolumePlugins, kubeDeps.DynamicPluginProber) if err != nil { return nil, err } if klet.enablePluginsWatcher { klet.pluginWatcher = pluginwatcher.NewWatcher(klet.getPluginsDir()) } 1.3.14. volumeManager // setup volumeManager klet.volumeManager = volumemanager.NewVolumeManager( kubeCfg.EnableControllerAttachDetach, nodeName, klet.podManager, klet.statusManager, klet.kubeClient, klet.volumePluginMgr, klet.containerRuntime, kubeDeps.Mounter, klet.getPodsDir(), kubeDeps.Recorder, experimentalCheckNodeCapabilitiesBeforeMount, keepTerminatedPodVolumes) 1.3.15. evictionManager // setup eviction manager evictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock) klet.evictionManager = evictionManager klet.admitHandlers.AddPodAdmitHandler(evictionAdmitHandler) 1.4. containerRuntime 目前pod所使用的runtime只有docker和remote两种，rkt已经废弃。 if containerRuntime == \"rkt\" { glog.Fatalln(\"rktnetes has been deprecated in favor of rktlet. Please see https://github.com/kubernetes-incubator/rktlet for more information.\") } 当runtime是docker的时候，会执行docker相关操作。 switch containerRuntime { case kubetypes.DockerContainerRuntime: // Create and start the CRI shim running as a grpc server. ... // The unix socket for kubelet dockershim communication. ... // Create dockerLegacyService when the logging driver is not supported. ... case kubetypes.RemoteContainerRuntime: // No-op. break default: return nil, fmt.Errorf(\"unsupported CRI runtime: %q\", containerRuntime) } 1.4.1. NewDockerService // Create and start the CRI shim running as a grpc server. streamingConfig := getStreamingConfig(kubeCfg, kubeDeps, crOptions) ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, &pluginSettings, runtimeCgroups, kubeCfg.CgroupDriver, crOptions.DockershimRootDirectory, !crOptions.RedirectContainerStreaming) if err != nil { return nil, err } if crOptions.RedirectContainerStreaming { klet.criHandler = ds } 1.4.2. NewDockerServer // The unix socket for kubelet dockershim communication. glog.V(5).Infof(\"RemoteRuntimeEndpoint: %q, RemoteImageEndpoint: %q\", remoteRuntimeEndpoint, remoteImageEndpoint) glog.V(2).Infof(\"Starting the GRPC server for the docker CRI shim.\") server := dockerremote.NewDockerServer(remoteRuntimeEndpoint, ds) if err := server.Start(); err != nil { return nil, err } 1.4.3. DockerServer.Start // Start starts the dockershim grpc server. func (s *DockerServer) Start() error { // Start the internal service. if err := s.service.Start(); err != nil { glog.Errorf(\"Unable to start docker service\") return err } glog.V(2).Infof(\"Start dockershim grpc server\") l, err := util.CreateListener(s.endpoint) if err != nil { return fmt.Errorf(\"failed to listen on %q: %v\", s.endpoint, err) } // Create the grpc server and register runtime and image services. s.server = grpc.NewServer( grpc.MaxRecvMsgSize(maxMsgSize), grpc.MaxSendMsgSize(maxMsgSize), ) runtimeapi.RegisterRuntimeServiceServer(s.server, s.service) runtimeapi.RegisterImageServiceServer(s.server, s.service) go func() { if err := s.server.Serve(l); err != nil { glog.Fatalf(\"Failed to serve connections: %v\", err) } }() return nil } 1.5. podWorker 构造podWorkers和workQueue。 klet.workQueue = queue.NewBasicWorkQueue(klet.clock) klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache) 1.5.1. PodWorkers接口 // PodWorkers is an abstract interface for testability. type PodWorkers interface { UpdatePod(options *UpdatePodOptions) ForgetNonExistingPodWorkers(desiredPods map[types.UID]empty) ForgetWorker(uid types.UID) } podWorker主要用来对pod相应事件进行处理和同步，包含以下三个方法：UpdatePod、ForgetNonExistingPodWorkers、ForgetWorker。 2. 总结 NewMainKubelet主要用来构造kubelet结构体，其中kubelet除了包含必要的配置和client（例如：kubeClient、csiClient等）外，最主要的包含各种manager来管理不同的任务。 核心的manager有以下几种： oomWatcher：监控pod内存是否发生OOM。 podManager：管理pod的生命周期，包括对pod的增删改查操作等。 containerGC：对死亡容器进行垃圾回收。 imageManager：对容器镜像进行垃圾回收。 statusManager：与apiserver同步pod状态，同时也作状态缓存。 volumeManager：对pod的volume进行attached/detached/mounted/unmounted操作。 evictionManager：保证节点稳定，必要时对pod进行驱逐（例如资源不足的情况下）。 NewMainKubelet还包含了serviceLister和nodeLister来监听service和node的列表变化。 kubelet使用到的containerRuntime目前主要是docker，其中rkt已废弃。NewMainKubelet启动了dockershim grpc server来执行docker相关操作。 构建了podWorker来对pod相关的更新逻辑进行处理。 参考文章： https://github.com/kubernetes/kubernetes/tree/v1.12.0 https://github.com/kubernetes/kubernetes/tree/v1.12.0/pkg/kubelet Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kubelet/startKubelet.html":{"url":"kubelet/startKubelet.html","title":"startKubelet","keywords":"","body":"kubelet源码分析（三）之 startKubelet 以下代码分析基于 kubernetes v1.12.0 版本。 本文主要分析startKubelet，其中主要是kubelet.Run部分，该部分的内容主要是初始化并运行一些manager。对于kubelet所包含的各种manager的执行逻辑和pod的生命周期管理逻辑待后续文章分析。 后续的文章主要会分类分析pkg/kubelet部分的代码实现。 kubelet的pkg代码目录结构： kubelet ├── apis # 定义一些相关接口 ├── cadvisor # cadvisor ├── cm # ContainerManager、cpu manger、cgroup manager ├── config ├── configmap # configmap manager ├── container # Runtime、ImageService ├── dockershim # docker的相关调用 ├── eviction # eviction manager ├── images # image manager ├── kubeletconfig ├── kuberuntime # 核心：kubeGenericRuntimeManager、runtime容器的相关操作 ├── lifecycle ├── mountpod ├── network # pod dns ├── nodelease ├── nodestatus # MachineInfo、节点相关信息 ├── pleg # PodLifecycleEventGenerator ├── pod # 核心：pod manager、mirror pod ├── preemption ├── qos # 资源服务质量，不过暂时内容很少 ├── remote # RemoteRuntimeService ├── server ├── stats # StatsProvider ├── status # status manager ├── types # PodUpdate、PodOperation ├── volumemanager # VolumeManager ├── kubelet.go # 核心: SyncHandler、kubelet的大部分操作 ├── kubelet_getters.go # 各种get操作，例如获取相关目录：getRootDir、getPodsDir、getPluginsDir ├── kubelet_network.go # ├── kubelet_network_linux.go ├── kubelet_node_status.go # registerWithAPIServer、initialNode、syncNodeStatus ├── kubelet_pods.go # 核心：pod的增删改查等相关操作、podKiller、 ├── kubelet_resources.go ├── kubelet_volumes.go # ListVolumesForPod、cleanupOrphanedPodDirs ├── oom_watcher.go # OOMWatcher ├── pod_container_deletor.go ├── pod_workers.go # 核心：PodWorkers、UpdatePodOptions、syncPodOptions、managePodLoop ├── runonce.go # RunOnce ├── runtime.go ... 1. startKubelet startKubelet的函数位于cmd/kubelet/app/server.go，启动并运行一个kubelet，运行kubelet的逻辑代码位于pkg/kubelet/kubelet.go。 主要内容如下： 运行一个kubelet，执行kubelet中各种manager的相关逻辑。 运行kubelet server启动监听服务。 此部分代码位于cmd/kubelet/app/server.go func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) { // start the kubelet go wait.Until(func() { k.Run(podCfg.Updates()) }, 0, wait.NeverStop) // start the kubelet server if enableServer { go k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling) } if kubeCfg.ReadOnlyPort > 0 { go k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort)) } } 2. Kubelet.Run Kubelet.Run方法主要将NewMainKubelet构造的各种manager运行起来，让各种manager执行相应的功能，大部分manager为常驻进程的方式运行。 Kubelet.Run完整代码如下： 此部分代码位于pkg/kubelet/kubelet.go // Run starts the kubelet reacting to config updates func (kl *Kubelet) Run(updates 以下对Kubelet.Run分段进行分析。 3. initializeModules initializeModules包含了imageManager、serverCertificateManager、oomWatcher和resourceAnalyzer。 主要流程如下： 创建文件系统目录，包括kubelet的root目录、pods的目录、plugins的目录和容器日志目录。 启动imageManager、serverCertificateManager、oomWatcher、resourceAnalyzer。 各种manager的说明如下： imageManager：负责镜像垃圾回收。 serverCertificateManager：负责处理证书。 oomWatcher：监控内存使用，是否发生内存耗尽。 resourceAnalyzer：监控资源使用情况。 完整代码如下： 此部分代码位于pkg/kubelet/kubelet.go // initializeModules will initialize internal modules that do not require the container runtime to be up. // Note that the modules here must not depend on modules that are not initialized here. func (kl *Kubelet) initializeModules() error { // Prometheus metrics. metrics.Register(kl.runtimeCache, collectors.NewVolumeStatsCollector(kl)) // Setup filesystem directories. if err := kl.setupDataDirs(); err != nil { return err } // If the container logs directory does not exist, create it. if _, err := os.Stat(ContainerLogsDir); err != nil { if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil { glog.Errorf(\"Failed to create directory %q: %v\", ContainerLogsDir, err) } } // Start the image manager. kl.imageManager.Start() // Start the certificate manager if it was enabled. if kl.serverCertificateManager != nil { kl.serverCertificateManager.Start() } // Start out of memory watcher. if err := kl.oomWatcher.Start(kl.nodeRef); err != nil { return fmt.Errorf(\"Failed to start OOM watcher %v\", err) } // Start resource analyzer kl.resourceAnalyzer.Start() return nil } 3.1. setupDataDirs initializeModules先创建相关目录。 具体目录如下： ContainerLogsDir：目录为/var/log/containers。 rootDirectory：由参数传入，一般为/var/lib/kubelet。 PodsDir：目录为{rootDirectory}/pods。 PluginsDir：目录为{rootDirectory}/plugins。 initializeModules中setupDataDirs的相关代码如下： // Setup filesystem directories. if err := kl.setupDataDirs(); err != nil { return err } // If the container logs directory does not exist, create it. if _, err := os.Stat(ContainerLogsDir); err != nil { if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil { glog.Errorf(\"Failed to create directory %q: %v\", ContainerLogsDir, err) } } setupDataDirs代码如下 // setupDataDirs creates: // 1. the root directory // 2. the pods directory // 3. the plugins directory func (kl *Kubelet) setupDataDirs() error { kl.rootDirectory = path.Clean(kl.rootDirectory) if err := os.MkdirAll(kl.getRootDir(), 0750); err != nil { return fmt.Errorf(\"error creating root directory: %v\", err) } if err := kl.mounter.MakeRShared(kl.getRootDir()); err != nil { return fmt.Errorf(\"error configuring root directory: %v\", err) } if err := os.MkdirAll(kl.getPodsDir(), 0750); err != nil { return fmt.Errorf(\"error creating pods directory: %v\", err) } if err := os.MkdirAll(kl.getPluginsDir(), 0750); err != nil { return fmt.Errorf(\"error creating plugins directory: %v\", err) } return nil } 3.2. manager initializeModules中的manager如下： // Start the image manager. kl.imageManager.Start() // Start the certificate manager if it was enabled. if kl.serverCertificateManager != nil { kl.serverCertificateManager.Start() } // Start out of memory watcher. if err := kl.oomWatcher.Start(kl.nodeRef); err != nil { return fmt.Errorf(\"Failed to start OOM watcher %v\", err) } // Start resource analyzer kl.resourceAnalyzer.Start() 4. 运行各种manager 4.1. volumeManager volumeManager主要运行一组异步循环，根据在此节点上安排的pod调整哪些volume需要attached/detached/mounted/unmounted。 // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) volumeManager.Run实现代码如下： func (vm *volumeManager) Run(sourcesReady config.SourcesReady, stopCh 4.2. syncNodeStatus syncNodeStatus通过goroutine的方式定期执行，它将节点的状态同步给master，必要的时候注册kubelet。 if kl.kubeClient != nil { // Start syncing node status immediately, this may set up things the runtime needs to run. go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop) go kl.fastStatusUpdateOnce() // start syncing lease if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) { go kl.nodeLeaseController.Run(wait.NeverStop) } } 4.3. updateRuntimeUp updateRuntimeUp调用容器运行时状态回调，在容器运行时首次启动时初始化运行时相关模块，如果状态检查失败则返回错误。 如果状态检查正常，在kubelet runtimeState中更新容器运行时的正常运行时间。 go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) 4.4. syncNetworkUtil 通过循环的方式同步iptables的规则，不过当前代码并没有执行任何操作。 // Start loop to sync iptables util rules if kl.makeIPTablesUtilChains { go wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop) } 4.5. podKiller 但pod没有被podworker正确处理的时候，启动一个goroutine负责杀死pod。 // Start a goroutine responsible for killing pods (that are not properly // handled by pod workers). go wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop) podKiller代码如下： 此部分代码位于pkg/kubelet/kubelet_pods.go // podKiller launches a goroutine to kill a pod received from the channel if // another goroutine isn't already in action. func (kl *Kubelet) podKiller() { killing := sets.NewString() // guard for the killing set lock := sync.Mutex{} for podPair := range kl.podKillingCh { runningPod := podPair.RunningPod apiPod := podPair.APIPod lock.Lock() exists := killing.Has(string(runningPod.ID)) if !exists { killing.Insert(string(runningPod.ID)) } lock.Unlock() if !exists { go func(apiPod *v1.Pod, runningPod *kubecontainer.Pod) { glog.V(2).Infof(\"Killing unwanted pod %q\", runningPod.Name) err := kl.killPod(apiPod, runningPod, nil, nil) if err != nil { glog.Errorf(\"Failed killing the pod %q: %v\", runningPod.Name, err) } lock.Lock() killing.Delete(string(runningPod.ID)) lock.Unlock() }(apiPod, runningPod) } } } 4.6. statusManager 使用apiserver同步pods状态; 也用作状态缓存。 // Start component sync loops. kl.statusManager.Start() statusManager.Start的实现代码如下： func (m *manager) Start() { // Don't start the status manager if we don't have a client. This will happen // on the master, where the kubelet is responsible for bootstrapping the pods // of the master components. if m.kubeClient == nil { glog.Infof(\"Kubernetes client is nil, not starting status manager.\") return } glog.Info(\"Starting to sync pod status with apiserver\") syncTicker := time.Tick(syncPeriod) // syncPod and syncBatch share the same go routine to avoid sync races. go wait.Forever(func() { select { case syncRequest := 4.7. probeManager 处理容器探针 kl.probeManager.Start() 4.8. runtimeClassManager // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil { go kl.runtimeClassManager.Run(wait.NeverStop) } 4.9. PodLifecycleEventGenerator // Start the pod lifecycle event generator. kl.pleg.Start() PodLifecycleEventGenerator是一个pod生命周期时间生成器接口，具体如下： // PodLifecycleEventGenerator contains functions for generating pod life cycle events. type PodLifecycleEventGenerator interface { Start() Watch() chan *PodLifecycleEvent Healthy() (bool, error) } start方法具体实现如下： // Start spawns a goroutine to relist periodically. func (g *GenericPLEG) Start() { go wait.Until(g.relist, g.relistPeriod, wait.NeverStop) } 4.10. syncLoop 最后调用syncLoop来执行同步变化变更的循环。 kl.syncLoop(updates, kl) 5. syncLoop syncLoop是处理变化的循环。 它监听来自三种channel（file，apiserver和http）的更改。 对于看到的任何新更改，将针对所需状态和运行状态运行同步。 如果没有看到配置的变化，将在每个同步频率秒同步最后已知的所需状态。 // syncLoop is the main loop for processing changes. It watches for changes from // three channels (file, apiserver, and http) and creates a union of them. For // any new change seen, will run a sync against desired state and running state. If // no changes are seen to the configuration, will synchronize the last known desired // state every sync-frequency seconds. Never returns. func (kl *Kubelet) syncLoop(updates 其中调用了syncLoopIteration的函数来执行更具体的监控pod变化的循环。syncLoopIteration代码逻辑待后续单独分析。 6. 总结 6.1. 基本流程 Kubelet.Run主要流程如下： 初始化模块，其实就是运行imageManager、serverCertificateManager、oomWatcher、resourceAnalyzer。 运行各种manager，大部分以常驻goroutine的方式运行，其中包括volumeManager、statusManager等。 执行处理变更的循环函数syncLoop，对pod的生命周期进行管理。 syncLoop： syncLoop函数，对pod的生命周期进行管理，其中syncLoop调用了syncLoopIteration函数，该函数根据podUpdate的信息，针对不同的操作，由SyncHandler来执行pod的增删改查等生命周期的管理，其中的syncHandler包括HandlePodSyncs和HandlePodCleanups等。该部分逻辑待后续文章具体分析。 6.2. Manager 以下介绍kubelet运行时涉及到的manager的内容。 manager 说明 imageManager 负责镜像垃圾回收 serverCertificateManager 负责处理证书 oomWatcher 监控内存使用，是否发生内存耗尽即OOM resourceAnalyzer 监控资源使用情况 volumeManager 对pod执行attached/detached/mounted/unmounted操作 statusManager 使用apiserver同步pods状态; 也用作状态缓存 probeManager 处理容器探针 runtimeClassManager 同步RuntimeClasses podKiller 负责杀死pod 参考文章： https://github.com/kubernetes/kubernetes/blob/v1.12.0/cmd/kubelet/app/server.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kubelet.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kubelet/syncLoopIteration.html":{"url":"kubelet/syncLoopIteration.html","title":"syncLoopIteration","keywords":"","body":"kubelet源码分析（四）之 syncLoopIteration 以下代码分析基于 kubernetes v1.12.0 版本。 本文主要分析kubelet中syncLoopIteration部分。syncLoopIteration通过几种channel来对不同类型的事件进行监听并做增删改查的处理。 1. syncLoop syncLoop是处理变更的循环。 它监听来自三种channel（file，apiserver和http）的更改。 对于看到的任何新更改，将针对所需状态和运行状态运行同步。 如果没有看到配置的变化，将在每个同步频率秒同步最后已知的所需状态。 此部分代码位于pkg/kubelet/kubelet.go // syncLoop is the main loop for processing changes. It watches for changes from // three channels (file, apiserver, and http) and creates a union of them. For // any new change seen, will run a sync against desired state and running state. If // no changes are seen to the configuration, will synchronize the last known desired // state every sync-frequency seconds. Never returns. func (kl *Kubelet) syncLoop(updates 其中调用了syncLoopIteration的函数来执行更具体的监控pod变化的循环。 2. syncLoopIteration syncLoopIteration主要通过几种channel来对不同类型的事件进行监听并处理。其中包括：configCh、plegCh、syncCh、houseKeepingCh、livenessManager.Updates()。 syncLoopIteration实际执行了pod的操作，此部分设置了几种不同的channel: configCh：将配置更改的pod分派给事件类型的相应处理程序回调。 plegCh：更新runtime缓存，同步pod。 syncCh：同步所有等待同步的pod。 houseKeepingCh：触发清理pod。 livenessManager.Updates()：对失败的pod或者liveness检查失败的pod进行sync操作。 syncLoopIteration部分代码位于pkg/kubelet/kubelet.go 2.1. configCh configCh将配置更改的pod分派给事件类型的相应处理程序回调，该部分主要通过SyncHandler对pod的不同事件进行增删改查等操作。 func (kl *Kubelet) syncLoopIteration(configCh 可以看出syncLoopIteration根据podUpdate的值来执行不同的pod操作，具体如下： ADD：HandlePodAdditions UPDATE：HandlePodUpdates REMOVE：HandlePodRemoves RECONCILE：HandlePodReconcile DELETE：HandlePodUpdates RESTORE：HandlePodAdditions podsToSync：HandlePodSyncs 其中执行pod的handler操作的是SyncHandler，该类型是一个接口，实现体为kubelet本身，具体见后续分析。 2.2. plegCh plegCh：更新runtime缓存，同步pod。此处调用了HandlePodSyncs的函数。 case e := 2.3. syncCh syncCh：同步所有等待同步的pod。此处调用了HandlePodSyncs的函数。 case 2.4. livenessManager.Update livenessManager.Updates()：对失败的pod或者liveness检查失败的pod进行sync操作。此处调用了HandlePodSyncs的函数。 case update := 2.5. housekeepingCh houseKeepingCh：触发清理pod。此处调用了HandlePodCleanups的函数。 case 3. SyncHandler SyncHandler是一个定义Pod的不同Handler的接口，具体是实现者是kubelet，该接口的方法主要在syncLoopIteration中调用，接口定义如下： // SyncHandler is an interface implemented by Kubelet, for testability type SyncHandler interface { HandlePodAdditions(pods []*v1.Pod) HandlePodUpdates(pods []*v1.Pod) HandlePodRemoves(pods []*v1.Pod) HandlePodReconcile(pods []*v1.Pod) HandlePodSyncs(pods []*v1.Pod) HandlePodCleanups() error } SyncHandler部分代码位于pkg/kubelet/kubelet.go 3.1. HandlePodAdditions HandlePodAdditions先根据pod创建时间对pod进行排序，然后遍历pod列表，来执行pod的相关操作。 // HandlePodAdditions is the callback in SyncHandler for pods being added from // a config source. func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) { start := kl.clock.Now() sort.Sort(sliceutils.PodsByCreationTime(pods)) for _, pod := range pods { ... } } 将pod添加到pod manager中。 for _, pod := range pods { // Responsible for checking limits in resolv.conf if kl.dnsConfigurer != nil && kl.dnsConfigurer.ResolverConfig != \"\" { kl.dnsConfigurer.CheckLimitsForResolvConf() } existingPods := kl.podManager.GetPods() // Always add the pod to the pod manager. Kubelet relies on the pod // manager as the source of truth for the desired state. If a pod does // not exist in the pod manager, it means that it has been deleted in // the apiserver and no action (other than cleanup) is required. kl.podManager.AddPod(pod) ... } 如果是mirror pod，则对mirror pod进行处理。 if kubepod.IsMirrorPod(pod) { kl.handleMirrorPod(pod, start) continue } 如果当前pod的状态不是Terminated状态，则判断是否接受该pod，如果不接受则将pod状态改为Failed。 if !kl.podIsTerminated(pod) { // Only go through the admission process if the pod is not // terminated. // We failed pods that we rejected, so activePods include all admitted // pods that are alive. activePods := kl.filterOutTerminatedPods(existingPods) // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok { kl.rejectPod(pod, reason, message) continue } } 执行dispatchWork函数，该函数是syncHandler中调用到的核心函数，该函数在pod worker中启动一个异步循环，来分派pod的相关操作。该函数的具体操作待后续分析。 mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start) 最后加pod添加到probe manager中。 kl.probeManager.AddPod(pod) 3.2. HandlePodUpdates HandlePodUpdates同样遍历pod列表，执行相应的操作。 // HandlePodUpdates is the callback in the SyncHandler interface for pods // being updated from a config source. func (kl *Kubelet) HandlePodUpdates(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { ... } } 将pod更新到pod manager中。 for _, pod := range pods { // Responsible for checking limits in resolv.conf if kl.dnsConfigurer != nil && kl.dnsConfigurer.ResolverConfig != \"\" { kl.dnsConfigurer.CheckLimitsForResolvConf() } kl.podManager.UpdatePod(pod) ... } 如果是mirror pod，则对mirror pod进行处理。 if kubepod.IsMirrorPod(pod) { kl.handleMirrorPod(pod, start) continue } 执行dispatchWork函数。 // TODO: Evaluate if we need to validate and reject updates. mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodUpdate, mirrorPod, start) 3.3. HandlePodRemoves HandlePodRemoves遍历pod列表。 // HandlePodRemoves is the callback in the SyncHandler interface for pods // being removed from a config source. func (kl *Kubelet) HandlePodRemoves(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { ... } } 从pod manager中删除pod。 for _, pod := range pods { kl.podManager.DeletePod(pod) ... } 如果是mirror pod，则对mirror pod进行处理。 if kubepod.IsMirrorPod(pod) { kl.handleMirrorPod(pod, start) continue } 调用kubelet的deletePod函数来删除pod。 // Deletion is allowed to fail because the periodic cleanup routine // will trigger deletion again. if err := kl.deletePod(pod); err != nil { glog.V(2).Infof(\"Failed to delete pod %q, err: %v\", format.Pod(pod), err) } deletePod 函数将需要删除的pod加入podKillingCh的channel中，有podKiller监听这个channel去执行删除任务，实现如下： // deletePod deletes the pod from the internal state of the kubelet by: // 1. stopping the associated pod worker asynchronously // 2. signaling to kill the pod by sending on the podKillingCh channel // // deletePod returns an error if not all sources are ready or the pod is not // found in the runtime cache. func (kl *Kubelet) deletePod(pod *v1.Pod) error { if pod == nil { return fmt.Errorf(\"deletePod does not allow nil pod\") } if !kl.sourcesReady.AllReady() { // If the sources aren't ready, skip deletion, as we may accidentally delete pods // for sources that haven't reported yet. return fmt.Errorf(\"skipping delete because sources aren't ready yet\") } kl.podWorkers.ForgetWorker(pod.UID) // Runtime cache may not have been updated to with the pod, but it's okay // because the periodic cleanup routine will attempt to delete again later. runningPods, err := kl.runtimeCache.GetPods() if err != nil { return fmt.Errorf(\"error listing containers: %v\", err) } runningPod := kubecontainer.Pods(runningPods).FindPod(\"\", pod.UID) if runningPod.IsEmpty() { return fmt.Errorf(\"pod not found\") } podPair := kubecontainer.PodPair{APIPod: pod, RunningPod: &runningPod} kl.podKillingCh 从probe manager中移除pod。 kl.probeManager.RemovePod(pod) 3.4. HandlePodReconcile 遍历pod列表。 // HandlePodReconcile is the callback in the SyncHandler interface for pods // that should be reconciled. func (kl *Kubelet) HandlePodReconcile(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { ... } } 将pod更新到pod manager中。 for _, pod := range pods { // Update the pod in pod manager, status manager will do periodically reconcile according // to the pod manager. kl.podManager.UpdatePod(pod) ... } 必要时调整pod的Ready状态，执行dispatchWork函数。 // Reconcile Pod \"Ready\" condition if necessary. Trigger sync pod for reconciliation. if status.NeedToReconcilePodReadiness(pod) { mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodSync, mirrorPod, start) } 如果pod被设定为需要被驱逐的，则删除pod中的容器。 // After an evicted pod is synced, all dead containers in the pod can be removed. if eviction.PodIsEvicted(pod.Status) { if podStatus, err := kl.podCache.Get(pod.UID); err == nil { kl.containerDeletor.deleteContainersInPod(\"\", podStatus, true) } } 3.5. HandlePodSyncs HandlePodSyncs是syncHandler接口回调函数，调用dispatchWork，通过pod worker来执行任务。 // HandlePodSyncs is the callback in the syncHandler interface for pods // that should be dispatched to pod workers for sync. func (kl *Kubelet) HandlePodSyncs(pods []*v1.Pod) { start := kl.clock.Now() for _, pod := range pods { mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodSync, mirrorPod, start) } } 3.6. HandlePodCleanups HandlePodCleanups主要用来执行pod的清理任务，其中包括terminating的pod，orphaned的pod等。 首先查看pod使用到的cgroup。 // HandlePodCleanups performs a series of cleanup work, including terminating // pod workers, killing unwanted pods, and removing orphaned volumes/pod // directories. // NOTE: This function is executed by the main sync loop, so it // should not contain any blocking calls. func (kl *Kubelet) HandlePodCleanups() error { // The kubelet lacks checkpointing, so we need to introspect the set of pods // in the cgroup tree prior to inspecting the set of pods in our pod manager. // this ensures our view of the cgroup tree does not mistakenly observe pods // that are added after the fact... var ( cgroupPods map[types.UID]cm.CgroupName err error ) if kl.cgroupsPerQOS { pcm := kl.containerManager.NewPodContainerManager() cgroupPods, err = pcm.GetAllPodsFromCgroups() if err != nil { return fmt.Errorf(\"failed to get list of pods that still exist on cgroup mounts: %v\", err) } } ... } 列出所有pod包括mirror pod。 allPods, mirrorPods := kl.podManager.GetPodsAndMirrorPods() // Pod phase progresses monotonically. Once a pod has reached a final state, // it should never leave regardless of the restart policy. The statuses // of such pods should not be changed, and there is no need to sync them. // TODO: the logic here does not handle two cases: // 1. If the containers were removed immediately after they died, kubelet // may fail to generate correct statuses, let alone filtering correctly. // 2. If kubelet restarted before writing the terminated status for a pod // to the apiserver, it could still restart the terminated pod (even // though the pod was not considered terminated by the apiserver). // These two conditions could be alleviated by checkpointing kubelet. activePods := kl.filterOutTerminatedPods(allPods) desiredPods := make(map[types.UID]empty) for _, pod := range activePods { desiredPods[pod.UID] = empty{} } pod worker停止不再存在的pod的任务，并从probe manager中清除pod。 // Stop the workers for no-longer existing pods. // TODO: is here the best place to forget pod workers? kl.podWorkers.ForgetNonExistingPodWorkers(desiredPods) kl.probeManager.CleanupPods(activePods) 将需要杀死的pod加入到podKillingCh的channel中，podKiller的任务会监听该channel并获取需要杀死的pod列表来执行杀死pod的操作。 runningPods, err := kl.runtimeCache.GetPods() if err != nil { glog.Errorf(\"Error listing containers: %#v\", err) return err } for _, pod := range runningPods { if _, found := desiredPods[pod.ID]; !found { kl.podKillingCh 当pod不再被绑定到该节点，移除podStatus，其中removeOrphanedPodStatuses最后调用的函数是statusManager的RemoveOrphanedStatuses方法。 kl.removeOrphanedPodStatuses(allPods, mirrorPods) 移除所有的orphaned volume。 // Remove any orphaned volumes. // Note that we pass all pods (including terminated pods) to the function, // so that we don't remove volumes associated with terminated but not yet // deleted pods. err = kl.cleanupOrphanedPodDirs(allPods, runningPods) if err != nil { // We want all cleanup tasks to be run even if one of them failed. So // we just log an error here and continue other cleanup tasks. // This also applies to the other clean up tasks. glog.Errorf(\"Failed cleaning up orphaned pod directories: %v\", err) } 移除mirror pod。 // Remove any orphaned mirror pods. kl.podManager.DeleteOrphanedMirrorPods() 删除不再运行的pod的cgroup。 // Remove any cgroups in the hierarchy for pods that are no longer running. if kl.cgroupsPerQOS { kl.cleanupOrphanedPodCgroups(cgroupPods, activePods) } 执行垃圾回收（GC）操作。 kl.backOff.GC() 4. dispatchWork dispatchWork通过pod worker启动一个异步的循环。 完整代码如下： // dispatchWork starts the asynchronous sync of the pod in a pod worker. // If the pod is terminated, dispatchWork func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) { if kl.podIsTerminated(pod) { if pod.DeletionTimestamp != nil { // If the pod is in a terminated state, there is no pod worker to // handle the work item. Check if the DeletionTimestamp has been // set, and force a status update to trigger a pod deletion request // to the apiserver. kl.statusManager.TerminatePod(pod) } return } // Run the sync in an async worker. kl.podWorkers.UpdatePod(&UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: syncType, OnCompleteFunc: func(err error) { if err != nil { metrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start)) } }, }) // Note the number of containers for new pods. if syncType == kubetypes.SyncPodCreate { metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers))) } } 以下分段进行分析： 如果pod的状态是处于Terminated状态，则执行statusManager的TerminatePod操作。 // dispatchWork starts the asynchronous sync of the pod in a pod worker. // If the pod is terminated, dispatchWork func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) { if kl.podIsTerminated(pod) { if pod.DeletionTimestamp != nil { // If the pod is in a terminated state, there is no pod worker to // handle the work item. Check if the DeletionTimestamp has been // set, and force a status update to trigger a pod deletion request // to the apiserver. kl.statusManager.TerminatePod(pod) } return } ... } 执行pod worker的UpdatePod函数，该函数是pod worker的核心函数，来执行pod相关操作。具体逻辑待下文分析。 // Run the sync in an async worker. kl.podWorkers.UpdatePod(&UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: syncType, OnCompleteFunc: func(err error) { if err != nil { metrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start)) } }, }) 当创建类型是SyncPodCreate（即创建pod的时候），统计新pod中容器的数目。 // Note the number of containers for new pods. if syncType == kubetypes.SyncPodCreate { metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers))) } 5. PodWorkers.UpdatePod PodWorkers是一个接口类型： // PodWorkers is an abstract interface for testability. type PodWorkers interface { UpdatePod(options *UpdatePodOptions) ForgetNonExistingPodWorkers(desiredPods map[types.UID]empty) ForgetWorker(uid types.UID) } 其中UpdatePod是一个核心方法，通过podUpdates的channel来传递需要处理的pod信息，对于新创建的pod每个pod都会由一个goroutine来执行managePodLoop。 此部分代码位于pkg/kubelet/pod_workers.go // Apply the new setting to the specified pod. // If the options provide an OnCompleteFunc, the function is invoked if the update is accepted. // Update requests are ignored if a kill pod request is pending. func (p *podWorkers) UpdatePod(options *UpdatePodOptions) { pod := options.Pod uid := pod.UID var podUpdates chan UpdatePodOptions var exists bool p.podLock.Lock() defer p.podLock.Unlock() if podUpdates, exists = p.podUpdates[uid]; !exists { // We need to have a buffer here, because checkForUpdates() method that // puts an update into channel is called from the same goroutine where // the channel is consumed. However, it is guaranteed that in such case // the channel is empty, so buffer of size 1 is enough. podUpdates = make(chan UpdatePodOptions, 1) p.podUpdates[uid] = podUpdates // Creating a new pod worker either means this is a new pod, or that the // kubelet just restarted. In either case the kubelet is willing to believe // the status of the pod for the first pod worker sync. See corresponding // comment in syncPod. go func() { defer runtime.HandleCrash() p.managePodLoop(podUpdates) }() } if !p.isWorking[pod.UID] { p.isWorking[pod.UID] = true podUpdates 6. managePodLoop managePodLoop通过读取podUpdateschannel的信息，执行syncPodFn函数，而syncPodFn函数在newPodWorkers的时候赋值了，即kubelet.syncPod。kubelet.syncPod具体代码逻辑待后续文章单独分析。 // newPodWorkers传入syncPod函数 klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache) newPodWorkers函数参考： func newPodWorkers(syncPodFn syncPodFnType, recorder record.EventRecorder, workQueue queue.WorkQueue, resyncInterval, backOffPeriod time.Duration, podCache kubecontainer.Cache) *podWorkers { return &podWorkers{ podUpdates: map[types.UID]chan UpdatePodOptions{}, isWorking: map[types.UID]bool{}, lastUndeliveredWorkUpdate: map[types.UID]UpdatePodOptions{}, syncPodFn: syncPodFn, // 构造传入klet.syncPod函数 recorder: recorder, workQueue: workQueue, resyncInterval: resyncInterval, backOffPeriod: backOffPeriod, podCache: podCache, } } managePodLoop函数参考： 此部分代码位于pkg/kubelet/pod_workers.go func (p *podWorkers) managePodLoop(podUpdates 7. 总结 syncLoopIteration基本流程如下： 通过几种channel来对不同类型的事件进行监听并处理。其中channel包括：configCh、plegCh、syncCh、houseKeepingCh、livenessManager.Updates()。 不同的SyncHandler执行不同的增删改查操作。 其中HandlePodAdditions、HandlePodUpdates、HandlePodReconcile、HandlePodSyncs都调用到了dispatchWork来执行pod的相关操作。HandlePodCleanups的pod清理任务，通过channel的方式加需要清理的pod给podKiller来清理。 dispatchWork调用podWorkers.UpdatePod执行异步操作。 podWorkers.UpdatePod中调用managePodLoop来执行pod相关操作循环。 channel类型及作用： configCh：将配置更改的pod分派给事件类型的相应处理程序回调。 plegCh：更新runtime缓存，同步pod。 syncCh：同步所有等待同步的pod。 houseKeepingCh：触发清理pod。 livenessManager.Updates()：对失败的pod或者liveness检查失败的pod进行sync操作。 参考： https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kubelet.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/pod_workers.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "},"kubelet/syncPod.html":{"url":"kubelet/syncPod.html","title":"syncPod","keywords":"","body":"kubelet源码分析（五）之 syncPod 以下代码分析基于 kubernetes v1.12.0 版本。 本文主要分析kubelet中syncPod的部分。 1. managePodLoop managePodLoop通过读取podUpdateschannel的信息，执行syncPodFn函数，而syncPodFn函数在newPodWorkers的时候赋值了，即kubelet.syncPod。 managePodLoop完整代码如下： 此部分代码位于pkg/kubelet/pod_workers.go func (p *podWorkers) managePodLoop(podUpdates 以下分析syncPod相关逻辑。 2. syncPod syncPod可以理解为是一个单个pod进行同步任务的事务脚本。其中入参是syncPodOptions，syncPodOptions记录了需要同步的pod的相关信息。具体定义如下： // syncPodOptions provides the arguments to a SyncPod operation. type syncPodOptions struct { // the mirror pod for the pod to sync, if it is a static pod mirrorPod *v1.Pod // pod to sync pod *v1.Pod // the type of update (create, update, sync) updateType kubetypes.SyncPodType // the current status podStatus *kubecontainer.PodStatus // if update type is kill, use the specified options to kill the pod. killPodOptions *KillPodOptions } syncPod主要执行以下的工作流： 如果是正在创建的pod，则记录pod worker的启动latency。 调用generateAPIPodStatus为pod提供v1.PodStatus信息。 如果pod是第一次运行，记录pod的启动latency。 更新status manager中的pod状态。 如果pod不应该被运行则杀死pod。 如果pod是一个static pod，并且没有对应的mirror pod，则创建一个mirror pod。 如果没有pod的数据目录则给pod创建对应的数据目录。 等待volume被attach或mount。 获取pod的secret数据。 调用container runtime的SyncPod函数，执行相关pod操作。 更新pod的ingress和egress的traffic limit。 当以上任务流中有任何的error，则return error。在下一次执行syncPod的任务流会被再次执行。对于错误信息会被记录到event中，方便debug。 以下对syncPod的执行过程进行分析。 syncPod的代码位于pkg/kubelet/kubelet.go 2.1. SyncPodKill 首先，获取syncPodOptions的pod信息。 func (kl *Kubelet) syncPod(o syncPodOptions) error { // pull out the required options pod := o.pod mirrorPod := o.mirrorPod podStatus := o.podStatus updateType := o.updateType ... } 如果pod是需要被杀死的，则执行killPod，会在指定的宽限期内杀死pod。 // if we want to kill a pod, do it now! if updateType == kubetypes.SyncPodKill { killPodOptions := o.killPodOptions if killPodOptions == nil || killPodOptions.PodStatusFunc == nil { return fmt.Errorf(\"kill pod options are required if update type is kill\") } apiPodStatus := killPodOptions.PodStatusFunc(pod, podStatus) kl.statusManager.SetPodStatus(pod, apiPodStatus) // we kill the pod with the specified grace period since this is a termination if err := kl.killPod(pod, nil, podStatus, killPodOptions.PodTerminationGracePeriodSecondsOverride); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, \"error killing pod: %v\", err) // there was an error killing the pod, so we return that error directly utilruntime.HandleError(err) return err } return nil } 2.2. SyncPodCreate 如果pod是需要被创建的，则记录pod的启动latency，latency与pod在apiserver中第一次被记录相关。 // Latency measurements for the main workflow are relative to the // first time the pod was seen by the API server. var firstSeenTime time.Time if firstSeenTimeStr, ok := pod.Annotations[kubetypes.ConfigFirstSeenAnnotationKey]; ok { firstSeenTime = kubetypes.ConvertToTimestamp(firstSeenTimeStr).Get() } // Record pod worker start latency if being created // TODO: make pod workers record their own latencies if updateType == kubetypes.SyncPodCreate { if !firstSeenTime.IsZero() { // This is the first time we are syncing the pod. Record the latency // since kubelet first saw the pod if firstSeenTime is set. metrics.PodWorkerStartLatency.Observe(metrics.SinceInMicroseconds(firstSeenTime)) } else { glog.V(3).Infof(\"First seen time not recorded for pod %q\", pod.UID) } } 通过pod和pod status生成最终的api pod status并设置pod的IP。 // Generate final API pod status with pod and status manager status apiPodStatus := kl.generateAPIPodStatus(pod, podStatus) // The pod IP may be changed in generateAPIPodStatus if the pod is using host network. (See #24576) // TODO(random-liu): After writing pod spec into container labels, check whether pod is using host network, and // set pod IP to hostIP directly in runtime.GetPodStatus podStatus.IP = apiPodStatus.PodIP 记录pod到running状态的时间。 // Record the time it takes for the pod to become running. existingStatus, ok := kl.statusManager.GetPodStatus(pod.UID) if !ok || existingStatus.Phase == v1.PodPending && apiPodStatus.Phase == v1.PodRunning && !firstSeenTime.IsZero() { metrics.PodStartLatency.Observe(metrics.SinceInMicroseconds(firstSeenTime)) } 如果pod是不可运行的，则更新pod和container的状态和相应的原因。 runnable := kl.canRunPod(pod) if !runnable.Admit { // Pod is not runnable; update the Pod and Container statuses to why. apiPodStatus.Reason = runnable.Reason apiPodStatus.Message = runnable.Message // Waiting containers are not creating. const waitingReason = \"Blocked\" for _, cs := range apiPodStatus.InitContainerStatuses { if cs.State.Waiting != nil { cs.State.Waiting.Reason = waitingReason } } for _, cs := range apiPodStatus.ContainerStatuses { if cs.State.Waiting != nil { cs.State.Waiting.Reason = waitingReason } } } 并更新status manager中的状态信息，杀死不可运行的pod。 // Update status in the status manager kl.statusManager.SetPodStatus(pod, apiPodStatus) // Kill pod if it should not be running if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed { var syncErr error if err := kl.killPod(pod, nil, podStatus, nil); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, \"error killing pod: %v\", err) syncErr = fmt.Errorf(\"error killing pod: %v\", err) utilruntime.HandleError(syncErr) } else { if !runnable.Admit { // There was no error killing the pod, but the pod cannot be run. // Return an error to signal that the sync loop should back off. syncErr = fmt.Errorf(\"pod cannot be run: %s\", runnable.Message) } } return syncErr } 如果网络插件还没到Ready状态，则只有在使用host网络模式的情况下才启动pod。 // If the network plugin is not ready, only start the pod if it uses the host network if rs := kl.runtimeState.networkErrors(); len(rs) != 0 && !kubecontainer.IsHostNetworkPod(pod) { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.NetworkNotReady, \"%s: %v\", NetworkNotReadyErrorMsg, rs) return fmt.Errorf(\"%s: %v\", NetworkNotReadyErrorMsg, rs) } 2.3. Cgroups 给pod创建Cgroups，如果cgroups-per-qos参数开启，则申请相应的资源。对于terminated的pod不需要创建或更新pod的Cgroups。 当重新启动kubelet并且启用cgroups-per-qos时，应该间歇性地终止所有pod的运行容器并在qos cgroup hierarchy下重新启动。 如果pod的cgroup已经存在或者pod第一次运行，不杀死pod中容器。 // Create Cgroups for the pod and apply resource parameters // to them if cgroups-per-qos flag is enabled. pcm := kl.containerManager.NewPodContainerManager() // If pod has already been terminated then we need not create // or update the pod's cgroup if !kl.podIsTerminated(pod) { // When the kubelet is restarted with the cgroups-per-qos // flag enabled, all the pod's running containers // should be killed intermittently and brought back up // under the qos cgroup hierarchy. // Check if this is the pod's first sync firstSync := true for _, containerStatus := range apiPodStatus.ContainerStatuses { if containerStatus.State.Running != nil { firstSync = false break } } // Don't kill containers in pod if pod's cgroups already // exists or the pod is running for the first time podKilled := false if !pcm.Exists(pod) && !firstSync { if err := kl.killPod(pod, nil, podStatus, nil); err == nil { podKilled = true } } ... 如果pod被杀死并且重启策略是Never，则不创建或更新对应的Cgroups，否则创建和更新pod的Cgroups。 // Create and Update pod's Cgroups // Don't create cgroups for run once pod if it was killed above // The current policy is not to restart the run once pods when // the kubelet is restarted with the new flag as run once pods are // expected to run only once and if the kubelet is restarted then // they are not expected to run again. // We don't create and apply updates to cgroup if its a run once pod and was killed above if !(podKilled && pod.Spec.RestartPolicy == v1.RestartPolicyNever) { if !pcm.Exists(pod) { if err := kl.containerManager.UpdateQOSCgroups(); err != nil { glog.V(2).Infof(\"Failed to update QoS cgroups while syncing pod: %v\", err) } if err := pcm.EnsureExists(pod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreatePodContainer, \"unable to ensure pod container exists: %v\", err) return fmt.Errorf(\"failed to ensure that the pod: %v cgroups exist and are correctly applied: %v\", pod.UID, err) } } } 其中创建Cgroups是通过containerManager的UpdateQOSCgroups来执行。 if err := kl.containerManager.UpdateQOSCgroups(); err != nil { glog.V(2).Infof(\"Failed to update QoS cgroups while syncing pod: %v\", err) } 2.4. Mirror Pod 如果pod是一个static pod，没有对应的mirror pod，则创建一个mirror pod；如果存在mirror pod则删除再重建一个mirror pod。 // Create Mirror Pod for Static Pod if it doesn't already exist if kubepod.IsStaticPod(pod) { podFullName := kubecontainer.GetPodFullName(pod) deleted := false if mirrorPod != nil { if mirrorPod.DeletionTimestamp != nil || !kl.podManager.IsMirrorPodOf(mirrorPod, pod) { // The mirror pod is semantically different from the static pod. Remove // it. The mirror pod will get recreated later. glog.Warningf(\"Deleting mirror pod %q because it is outdated\", format.Pod(mirrorPod)) if err := kl.podManager.DeleteMirrorPod(podFullName); err != nil { glog.Errorf(\"Failed deleting mirror pod %q: %v\", format.Pod(mirrorPod), err) } else { deleted = true } } } if mirrorPod == nil || deleted { node, err := kl.GetNode() if err != nil || node.DeletionTimestamp != nil { glog.V(4).Infof(\"No need to create a mirror pod, since node %q has been removed from the cluster\", kl.nodeName) } else { glog.V(4).Infof(\"Creating a mirror pod for static pod %q\", format.Pod(pod)) if err := kl.podManager.CreateMirrorPod(pod); err != nil { glog.Errorf(\"Failed creating a mirror pod for %q: %v\", format.Pod(pod), err) } } } } 2.5. makePodDataDirs 给pod创建数据目录。 // Make data directories for the pod if err := kl.makePodDataDirs(pod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToMakePodDataDirectories, \"error making pod data directories: %v\", err) glog.Errorf(\"Unable to make pod data directories for pod %q: %v\", format.Pod(pod), err) return err } 其中数据目录包括 PodDir：{kubelet.rootDirectory}/pods/podUID PodVolumesDir：{PodDir}/volumes PodPluginsDir：{PodDir}/plugins // makePodDataDirs creates the dirs for the pod datas. func (kl *Kubelet) makePodDataDirs(pod *v1.Pod) error { uid := pod.UID if err := os.MkdirAll(kl.getPodDir(uid), 0750); err != nil && !os.IsExist(err) { return err } if err := os.MkdirAll(kl.getPodVolumesDir(uid), 0750); err != nil && !os.IsExist(err) { return err } if err := os.MkdirAll(kl.getPodPluginsDir(uid), 0750); err != nil && !os.IsExist(err) { return err } return nil } 2.6. mount volumes 对非terminated状态的pod挂载volume。 // Volume manager will not mount volumes for terminated pods if !kl.podIsTerminated(pod) { // Wait for volumes to attach/mount if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil { kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, \"Unable to mount volumes for pod %q: %v\", format.Pod(pod), err) glog.Errorf(\"Unable to mount volumes for pod %q: %v; skipping pod\", format.Pod(pod), err) return err } } 2.7. PullSecretsForPod 获取pod的secret数据。 // Fetch the pull secrets for the pod pullSecrets := kl.getPullSecretsForPod(pod) getPullSecretsForPod具体实现函数如下： // getPullSecretsForPod inspects the Pod and retrieves the referenced pull // secrets. func (kl *Kubelet) getPullSecretsForPod(pod *v1.Pod) []v1.Secret { pullSecrets := []v1.Secret{} for _, secretRef := range pod.Spec.ImagePullSecrets { secret, err := kl.secretManager.GetSecret(pod.Namespace, secretRef.Name) if err != nil { glog.Warningf(\"Unable to retrieve pull secret %s/%s for %s/%s due to %v. The image pull may not succeed.\", pod.Namespace, secretRef.Name, pod.Namespace, pod.Name, err) continue } pullSecrets = append(pullSecrets, *secret) } return pullSecrets } 2.8. containerRuntime.SyncPod 调用container runtime的SyncPod函数，执行相关pod操作，由此kubelet.syncPod的操作逻辑转入containerRuntime.SyncPod函数中。 // Call the container runtime's SyncPod callback result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) if err := result.Error(); err != nil { // Do not return error if the only failures were pods in backoff for _, r := range result.SyncResults { if r.Error != kubecontainer.ErrCrashLoopBackOff && r.Error != images.ErrImagePullBackOff { // Do not record an event here, as we keep all event logging for sync pod failures // local to container runtime so we get better errors return err } } return nil } 3. Runtime.SyncPod SyncPod主要执行sync操作使得运行的pod达到期望状态的pod。主要执行以下操作： 计算sandbox和container的变化。 必要的时候杀死pod。 杀死所有不需要运行的container。 必要时创建sandbox。 创建init container。 创建正常的container。 Runtime.SyncPod部分代码位于pkg/kubelet/kuberuntime/kuberuntime_manager.go 3.1. computePodActions 计算sandbox和container的变化。 // Step 1: Compute sandbox and container changes. podContainerChanges := m.computePodActions(pod, podStatus) glog.V(3).Infof(\"computePodActions got %+v for pod %q\", podContainerChanges, format.Pod(pod)) if podContainerChanges.CreateSandbox { ref, err := ref.GetReference(legacyscheme.Scheme, pod) if err != nil { glog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), err) } if podContainerChanges.SandboxID != \"\" { m.recorder.Eventf(ref, v1.EventTypeNormal, events.SandboxChanged, \"Pod sandbox changed, it will be killed and re-created.\") } else { glog.V(4).Infof(\"SyncPod received new pod %q, will create a sandbox for it\", format.Pod(pod)) } } 3.2. killPodWithSyncResult 必要的时候杀死pod。 // Step 2: Kill the pod if the sandbox has changed. if podContainerChanges.KillPod { if !podContainerChanges.CreateSandbox { glog.V(4).Infof(\"Stopping PodSandbox for %q because all other containers are dead.\", format.Pod(pod)) } else { glog.V(4).Infof(\"Stopping PodSandbox for %q, will start new one\", format.Pod(pod)) } killResult := m.killPodWithSyncResult(pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil) result.AddPodSyncResult(killResult) if killResult.Error() != nil { glog.Errorf(\"killPodWithSyncResult failed: %v\", killResult.Error()) return } if podContainerChanges.CreateSandbox { m.purgeInitContainers(pod, podStatus) } } 3.3. killContainer 杀死所有不需要运行的container。 // Step 3: kill any running containers in this pod which are not to keep. for containerID, containerInfo := range podContainerChanges.ContainersToKill { glog.V(3).Infof(\"Killing unwanted container %q(id=%q) for pod %q\", containerInfo.name, containerID, format.Pod(pod)) killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, containerInfo.name) result.AddSyncResult(killContainerResult) if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil { killContainerResult.Fail(kubecontainer.ErrKillContainer, err.Error()) glog.Errorf(\"killContainer %q(id=%q) for pod %q failed: %v\", containerInfo.name, containerID, format.Pod(pod), err) return } } 3.4. createPodSandbox 必要时创建sandbox。 // Step 4: Create a sandbox for the pod if necessary. ... glog.V(4).Infof(\"Creating sandbox for pod %q\", format.Pod(pod)) createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, format.Pod(pod)) result.AddSyncResult(createSandboxResult) podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt) if err != nil { createSandboxResult.Fail(kubecontainer.ErrCreatePodSandbox, msg) glog.Errorf(\"createPodSandbox for pod %q failed: %v\", format.Pod(pod), err) ref, referr := ref.GetReference(legacyscheme.Scheme, pod) if referr != nil { glog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), referr) } m.recorder.Eventf(ref, v1.EventTypeWarning, events.FailedCreatePodSandBox, \"Failed create pod sandbox: %v\", err) return } glog.V(4).Infof(\"Created PodSandbox %q for pod %q\", podSandboxID, format.Pod(pod)) 3.5. start init container 创建init container。 // Step 5: start the init container. if container := podContainerChanges.NextInitContainerToStart; container != nil { // Start the next init container. startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name) result.AddSyncResult(startContainerResult) isInBackOff, msg, err := m.doBackOff(pod, container, podStatus, backOff) if isInBackOff { startContainerResult.Fail(err, msg) glog.V(4).Infof(\"Backing Off restarting init container %+v in pod %v\", container, format.Pod(pod)) return } glog.V(4).Infof(\"Creating init container %+v in pod %v\", container, format.Pod(pod)) if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil { startContainerResult.Fail(err, msg) utilruntime.HandleError(fmt.Errorf(\"init container start failed: %v: %s\", err, msg)) return } // Successfully started the container; clear the entry in the failure glog.V(4).Infof(\"Completed init container %q for pod %q\", container.Name, format.Pod(pod)) } 3.6. start containers 创建正常的container。 // Step 6: start containers in podContainerChanges.ContainersToStart. for _, idx := range podContainerChanges.ContainersToStart { container := &pod.Spec.Containers[idx] startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, container.Name) result.AddSyncResult(startContainerResult) isInBackOff, msg, err := m.doBackOff(pod, container, podStatus, backOff) if isInBackOff { startContainerResult.Fail(err, msg) glog.V(4).Infof(\"Backing Off restarting container %+v in pod %v\", container, format.Pod(pod)) continue } glog.V(4).Infof(\"Creating container %+v in pod %v\", container, format.Pod(pod)) // 通过startContainer来运行容器 if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil { startContainerResult.Fail(err, msg) // known errors that are logged in other places are logged at higher levels here to avoid // repetitive log spam switch { case err == images.ErrImagePullBackOff: glog.V(3).Infof(\"container start failed: %v: %s\", err, msg) default: utilruntime.HandleError(fmt.Errorf(\"container start failed: %v: %s\", err, msg)) } continue } } 4. startContainer startContainer启动一个容器并返回是否成功。 主要包括以下几个步骤： 拉取镜像 创建容器 启动容器 运行post start lifecycle hooks(如果有设置此项) startContainer完整代码如下： startContainer部分代码位于pkg/kubelet/kuberuntime/kuberuntime_container.go // startContainer starts a container and returns a message indicates why it is failed on error. // It starts the container through the following steps: // * pull the image // * create the container // * start the container // * run the post start lifecycle hooks (if applicable) func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) { // Step 1: pull the image. imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets) if err != nil { m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return msg, err } // Step 2: create the container. ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil { glog.Errorf(\"Can't make a ref to pod %q, container %v: %v\", format.Pod(pod), container.Name, err) } glog.V(4).Infof(\"Generating ref for container %s: %#v\", container.Name, ref) // For a new container, the RestartCount should be 0 restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil { restartCount = containerStatus.RestartCount + 1 } containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType) if cleanupAction != nil { defer cleanupAction() } if err != nil { m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrCreateContainerConfig } containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrCreateContainer } err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Internal PreStartContainer hook failed: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrPreStartHook } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.CreatedContainer, \"Created container\") if ref != nil { m.containerRefManager.SetRef(kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, }, ref) } // Step 3: start the container. err = m.runtimeService.StartContainer(containerID) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), kubecontainer.ErrRunContainer } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.StartedContainer, \"Started container\") // Symlink container logs to the legacy container log location for cluster logging // support. // TODO(random-liu): Remove this after cluster logging supports CRI container log path. containerMeta := containerConfig.GetMetadata() sandboxMeta := podSandboxConfig.GetMetadata() legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) // only create legacy symlink if containerLog path exists (or the error is not IsNotExist). // Because if containerLog path does not exist, only dandling legacySymlink is created. // This dangling legacySymlink is later removed by container gc, so it does not make sense // to create it in the first place. it happens when journald logging driver is used with docker. if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) { if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil { glog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\", legacySymlink, containerID, containerLog, err) } } // Step 4: execute the post start hook. if container.Lifecycle != nil && container.Lifecycle.PostStart != nil { kubeContainerID := kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, } msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil { m.recordContainerEvent(pod, container, kubeContainerID.ID, v1.EventTypeWarning, events.FailedPostStartHook, msg) if err := m.killContainer(pod, kubeContainerID, container.Name, \"FailedPostStartHook\", nil); err != nil { glog.Errorf(\"Failed to kill container %q(id=%q) in pod %q: %v, %v\", container.Name, kubeContainerID.String(), format.Pod(pod), ErrPostStartHook, err) } return msg, fmt.Errorf(\"%s: %v\", ErrPostStartHook, handlerErr) } } return \"\", nil } 以下对startContainer分段分析： 4.1. pull image 通过EnsureImageExists方法拉取拉取指定pod容器的镜像，并返回镜像信息和错误。 // Step 1: pull the image. imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets) if err != nil { m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return msg, err } 4.2. CreateContainer 首先生成container的*v1.ObjectReference对象，该对象包括container的相关信息。 // Step 2: create the container. ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil { glog.Errorf(\"Can't make a ref to pod %q, container %v: %v\", format.Pod(pod), container.Name, err) } glog.V(4).Infof(\"Generating ref for container %s: %#v\", container.Name, ref) 统计container的重启次数，新的容器默认重启次数为0。 // For a new container, the RestartCount should be 0 restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil { restartCount = containerStatus.RestartCount + 1 } 生成container的配置。 containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType) if cleanupAction != nil { defer cleanupAction() } if err != nil { m.recordContainerEvent(pod, container, \"\", v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrCreateContainerConfig } 调用runtimeService，执行CreateContainer的操作。 containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToCreateContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrCreateContainer } err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Internal PreStartContainer hook failed: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), ErrPreStartHook } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.CreatedContainer, \"Created container\") if ref != nil { m.containerRefManager.SetRef(kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, }, ref) } 4.3. StartContainer 执行runtimeService的StartContainer方法，来启动容器。 // Step 3: start the container. err = m.runtimeService.StartContainer(containerID) if err != nil { m.recordContainerEvent(pod, container, containerID, v1.EventTypeWarning, events.FailedToStartContainer, \"Error: %v\", grpc.ErrorDesc(err)) return grpc.ErrorDesc(err), kubecontainer.ErrRunContainer } m.recordContainerEvent(pod, container, containerID, v1.EventTypeNormal, events.StartedContainer, \"Started container\") // Symlink container logs to the legacy container log location for cluster logging // support. // TODO(random-liu): Remove this after cluster logging supports CRI container log path. containerMeta := containerConfig.GetMetadata() sandboxMeta := podSandboxConfig.GetMetadata() legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) // only create legacy symlink if containerLog path exists (or the error is not IsNotExist). // Because if containerLog path does not exist, only dandling legacySymlink is created. // This dangling legacySymlink is later removed by container gc, so it does not make sense // to create it in the first place. it happens when journald logging driver is used with docker. if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) { if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil { glog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\", legacySymlink, containerID, containerLog, err) } } 4.4. execute post start hook 如果有指定Lifecycle.PostStart，则执行PostStart操作，PostStart如果执行失败，则容器会根据重启的规则进行重启。 // Step 4: execute the post start hook. if container.Lifecycle != nil && container.Lifecycle.PostStart != nil { kubeContainerID := kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, } msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil { m.recordContainerEvent(pod, container, kubeContainerID.ID, v1.EventTypeWarning, events.FailedPostStartHook, msg) if err := m.killContainer(pod, kubeContainerID, container.Name, \"FailedPostStartHook\", nil); err != nil { glog.Errorf(\"Failed to kill container %q(id=%q) in pod %q: %v, %v\", container.Name, kubeContainerID.String(), format.Pod(pod), ErrPostStartHook, err) } return msg, fmt.Errorf(\"%s: %v\", ErrPostStartHook, handlerErr) } } 5. 总结 kubelet的工作是管理pod在Node上的生命周期（包括增删改查），kubelet通过各种类型的manager异步工作各自执行各自的任务，其中使用到了多种的channel来控制状态信号变化的传递，例如比较重要的channel有podUpdates ，来传递pod的变化情况。 创建pod的调用逻辑 syncLoopIteration-->kubetypes.ADD-->HandlePodAdditions(u.Pods)-->dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start)-->podWorkers.UpdatePod-->managePodLoop(podUpdates)-->syncPod(o syncPodOptions)-->containerRuntime.SyncPod-->startContainer 参考： https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kubelet.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/pod_workers.go https://github.com/kubernetes/kubernetes/blob/v1.12.0/pkg/kubelet/kuberuntime/kuberuntime_container.go Copyright © www.huweihuang.com 2017-2018 all right reserved，powered by GitbookUpdated at 2019-07-31 10:18:04 "}}